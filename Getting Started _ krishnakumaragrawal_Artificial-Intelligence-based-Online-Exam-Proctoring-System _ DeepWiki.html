<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" href="/_next/static/media/4cf2300e9c8272f7-s.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="preload" href="/_next/static/media/93f479601ee12b01-s.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="stylesheet" href="/_next/static/css/de70bee13400563f.css?dpl=dpl_3BvSF9SjLWU2C3NrwwXTj5194PnT" data-precedence="next"/><link rel="stylesheet" href="/_next/static/css/fc1993f38795cea2.css?dpl=dpl_3BvSF9SjLWU2C3NrwwXTj5194PnT" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/webpack-acbbbb548492d4a6.js?dpl=dpl_3BvSF9SjLWU2C3NrwwXTj5194PnT"/><script src="/_next/static/chunks/4bd1b696-cebf68b71ed1e85d.js?dpl=dpl_3BvSF9SjLWU2C3NrwwXTj5194PnT" async=""></script><script src="/_next/static/chunks/1684-a1275480b978a231.js?dpl=dpl_3BvSF9SjLWU2C3NrwwXTj5194PnT" async=""></script><script src="/_next/static/chunks/main-app-8ab5af3d6b81086e.js?dpl=dpl_3BvSF9SjLWU2C3NrwwXTj5194PnT" async=""></script><script src="/_next/static/chunks/b1298b8d-549c141f97a3b262.js?dpl=dpl_3BvSF9SjLWU2C3NrwwXTj5194PnT" async=""></script><script src="/_next/static/chunks/378e5a93-3b0f971d3611a8a5.js?dpl=dpl_3BvSF9SjLWU2C3NrwwXTj5194PnT" async=""></script><script src="/_next/static/chunks/f7f68e2d-40290491c524df5c.js?dpl=dpl_3BvSF9SjLWU2C3NrwwXTj5194PnT" async=""></script><script src="/_next/static/chunks/5009-cf1c1739f4eccbfa.js?dpl=dpl_3BvSF9SjLWU2C3NrwwXTj5194PnT" async=""></script><script src="/_next/static/chunks/3377-d302682beb4206f6.js?dpl=dpl_3BvSF9SjLWU2C3NrwwXTj5194PnT" async=""></script><script src="/_next/static/chunks/3224-7e9887ea92eab174.js?dpl=dpl_3BvSF9SjLWU2C3NrwwXTj5194PnT" async=""></script><script src="/_next/static/chunks/6967-c4b815e71e97ed18.js?dpl=dpl_3BvSF9SjLWU2C3NrwwXTj5194PnT" async=""></script><script src="/_next/static/chunks/app/layout-97dc749f3c752109.js?dpl=dpl_3BvSF9SjLWU2C3NrwwXTj5194PnT" async=""></script><script src="/_next/static/chunks/7bf36345-06f80506190927ed.js?dpl=dpl_3BvSF9SjLWU2C3NrwwXTj5194PnT" async=""></script><script src="/_next/static/chunks/c16f53c3-1a60b9b77b3e1d4b.js?dpl=dpl_3BvSF9SjLWU2C3NrwwXTj5194PnT" async=""></script><script src="/_next/static/chunks/537-bef73e6675141c6f.js?dpl=dpl_3BvSF9SjLWU2C3NrwwXTj5194PnT" async=""></script><script src="/_next/static/chunks/8039-e53433e94d896525.js?dpl=dpl_3BvSF9SjLWU2C3NrwwXTj5194PnT" async=""></script><script src="/_next/static/chunks/2136-947db7298d61ada7.js?dpl=dpl_3BvSF9SjLWU2C3NrwwXTj5194PnT" async=""></script><script src="/_next/static/chunks/9769-220603704fe7fef4.js?dpl=dpl_3BvSF9SjLWU2C3NrwwXTj5194PnT" async=""></script><script src="/_next/static/chunks/8674-a5b7b13d963b8a5d.js?dpl=dpl_3BvSF9SjLWU2C3NrwwXTj5194PnT" async=""></script><script src="/_next/static/chunks/3283-f8ea31617a625982.js?dpl=dpl_3BvSF9SjLWU2C3NrwwXTj5194PnT" async=""></script><script src="/_next/static/chunks/5104-74152de4b96231ef.js?dpl=dpl_3BvSF9SjLWU2C3NrwwXTj5194PnT" async=""></script><script src="/_next/static/chunks/5024-17a1979955106ff0.js?dpl=dpl_3BvSF9SjLWU2C3NrwwXTj5194PnT" async=""></script><script src="/_next/static/chunks/2853-5659559f5634fdac.js?dpl=dpl_3BvSF9SjLWU2C3NrwwXTj5194PnT" async=""></script><script src="/_next/static/chunks/app/%5Borg%5D/%5Brepo%5D/%5B%5B...wikiRoutes%5D%5D/page-1febda9ce73dd135.js?dpl=dpl_3BvSF9SjLWU2C3NrwwXTj5194PnT" async=""></script><script src="/_next/static/chunks/3449-8ea8d12ec92ab7b3.js?dpl=dpl_3BvSF9SjLWU2C3NrwwXTj5194PnT" async=""></script><script src="/_next/static/chunks/2015-d2f0d3f0cd483d40.js?dpl=dpl_3BvSF9SjLWU2C3NrwwXTj5194PnT" async=""></script><script src="/_next/static/chunks/736-daf66278216745a2.js?dpl=dpl_3BvSF9SjLWU2C3NrwwXTj5194PnT" async=""></script><script src="/_next/static/chunks/app/%5Borg%5D/%5Brepo%5D/layout-3efb06fa04ccb94c.js?dpl=dpl_3BvSF9SjLWU2C3NrwwXTj5194PnT" async=""></script><meta name="next-size-adjust" content=""/><title>Getting Started | krishnakumaragrawal/Artificial-Intelligence-based-Online-Exam-Proctoring-System | DeepWiki</title><meta name="description" content="This document provides a comprehensive guide to setting up and running the AI-based Online Exam Proctoring System. It covers the primary entry points, system initialization, and basic usage patterns t"/><meta property="og:title" content="Getting Started | krishnakumaragrawal/Artificial-Intelligence-based-Online-Exam-Proctoring-System | DeepWiki"/><meta property="og:description" content="This document provides a comprehensive guide to setting up and running the AI-based Online Exam Proctoring System. It covers the primary entry points, system initialization, and basic usage patterns t"/><meta property="og:url" content="https://deepwiki.com/krishnakumaragrawal/Artificial-Intelligence-based-Online-Exam-Proctoring-System/2-getting-started"/><meta property="og:site_name" content="DeepWiki"/><meta property="og:type" content="website"/><meta name="twitter:card" content="summary"/><meta name="twitter:title" content="Getting Started | krishnakumaragrawal/Artificial-Intelligence-based-Online-Exam-Proctoring-System | DeepWiki"/><meta name="twitter:description" content="This document provides a comprehensive guide to setting up and running the AI-based Online Exam Proctoring System. It covers the primary entry points, system initialization, and basic usage patterns t"/><link rel="icon" href="/favicon.ico" type="image/x-icon" sizes="48x48"/><link rel="icon" href="/icon.png?66aaf51e0e68c818" type="image/png" sizes="16x16"/><link rel="apple-touch-icon" href="/apple-icon.png?a4f658907db0ab87" type="image/png" sizes="180x180"/><script src="/_next/static/chunks/polyfills-42372ed130431b0a.js?dpl=dpl_3BvSF9SjLWU2C3NrwwXTj5194PnT" noModule=""></script></head><body class="__variable_188709 font-geist-sans relative min-h-screen __variable_9a8899 bg-background antialiased"><section aria-label="Notifications alt+T" tabindex="-1" aria-live="polite" aria-relevant="additions text" aria-atomic="false"></section><script>((e,t,r,n,a,o,i,s)=>{let u=document.documentElement,l=["light","dark"];function c(t){var r;(Array.isArray(e)?e:[e]).forEach(e=>{let r="class"===e,n=r&&o?a.map(e=>o[e]||e):a;r?(u.classList.remove(...n),u.classList.add(o&&o[t]?o[t]:t)):u.setAttribute(e,t)}),r=t,s&&l.includes(r)&&(u.style.colorScheme=r)}if(n)c(n);else try{let e=localStorage.getItem(t)||r,n=i&&"system"===e?window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light":e;c(n)}catch(e){}})("class","theme","light",null,["light","dark"],null,true,true)</script><!--$--><div class="flex min-h-screen w-full flex-col text-white" id="codebase-wiki-repo-page"><div class="bg-background border-b-border sticky top-0 z-30 border-b border-dashed"><div class="font-geist-mono relative flex h-8 items-center justify-center text-xs font-medium sm:hidden"><div class="powered-by-devin-gradient absolute inset-0 z-[-1] h-8 w-full"></div><a class="flex items-center gap-2" href="/private-repo"><svg class="size-3 [&amp;_path]:stroke-0 [&amp;_path]:animate-[custom-pulse_1.8s_infinite_var(--delay,0s)]" xmlns="http://www.w3.org/2000/svg" viewBox="110 110 460 500"><path style="fill:#21c19a" class="[--delay:0.6s]" d="M418.73,332.37c9.84-5.68,22.07-5.68,31.91,0l25.49,14.71c.82.48,1.69.8,2.58,1.06.19.06.37.11.55.16.87.21,1.76.34,2.65.35.04,0,.08.02.13.02.1,0,.19-.03.29-.04.83-.02,1.64-.13,2.45-.32.14-.03.28-.05.42-.09.87-.24,1.7-.59,2.5-1.03.08-.04.17-.06.25-.1l50.97-29.43c3.65-2.11,5.9-6.01,5.9-10.22v-58.86c0-4.22-2.25-8.11-5.9-10.22l-50.97-29.43c-3.65-2.11-8.15-2.11-11.81,0l-50.97,29.43c-.08.04-.13.11-.2.16-.78.48-1.51,1.02-2.15,1.66-.1.1-.18.21-.28.31-.57.6-1.08,1.26-1.51,1.97-.07.12-.15.22-.22.34-.44.77-.77,1.6-1.03,2.47-.05.19-.1.37-.14.56-.22.89-.37,1.81-.37,2.76v29.43c0,11.36-6.11,21.95-15.95,27.63-9.84,5.68-22.06,5.68-31.91,0l-25.49-14.71c-.82-.48-1.69-.8-2.57-1.06-.19-.06-.37-.11-.56-.16-.88-.21-1.76-.34-2.65-.34-.13,0-.26.02-.4.02-.84.02-1.66.13-2.47.32-.13.03-.27.05-.4.09-.87.24-1.71.6-2.51,1.04-.08.04-.16.06-.24.1l-50.97,29.43c-3.65,2.11-5.9,6.01-5.9,10.22v58.86c0,4.22,2.25,8.11,5.9,10.22l50.97,29.43c.08.04.17.06.24.1.8.44,1.64.79,2.5,1.03.14.04.28.06.42.09.81.19,1.62.3,2.45.32.1,0,.19.04.29.04.04,0,.08-.02.13-.02.89,0,1.77-.13,2.65-.35.19-.04.37-.1.56-.16.88-.26,1.75-.59,2.58-1.06l25.49-14.71c9.84-5.68,22.06-5.68,31.91,0,9.84,5.68,15.95,16.27,15.95,27.63v29.43c0,.95.15,1.87.37,2.76.05.19.09.37.14.56.25.86.59,1.69,1.03,2.47.07.12.15.22.22.34.43.71.94,1.37,1.51,1.97.1.1.18.21.28.31.65.63,1.37,1.18,2.15,1.66.07.04.13.11.2.16l50.97,29.43c1.83,1.05,3.86,1.58,5.9,1.58s4.08-.53,5.9-1.58l50.97-29.43c3.65-2.11,5.9-6.01,5.9-10.22v-58.86c0-4.22-2.25-8.11-5.9-10.22l-50.97-29.43c-.08-.04-.16-.06-.24-.1-.8-.44-1.64-.8-2.51-1.04-.13-.04-.26-.05-.39-.09-.82-.2-1.65-.31-2.49-.33-.13,0-.25-.02-.38-.02-.89,0-1.78.13-2.66.35-.18.04-.36.1-.54.15-.88.26-1.75.59-2.58,1.07l-25.49,14.72c-9.84,5.68-22.07,5.68-31.9,0-9.84-5.68-15.95-16.27-15.95-27.63s6.11-21.95,15.95-27.63Z"></path><path style="fill:#3969ca" d="M141.09,317.65l50.97,29.43c1.83,1.05,3.86,1.58,5.9,1.58s4.08-.53,5.9-1.58l50.97-29.43c.08-.04.13-.11.2-.16.78-.48,1.51-1.02,2.15-1.66.1-.1.18-.21.28-.31.57-.6,1.08-1.26,1.51-1.97.07-.12.15-.22.22-.34.44-.77.77-1.6,1.03-2.47.05-.19.1-.37.14-.56.22-.89.37-1.81.37-2.76v-29.43c0-11.36,6.11-21.95,15.96-27.63s22.06-5.68,31.91,0l25.49,14.71c.82.48,1.69.8,2.57,1.06.19.06.37.11.56.16.87.21,1.76.34,2.64.35.04,0,.09.02.13.02.1,0,.19-.04.29-.04.83-.02,1.65-.13,2.45-.32.14-.03.28-.05.41-.09.87-.24,1.71-.6,2.51-1.04.08-.04.16-.06.24-.1l50.97-29.43c3.65-2.11,5.9-6.01,5.9-10.22v-58.86c0-4.22-2.25-8.11-5.9-10.22l-50.97-29.43c-3.65-2.11-8.15-2.11-11.81,0l-50.97,29.43c-.08.04-.13.11-.2.16-.78.48-1.51,1.02-2.15,1.66-.1.1-.18.21-.28.31-.57.6-1.08,1.26-1.51,1.97-.07.12-.15.22-.22.34-.44.77-.77,1.6-1.03,2.47-.05.19-.1.37-.14.56-.22.89-.37,1.81-.37,2.76v29.43c0,11.36-6.11,21.95-15.95,27.63-9.84,5.68-22.07,5.68-31.91,0l-25.49-14.71c-.82-.48-1.69-.8-2.58-1.06-.19-.06-.37-.11-.55-.16-.88-.21-1.76-.34-2.65-.35-.13,0-.26.02-.4.02-.83.02-1.66.13-2.47.32-.13.03-.27.05-.4.09-.87.24-1.71.6-2.51,1.04-.08.04-.16.06-.24.1l-50.97,29.43c-3.65,2.11-5.9,6.01-5.9,10.22v58.86c0,4.22,2.25,8.11,5.9,10.22Z"></path><path style="fill:#0294de" class="[--delay:1.2s]" d="M396.88,484.35l-50.97-29.43c-.08-.04-.17-.06-.24-.1-.8-.44-1.64-.79-2.51-1.03-.14-.04-.27-.06-.41-.09-.81-.19-1.64-.3-2.47-.32-.13,0-.26-.02-.39-.02-.89,0-1.78.13-2.66.35-.18.04-.36.1-.54.15-.88.26-1.76.59-2.58,1.07l-25.49,14.72c-9.84,5.68-22.06,5.68-31.9,0-9.84-5.68-15.96-16.27-15.96-27.63v-29.43c0-.95-.15-1.87-.37-2.76-.05-.19-.09-.37-.14-.56-.25-.86-.59-1.69-1.03-2.47-.07-.12-.15-.22-.22-.34-.43-.71-.94-1.37-1.51-1.97-.1-.1-.18-.21-.28-.31-.65-.63-1.37-1.18-2.15-1.66-.07-.04-.13-.11-.2-.16l-50.97-29.43c-3.65-2.11-8.15-2.11-11.81,0l-50.97,29.43c-3.65,2.11-5.9,6.01-5.9,10.22v58.86c0,4.22,2.25,8.11,5.9,10.22l50.97,29.43c.08.04.17.06.25.1.8.44,1.63.79,2.5,1.03.14.04.29.06.43.09.8.19,1.61.3,2.43.32.1,0,.2.04.3.04.04,0,.09-.02.13-.02.88,0,1.77-.13,2.64-.34.19-.04.37-.1.56-.16.88-.26,1.75-.59,2.57-1.06l25.49-14.71c9.84-5.68,22.06-5.68,31.91,0,9.84,5.68,15.95,16.27,15.95,27.63v29.43c0,.95.15,1.87.37,2.76.05.19.09.37.14.56.25.86.59,1.69,1.03,2.47.07.12.15.22.22.34.43.71.94,1.37,1.51,1.97.1.1.18.21.28.31.65.63,1.37,1.18,2.15,1.66.07.04.13.11.2.16l50.97,29.43c1.83,1.05,3.86,1.58,5.9,1.58s4.08-.53,5.9-1.58l50.97-29.43c3.65-2.11,5.9-6.01,5.9-10.22v-58.86c0-4.22-2.25-8.11-5.9-10.22Z"></path></svg>Index your code with Devin</a></div><div class="container-wrapper"><div class="container mx-auto flex w-full flex-row items-center gap-2 py-4 md:py-6"><a class="flex items-center gap-3" href="https://deepwiki.com"><span class="text-base font-medium leading-none md:text-lg hidden sm:block">DeepWiki</span></a><div class="flex-1"><div class="flex flex-row items-center gap-2"><a class="block text-xs font-medium leading-none text-white sm:hidden md:text-lg" href="/">DeepWiki</a><p class="text-sm font-normal leading-none md:text-lg"><a href="https://github.com/krishnakumaragrawal/Artificial-Intelligence-based-Online-Exam-Proctoring-System" target="_blank" rel="noopener noreferrer" class="text-muted-foreground hover:text-muted-foreground/80 transition-colors">krishnakumaragrawal/Artificial-Intelligence-based-Online-Exam-Proctoring-System</a></p></div></div><div class="flex items-center gap-4"><a class="group hidden items-center gap-1.5 md:flex" href="/private-repo"><div class="relative"><span class="text-foreground/70 group-hover:text-foreground text-xs font-light transition-colors">Index your code with</span><div class="absolute bottom-0 left-0 h-[1px] w-0 bg-black/30 transition-all duration-300 group-hover:w-full"></div></div><div class="flex items-center gap-1 transition-transform duration-300 group-hover:translate-x-0.5"><svg class="size-4 transform transition-transform duration-700 group-hover:rotate-180 [&amp;_path]:stroke-0" xmlns="http://www.w3.org/2000/svg" viewBox="110 110 460 500"><path style="fill:#21c19a" class="" d="M418.73,332.37c9.84-5.68,22.07-5.68,31.91,0l25.49,14.71c.82.48,1.69.8,2.58,1.06.19.06.37.11.55.16.87.21,1.76.34,2.65.35.04,0,.08.02.13.02.1,0,.19-.03.29-.04.83-.02,1.64-.13,2.45-.32.14-.03.28-.05.42-.09.87-.24,1.7-.59,2.5-1.03.08-.04.17-.06.25-.1l50.97-29.43c3.65-2.11,5.9-6.01,5.9-10.22v-58.86c0-4.22-2.25-8.11-5.9-10.22l-50.97-29.43c-3.65-2.11-8.15-2.11-11.81,0l-50.97,29.43c-.08.04-.13.11-.2.16-.78.48-1.51,1.02-2.15,1.66-.1.1-.18.21-.28.31-.57.6-1.08,1.26-1.51,1.97-.07.12-.15.22-.22.34-.44.77-.77,1.6-1.03,2.47-.05.19-.1.37-.14.56-.22.89-.37,1.81-.37,2.76v29.43c0,11.36-6.11,21.95-15.95,27.63-9.84,5.68-22.06,5.68-31.91,0l-25.49-14.71c-.82-.48-1.69-.8-2.57-1.06-.19-.06-.37-.11-.56-.16-.88-.21-1.76-.34-2.65-.34-.13,0-.26.02-.4.02-.84.02-1.66.13-2.47.32-.13.03-.27.05-.4.09-.87.24-1.71.6-2.51,1.04-.08.04-.16.06-.24.1l-50.97,29.43c-3.65,2.11-5.9,6.01-5.9,10.22v58.86c0,4.22,2.25,8.11,5.9,10.22l50.97,29.43c.08.04.17.06.24.1.8.44,1.64.79,2.5,1.03.14.04.28.06.42.09.81.19,1.62.3,2.45.32.1,0,.19.04.29.04.04,0,.08-.02.13-.02.89,0,1.77-.13,2.65-.35.19-.04.37-.1.56-.16.88-.26,1.75-.59,2.58-1.06l25.49-14.71c9.84-5.68,22.06-5.68,31.91,0,9.84,5.68,15.95,16.27,15.95,27.63v29.43c0,.95.15,1.87.37,2.76.05.19.09.37.14.56.25.86.59,1.69,1.03,2.47.07.12.15.22.22.34.43.71.94,1.37,1.51,1.97.1.1.18.21.28.31.65.63,1.37,1.18,2.15,1.66.07.04.13.11.2.16l50.97,29.43c1.83,1.05,3.86,1.58,5.9,1.58s4.08-.53,5.9-1.58l50.97-29.43c3.65-2.11,5.9-6.01,5.9-10.22v-58.86c0-4.22-2.25-8.11-5.9-10.22l-50.97-29.43c-.08-.04-.16-.06-.24-.1-.8-.44-1.64-.8-2.51-1.04-.13-.04-.26-.05-.39-.09-.82-.2-1.65-.31-2.49-.33-.13,0-.25-.02-.38-.02-.89,0-1.78.13-2.66.35-.18.04-.36.1-.54.15-.88.26-1.75.59-2.58,1.07l-25.49,14.72c-9.84,5.68-22.07,5.68-31.9,0-9.84-5.68-15.95-16.27-15.95-27.63s6.11-21.95,15.95-27.63Z"></path><path style="fill:#3969ca" d="M141.09,317.65l50.97,29.43c1.83,1.05,3.86,1.58,5.9,1.58s4.08-.53,5.9-1.58l50.97-29.43c.08-.04.13-.11.2-.16.78-.48,1.51-1.02,2.15-1.66.1-.1.18-.21.28-.31.57-.6,1.08-1.26,1.51-1.97.07-.12.15-.22.22-.34.44-.77.77-1.6,1.03-2.47.05-.19.1-.37.14-.56.22-.89.37-1.81.37-2.76v-29.43c0-11.36,6.11-21.95,15.96-27.63s22.06-5.68,31.91,0l25.49,14.71c.82.48,1.69.8,2.57,1.06.19.06.37.11.56.16.87.21,1.76.34,2.64.35.04,0,.09.02.13.02.1,0,.19-.04.29-.04.83-.02,1.65-.13,2.45-.32.14-.03.28-.05.41-.09.87-.24,1.71-.6,2.51-1.04.08-.04.16-.06.24-.1l50.97-29.43c3.65-2.11,5.9-6.01,5.9-10.22v-58.86c0-4.22-2.25-8.11-5.9-10.22l-50.97-29.43c-3.65-2.11-8.15-2.11-11.81,0l-50.97,29.43c-.08.04-.13.11-.2.16-.78.48-1.51,1.02-2.15,1.66-.1.1-.18.21-.28.31-.57.6-1.08,1.26-1.51,1.97-.07.12-.15.22-.22.34-.44.77-.77,1.6-1.03,2.47-.05.19-.1.37-.14.56-.22.89-.37,1.81-.37,2.76v29.43c0,11.36-6.11,21.95-15.95,27.63-9.84,5.68-22.07,5.68-31.91,0l-25.49-14.71c-.82-.48-1.69-.8-2.58-1.06-.19-.06-.37-.11-.55-.16-.88-.21-1.76-.34-2.65-.35-.13,0-.26.02-.4.02-.83.02-1.66.13-2.47.32-.13.03-.27.05-.4.09-.87.24-1.71.6-2.51,1.04-.08.04-.16.06-.24.1l-50.97,29.43c-3.65,2.11-5.9,6.01-5.9,10.22v58.86c0,4.22,2.25,8.11,5.9,10.22Z"></path><path style="fill:#0294de" class="" d="M396.88,484.35l-50.97-29.43c-.08-.04-.17-.06-.24-.1-.8-.44-1.64-.79-2.51-1.03-.14-.04-.27-.06-.41-.09-.81-.19-1.64-.3-2.47-.32-.13,0-.26-.02-.39-.02-.89,0-1.78.13-2.66.35-.18.04-.36.1-.54.15-.88.26-1.76.59-2.58,1.07l-25.49,14.72c-9.84,5.68-22.06,5.68-31.9,0-9.84-5.68-15.96-16.27-15.96-27.63v-29.43c0-.95-.15-1.87-.37-2.76-.05-.19-.09-.37-.14-.56-.25-.86-.59-1.69-1.03-2.47-.07-.12-.15-.22-.22-.34-.43-.71-.94-1.37-1.51-1.97-.1-.1-.18-.21-.28-.31-.65-.63-1.37-1.18-2.15-1.66-.07-.04-.13-.11-.2-.16l-50.97-29.43c-3.65-2.11-8.15-2.11-11.81,0l-50.97,29.43c-3.65,2.11-5.9,6.01-5.9,10.22v58.86c0,4.22,2.25,8.11,5.9,10.22l50.97,29.43c.08.04.17.06.25.1.8.44,1.63.79,2.5,1.03.14.04.29.06.43.09.8.19,1.61.3,2.43.32.1,0,.2.04.3.04.04,0,.09-.02.13-.02.88,0,1.77-.13,2.64-.34.19-.04.37-.1.56-.16.88-.26,1.75-.59,2.57-1.06l25.49-14.71c9.84-5.68,22.06-5.68,31.91,0,9.84,5.68,15.95,16.27,15.95,27.63v29.43c0,.95.15,1.87.37,2.76.05.19.09.37.14.56.25.86.59,1.69,1.03,2.47.07.12.15.22.22.34.43.71.94,1.37,1.51,1.97.1.1.18.21.28.31.65.63,1.37,1.18,2.15,1.66.07.04.13.11.2.16l50.97,29.43c1.83,1.05,3.86,1.58,5.9,1.58s4.08-.53,5.9-1.58l50.97-29.43c3.65-2.11,5.9-6.01,5.9-10.22v-58.86c0-4.22-2.25-8.11-5.9-10.22Z"></path></svg><span class="text-sm font-medium">Devin</span></div></a><button class="flex items-center rounded-md !text-white cursor-pointer transition-all border bg-blue-500 hover:bg-blue-600 border-blue-500 hover:border-blue-600 dark:bg-blue-900 dark:hover:bg-blue-800 dark:border-blue-900 dark:hover:border-blue-800 disabled:cursor-default disabled:opacity-50 disabled:hover:bg-blue-500 disabled:hover:border-blue-500 dark:disabled:hover:bg-blue-900 dark:disabled:hover:border-blue-900 gap-1.5 px-3 py-1.5 text-sm" aria-label="Share"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="h-4 w-4"><circle cx="18" cy="5" r="3"></circle><circle cx="6" cy="12" r="3"></circle><circle cx="18" cy="19" r="3"></circle><line x1="8.59" y1="13.51" x2="15.42" y2="17.49"></line><line x1="15.41" y1="6.51" x2="8.59" y2="10.49"></line></svg><span>Share</span></button><div class="h-8 w-8"></div></div></div></div></div><!--$--><div class="w-full flex-1"><div class="container-wrapper relative mx-auto h-full px-0"><div class="container relative mx-auto flex h-full w-full flex-col gap-0 max-md:!px-0 md:flex-row md:gap-6 lg:gap-10"><div class="border-r-border hidden max-h-screen border-r border-dashed py-6 pr-4 transition-[border-radius] md:sticky md:left-0 md:top-20 md:block md:h-[calc(100vh-82px)] md:w-64 md:flex-shrink-0 md:overflow-y-auto lg:py-9 xl:w-72"><div class="flex h-full w-full max-w-full flex-shrink-0 flex-col overflow-hidden" style="scrollbar-color:var(--color-border) transparent"><div class="flex-shrink-0 px-2"><div class="text-secondary pb-1 text-xs">Last indexed: <!-- -->26 June 2025<!-- --> (<a href="https://github.com/krishnakumaragrawal/Artificial-Intelligence-based-Online-Exam-Proctoring-System/commits/2b4f4e85" target="_blank" rel="noopener noreferrer">2b4f4e</a>)</div></div><ul class="flex-1 flex-shrink-0 space-y-1 overflow-y-auto py-1" style="scrollbar-width:none"><li style="padding-left:0"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/krishnakumaragrawal/Artificial-Intelligence-based-Online-Exam-Proctoring-System/1-overview">Overview</a></li><li style="padding-left:0"><a data-selected="true" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/krishnakumaragrawal/Artificial-Intelligence-based-Online-Exam-Proctoring-System/2-getting-started">Getting Started</a></li><li style="padding-left:0"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/krishnakumaragrawal/Artificial-Intelligence-based-Online-Exam-Proctoring-System/3-system-architecture">System Architecture</a></li><li style="padding-left:12px"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/krishnakumaragrawal/Artificial-Intelligence-based-Online-Exam-Proctoring-System/3.1-core-application-components">Core Application Components</a></li><li style="padding-left:12px"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/krishnakumaragrawal/Artificial-Intelligence-based-Online-Exam-Proctoring-System/3.2-activity-logging-system">Activity Logging System</a></li><li style="padding-left:0"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/krishnakumaragrawal/Artificial-Intelligence-based-Online-Exam-Proctoring-System/4-ai-detection-engine">AI Detection Engine</a></li><li style="padding-left:12px"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/krishnakumaragrawal/Artificial-Intelligence-based-Online-Exam-Proctoring-System/4.1-facial-analysis-modules">Facial Analysis Modules</a></li><li style="padding-left:12px"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/krishnakumaragrawal/Artificial-Intelligence-based-Online-Exam-Proctoring-System/4.2-object-and-audio-detection">Object and Audio Detection</a></li><li style="padding-left:0"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/krishnakumaragrawal/Artificial-Intelligence-based-Online-Exam-Proctoring-System/5-web-interface">Web Interface</a></li><li style="padding-left:12px"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/krishnakumaragrawal/Artificial-Intelligence-based-Online-Exam-Proctoring-System/5.1-authentication-system">Authentication System</a></li><li style="padding-left:12px"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/krishnakumaragrawal/Artificial-Intelligence-based-Online-Exam-Proctoring-System/5.2-quiz-interface">Quiz Interface</a></li><li style="padding-left:0"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/krishnakumaragrawal/Artificial-Intelligence-based-Online-Exam-Proctoring-System/6-database-and-data-management">Database and Data Management</a></li><li style="padding-left:0"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/krishnakumaragrawal/Artificial-Intelligence-based-Online-Exam-Proctoring-System/7-ai-models-and-configuration">AI Models and Configuration</a></li><li style="padding-left:12px"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/krishnakumaragrawal/Artificial-Intelligence-based-Online-Exam-Proctoring-System/7.1-yolo-object-detection-models">YOLO Object Detection Models</a></li><li style="padding-left:12px"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/krishnakumaragrawal/Artificial-Intelligence-based-Online-Exam-Proctoring-System/7.2-facial-landmark-models">Facial Landmark Models</a></li><li style="padding-left:0"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/krishnakumaragrawal/Artificial-Intelligence-based-Online-Exam-Proctoring-System/8-development-and-testing">Development and Testing</a></li><li style="padding-left:0"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/krishnakumaragrawal/Artificial-Intelligence-based-Online-Exam-Proctoring-System/9-static-assets">Static Assets</a></li></ul></div></div><div class="flex h-full flex-1 flex-col overflow-hidden"><div class="bg-background border-b-border sticky top-0 z-10 border-b border-dashed md:hidden"><div class="flex cursor-pointer items-center gap-2 p-3"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" viewBox="0 0 256 256" class="transition-transform"><path d="M184.49,136.49l-80,80a12,12,0,0,1-17-17L159,128,87.51,56.49a12,12,0,1,1,17-17l80,80A12,12,0,0,1,184.49,136.49Z"></path></svg><span class="truncate text-base font-normal">Menu</span></div></div><div class="relative flex-1 overflow-y-auto px-3 pt-3 md:rounded-md md:px-0 md:pt-0 [&amp;_::selection]:bg-purple-500/40" style="scrollbar-color:var(--color-night) transparent"><div class="pb-30 mx-auto max-w-2xl md:pb-40 md:pt-6 lg:pt-8"><div class="prose prose-invert dark:prose-invert prose-headings:text-inherit prose-p:text-inherit max-w-none"><div><div class="prose-custom prose-custom-md prose-custom-gray !max-w-none text-neutral-300 [overflow-wrap:anywhere]"><h1 id="getting-started" class="group" data-header="true">Getting Started<button class="relative ml-2 cursor-pointer align-baseline text-gray-400 opacity-0 transition-opacity hover:text-blue-400 focus:opacity-100 group-hover:opacity-100" aria-label="Copy link to header"><svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" fill="currentColor" viewBox="0 0 256 256" class="h-4 w-4"><path d="M117.18,188.74a12,12,0,0,1,0,17l-5.12,5.12A58.26,58.26,0,0,1,70.6,228h0A58.62,58.62,0,0,1,29.14,127.92L63.89,93.17a58.64,58.64,0,0,1,98.56,28.11,12,12,0,1,1-23.37,5.44,34.65,34.65,0,0,0-58.22-16.58L46.11,144.89A34.62,34.62,0,0,0,70.57,204h0a34.41,34.41,0,0,0,24.49-10.14l5.11-5.12A12,12,0,0,1,117.18,188.74ZM226.83,45.17a58.65,58.65,0,0,0-82.93,0l-5.11,5.11a12,12,0,0,0,17,17l5.12-5.12a34.63,34.63,0,1,1,49,49L175.1,145.86A34.39,34.39,0,0,1,150.61,156h0a34.63,34.63,0,0,1-33.69-26.72,12,12,0,0,0-23.38,5.44A58.64,58.64,0,0,0,150.56,180h.05a58.28,58.28,0,0,0,41.47-17.17l34.75-34.75a58.62,58.62,0,0,0,0-82.91Z"></path></svg></button></h1>
<details>
<summary>Relevant source files</summary>
<ul>
<li><a href="https://github.com/krishnakumaragrawal/Artificial-Intelligence-based-Online-Exam-Proctoring-System/blob/2b4f4e85/app.py" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4] rounded-r"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>app.py</span></a></li>
<li><a href="https://github.com/krishnakumaragrawal/Artificial-Intelligence-based-Online-Exam-Proctoring-System/blob/2b4f4e85/main.py" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4] rounded-r"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>main.py</span></a></li>
<li><a href="https://github.com/krishnakumaragrawal/Artificial-Intelligence-based-Online-Exam-Proctoring-System/blob/2b4f4e85/server.py" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4] rounded-r"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>server.py</span></a></li>
</ul>
</details>
<p>This document provides a comprehensive guide to setting up and running the AI-based Online Exam Proctoring System. It covers the primary entry points, system initialization, and basic usage patterns to help you get the system operational quickly.</p>
<p>The system can be run in two modes: as a standalone desktop application for direct proctoring or as a web-based service with user authentication and exam management. For detailed information about the AI detection modules, see <a href="/krishnakumaragrawal/Artificial-Intelligence-based-Online-Exam-Proctoring-System/4-ai-detection-engine" class="text-neutral-300 hover:text-neutral-200 hover:underline">AI Detection Engine</a>. For web interface configuration, see <a href="/krishnakumaragrawal/Artificial-Intelligence-based-Online-Exam-Proctoring-System/5-web-interface" class="text-neutral-300 hover:text-neutral-200 hover:underline">Web Interface</a>.</p>
<h2 id="prerequisites-and-system-requirements" class="group" data-header="true">Prerequisites and System Requirements<button class="relative ml-2 cursor-pointer align-baseline text-gray-400 opacity-0 transition-opacity hover:text-blue-400 focus:opacity-100 group-hover:opacity-100" aria-label="Copy link to header"><svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" fill="currentColor" viewBox="0 0 256 256" class="h-4 w-4"><path d="M117.18,188.74a12,12,0,0,1,0,17l-5.12,5.12A58.26,58.26,0,0,1,70.6,228h0A58.62,58.62,0,0,1,29.14,127.92L63.89,93.17a58.64,58.64,0,0,1,98.56,28.11,12,12,0,1,1-23.37,5.44,34.65,34.65,0,0,0-58.22-16.58L46.11,144.89A34.62,34.62,0,0,0,70.57,204h0a34.41,34.41,0,0,0,24.49-10.14l5.11-5.12A12,12,0,0,1,117.18,188.74ZM226.83,45.17a58.65,58.65,0,0,0-82.93,0l-5.11,5.11a12,12,0,0,0,17,17l5.12-5.12a34.63,34.63,0,1,1,49,49L175.1,145.86A34.39,34.39,0,0,1,150.61,156h0a34.63,34.63,0,0,1-33.69-26.72,12,12,0,0,0-23.38,5.44A58.64,58.64,0,0,0,150.56,180h.05a58.28,58.28,0,0,0,41.47-17.17l34.75-34.75a58.62,58.62,0,0,0,0-82.91Z"></path></svg></button></h2>
<p>Before running the system, ensure you have the following components installed and configured:</p>
<h3 id="required-dependencies" class="group" data-header="true">Required Dependencies<button class="relative ml-2 cursor-pointer align-baseline text-gray-400 opacity-0 transition-opacity hover:text-blue-400 focus:opacity-100 group-hover:opacity-100" aria-label="Copy link to header"><svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" fill="currentColor" viewBox="0 0 256 256" class="h-4 w-4"><path d="M117.18,188.74a12,12,0,0,1,0,17l-5.12,5.12A58.26,58.26,0,0,1,70.6,228h0A58.62,58.62,0,0,1,29.14,127.92L63.89,93.17a58.64,58.64,0,0,1,98.56,28.11,12,12,0,1,1-23.37,5.44,34.65,34.65,0,0,0-58.22-16.58L46.11,144.89A34.62,34.62,0,0,0,70.57,204h0a34.41,34.41,0,0,0,24.49-10.14l5.11-5.12A12,12,0,0,1,117.18,188.74ZM226.83,45.17a58.65,58.65,0,0,0-82.93,0l-5.11,5.11a12,12,0,0,0,17,17l5.12-5.12a34.63,34.63,0,1,1,49,49L175.1,145.86A34.39,34.39,0,0,1,150.61,156h0a34.63,34.63,0,0,1-33.69-26.72,12,12,0,0,0-23.38,5.44A58.64,58.64,0,0,0,150.56,180h.05a58.28,58.28,0,0,0,41.47-17.17l34.75-34.75a58.62,58.62,0,0,0,0-82.91Z"></path></svg></button></h3>
<p>The system requires several Python packages and external models:</p>








































<table><thead><tr><th>Component</th><th>Purpose</th><th>Used By</th></tr></thead><tbody><tr><td>OpenCV (<code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">cv2</code>)</td><td>Video capture and image processing</td><td><a href="https://github.com/krishnakumaragrawal/Artificial-Intelligence-based-Online-Exam-Proctoring-System/blob/2b4f4e85/app.py#L1-L1" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4]"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>app.py</span><span class="flex flex-shrink-0 items-center rounded-r border-l px-2 py-1.5 border-[#dddddd] bg-[#d8d8d8] text-[#666666] dark:border-[#333333] dark:bg-[#2a2a2a] dark:text-[#888888]">1</span></a> <a href="https://github.com/krishnakumaragrawal/Artificial-Intelligence-based-Online-Exam-Proctoring-System/blob/2b4f4e85/main.py#L1-L1" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4]"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>main.py</span><span class="flex flex-shrink-0 items-center rounded-r border-l px-2 py-1.5 border-[#dddddd] bg-[#d8d8d8] text-[#666666] dark:border-[#333333] dark:bg-[#2a2a2a] dark:text-[#888888]">1</span></a></td></tr><tr><td>dlib</td><td>Facial landmark detection</td><td>Multiple detection modules</td></tr><tr><td>Flask</td><td>Web server framework</td><td><a href="https://github.com/krishnakumaragrawal/Artificial-Intelligence-based-Online-Exam-Proctoring-System/blob/2b4f4e85/server.py#L1-L1" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4]"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>server.py</span><span class="flex flex-shrink-0 items-center rounded-r border-l px-2 py-1.5 border-[#dddddd] bg-[#d8d8d8] text-[#666666] dark:border-[#333333] dark:bg-[#2a2a2a] dark:text-[#888888]">1</span></a></td></tr><tr><td>MySQL</td><td>Database backend</td><td><a href="https://github.com/krishnakumaragrawal/Artificial-Intelligence-based-Online-Exam-Proctoring-System/blob/2b4f4e85/server.py#L3-L3" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4]"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>server.py</span><span class="flex flex-shrink-0 items-center rounded-r border-l px-2 py-1.5 border-[#dddddd] bg-[#d8d8d8] text-[#666666] dark:border-[#333333] dark:bg-[#2a2a2a] dark:text-[#888888]">3</span></a></td></tr><tr><td>YOLO models</td><td>Object detection</td><td><code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">detectObject()</code> function</td></tr><tr><td>Winsound</td><td>Audio alerts</td><td><a href="https://github.com/krishnakumaragrawal/Artificial-Intelligence-based-Online-Exam-Proctoring-System/blob/2b4f4e85/app.py#L10-L10" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4]"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>app.py</span><span class="flex flex-shrink-0 items-center rounded-r border-l px-2 py-1.5 border-[#dddddd] bg-[#d8d8d8] text-[#666666] dark:border-[#333333] dark:bg-[#2a2a2a] dark:text-[#888888]">10</span></a> <a href="https://github.com/krishnakumaragrawal/Artificial-Intelligence-based-Online-Exam-Proctoring-System/blob/2b4f4e85/main.py#L4-L4" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4]"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>main.py</span><span class="flex flex-shrink-0 items-center rounded-r border-l px-2 py-1.5 border-[#dddddd] bg-[#d8d8d8] text-[#666666] dark:border-[#333333] dark:bg-[#2a2a2a] dark:text-[#888888]">4</span></a></td></tr></tbody></table>
<h3 id="hardware-requirements" class="group" data-header="true">Hardware Requirements<button class="relative ml-2 cursor-pointer align-baseline text-gray-400 opacity-0 transition-opacity hover:text-blue-400 focus:opacity-100 group-hover:opacity-100" aria-label="Copy link to header"><svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" fill="currentColor" viewBox="0 0 256 256" class="h-4 w-4"><path d="M117.18,188.74a12,12,0,0,1,0,17l-5.12,5.12A58.26,58.26,0,0,1,70.6,228h0A58.62,58.62,0,0,1,29.14,127.92L63.89,93.17a58.64,58.64,0,0,1,98.56,28.11,12,12,0,1,1-23.37,5.44,34.65,34.65,0,0,0-58.22-16.58L46.11,144.89A34.62,34.62,0,0,0,70.57,204h0a34.41,34.41,0,0,0,24.49-10.14l5.11-5.12A12,12,0,0,1,117.18,188.74ZM226.83,45.17a58.65,58.65,0,0,0-82.93,0l-5.11,5.11a12,12,0,0,0,17,17l5.12-5.12a34.63,34.63,0,1,1,49,49L175.1,145.86A34.39,34.39,0,0,1,150.61,156h0a34.63,34.63,0,0,1-33.69-26.72,12,12,0,0,0-23.38,5.44A58.64,58.64,0,0,0,150.56,180h.05a58.28,58.28,0,0,0,41.47-17.17l34.75-34.75a58.62,58.62,0,0,0,0-82.91Z"></path></svg></button></h3>
<ul>
<li><strong>Webcam</strong>: Required for video capture via <code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">cv2.VideoCapture(0)</code> <a href="https://github.com/krishnakumaragrawal/Artificial-Intelligence-based-Online-Exam-Proctoring-System/blob/2b4f4e85/app.py#L21-L21" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4]"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>app.py</span><span class="flex flex-shrink-0 items-center rounded-r border-l px-2 py-1.5 border-[#dddddd] bg-[#d8d8d8] text-[#666666] dark:border-[#333333] dark:bg-[#2a2a2a] dark:text-[#888888]">21</span></a></li>
<li><strong>Microphone</strong>: Optional for audio detection capabilities</li>
<li><strong>Speakers</strong>: Required for audio alerts using <code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">winsound.Beep()</code> <a href="https://github.com/krishnakumaragrawal/Artificial-Intelligence-based-Online-Exam-Proctoring-System/blob/2b4f4e85/app.py#L32-L32" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4]"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>app.py</span><span class="flex flex-shrink-0 items-center rounded-r border-l px-2 py-1.5 border-[#dddddd] bg-[#d8d8d8] text-[#666666] dark:border-[#333333] dark:bg-[#2a2a2a] dark:text-[#888888]">32</span></a></li>
</ul>
<p><strong>Sources:</strong> <a href="https://github.com/krishnakumaragrawal/Artificial-Intelligence-based-Online-Exam-Proctoring-System/blob/2b4f4e85/app.py#L1-L26" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4]"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>app.py</span><span class="flex flex-shrink-0 items-center rounded-r border-l px-2 py-1.5 border-[#dddddd] bg-[#d8d8d8] text-[#666666] dark:border-[#333333] dark:bg-[#2a2a2a] dark:text-[#888888]">1-26</span></a> <a href="https://github.com/krishnakumaragrawal/Artificial-Intelligence-based-Online-Exam-Proctoring-System/blob/2b4f4e85/main.py#L1-L29" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4]"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>main.py</span><span class="flex flex-shrink-0 items-center rounded-r border-l px-2 py-1.5 border-[#dddddd] bg-[#d8d8d8] text-[#666666] dark:border-[#333333] dark:bg-[#2a2a2a] dark:text-[#888888]">1-29</span></a> <a href="https://github.com/krishnakumaragrawal/Artificial-Intelligence-based-Online-Exam-Proctoring-System/blob/2b4f4e85/server.py#L1-L9" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4]"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>server.py</span><span class="flex flex-shrink-0 items-center rounded-r border-l px-2 py-1.5 border-[#dddddd] bg-[#d8d8d8] text-[#666666] dark:border-[#333333] dark:bg-[#2a2a2a] dark:text-[#888888]">1-9</span></a></p>
<h2 id="system-entry-points" class="group" data-header="true">System Entry Points<button class="relative ml-2 cursor-pointer align-baseline text-gray-400 opacity-0 transition-opacity hover:text-blue-400 focus:opacity-100 group-hover:opacity-100" aria-label="Copy link to header"><svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" fill="currentColor" viewBox="0 0 256 256" class="h-4 w-4"><path d="M117.18,188.74a12,12,0,0,1,0,17l-5.12,5.12A58.26,58.26,0,0,1,70.6,228h0A58.62,58.62,0,0,1,29.14,127.92L63.89,93.17a58.64,58.64,0,0,1,98.56,28.11,12,12,0,1,1-23.37,5.44,34.65,34.65,0,0,0-58.22-16.58L46.11,144.89A34.62,34.62,0,0,0,70.57,204h0a34.41,34.41,0,0,0,24.49-10.14l5.11-5.12A12,12,0,0,1,117.18,188.74ZM226.83,45.17a58.65,58.65,0,0,0-82.93,0l-5.11,5.11a12,12,0,0,0,17,17l5.12-5.12a34.63,34.63,0,1,1,49,49L175.1,145.86A34.39,34.39,0,0,1,150.61,156h0a34.63,34.63,0,0,1-33.69-26.72,12,12,0,0,0-23.38,5.44A58.64,58.64,0,0,0,150.56,180h.05a58.28,58.28,0,0,0,41.47-17.17l34.75-34.75a58.62,58.62,0,0,0,0-82.91Z"></path></svg></button></h2>
<p>The system provides two primary entry points depending on your usage requirements:</p>
<h3 id="entry-point-comparison" class="group" data-header="true">Entry Point Comparison<button class="relative ml-2 cursor-pointer align-baseline text-gray-400 opacity-0 transition-opacity hover:text-blue-400 focus:opacity-100 group-hover:opacity-100" aria-label="Copy link to header"><svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" fill="currentColor" viewBox="0 0 256 256" class="h-4 w-4"><path d="M117.18,188.74a12,12,0,0,1,0,17l-5.12,5.12A58.26,58.26,0,0,1,70.6,228h0A58.62,58.62,0,0,1,29.14,127.92L63.89,93.17a58.64,58.64,0,0,1,98.56,28.11,12,12,0,1,1-23.37,5.44,34.65,34.65,0,0,0-58.22-16.58L46.11,144.89A34.62,34.62,0,0,0,70.57,204h0a34.41,34.41,0,0,0,24.49-10.14l5.11-5.12A12,12,0,0,1,117.18,188.74ZM226.83,45.17a58.65,58.65,0,0,0-82.93,0l-5.11,5.11a12,12,0,0,0,17,17l5.12-5.12a34.63,34.63,0,1,1,49,49L175.1,145.86A34.39,34.39,0,0,1,150.61,156h0a34.63,34.63,0,0,1-33.69-26.72,12,12,0,0,0-23.38,5.44A58.64,58.64,0,0,0,150.56,180h.05a58.28,58.28,0,0,0,41.47-17.17l34.75-34.75a58.62,58.62,0,0,0,0-82.91Z"></path></svg></button></h3>
<pre class="px-2 py-1.5 has-[code]:rounded-md has-[code]:!bg-[#e5e5e5] has-[div]:bg-transparent has-[div]:!p-0 has-[code]:text-stone-900 dark:has-[code]:!bg-[#242424] has-[code]:dark:text-white [&amp;_code]:block [&amp;_code]:border-none [&amp;_code]:bg-transparent [&amp;_code]:p-0"><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><!--/$--></pre>
<h3 id="standalone-desktop-application" class="group" data-header="true">Standalone Desktop Application<button class="relative ml-2 cursor-pointer align-baseline text-gray-400 opacity-0 transition-opacity hover:text-blue-400 focus:opacity-100 group-hover:opacity-100" aria-label="Copy link to header"><svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" fill="currentColor" viewBox="0 0 256 256" class="h-4 w-4"><path d="M117.18,188.74a12,12,0,0,1,0,17l-5.12,5.12A58.26,58.26,0,0,1,70.6,228h0A58.62,58.62,0,0,1,29.14,127.92L63.89,93.17a58.64,58.64,0,0,1,98.56,28.11,12,12,0,1,1-23.37,5.44,34.65,34.65,0,0,0-58.22-16.58L46.11,144.89A34.62,34.62,0,0,0,70.57,204h0a34.41,34.41,0,0,0,24.49-10.14l5.11-5.12A12,12,0,0,1,117.18,188.74ZM226.83,45.17a58.65,58.65,0,0,0-82.93,0l-5.11,5.11a12,12,0,0,0,17,17l5.12-5.12a34.63,34.63,0,1,1,49,49L175.1,145.86A34.39,34.39,0,0,1,150.61,156h0a34.63,34.63,0,0,1-33.69-26.72,12,12,0,0,0-23.38,5.44A58.64,58.64,0,0,0,150.56,180h.05a58.28,58.28,0,0,0,41.47-17.17l34.75-34.75a58.62,58.62,0,0,0,0-82.91Z"></path></svg></button></h3>
<p>To run the system as a standalone desktop application:</p>
<pre class="px-2 py-1.5 has-[code]:rounded-md has-[code]:!bg-[#e5e5e5] has-[div]:bg-transparent has-[div]:!p-0 has-[code]:text-stone-900 dark:has-[code]:!bg-[#242424] has-[code]:dark:text-white [&amp;_code]:block [&amp;_code]:border-none [&amp;_code]:bg-transparent [&amp;_code]:p-0"><div class="rounded-sm border border-[#8F8F8F]/30 p-2 text-xs font-normal leading-[15px]" style="background-color:transparent;min-height:2em"></div></pre>
<p>This mode initializes the camera directly and runs the proctoring algorithm with a local OpenCV window display. The <code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">proctoringAlgo()</code> function <a href="https://github.com/krishnakumaragrawal/Artificial-Intelligence-based-Online-Exam-Proctoring-System/blob/2b4f4e85/app.py#L43-L43" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4]"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>app.py</span><span class="flex flex-shrink-0 items-center rounded-r border-l px-2 py-1.5 border-[#dddddd] bg-[#d8d8d8] text-[#666666] dark:border-[#333333] dark:bg-[#2a2a2a] dark:text-[#888888]">43</span></a> will:</p>
<ol>
<li>Initialize <code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">cv2.VideoCapture(0)</code> for webcam access <a href="https://github.com/krishnakumaragrawal/Artificial-Intelligence-based-Online-Exam-Proctoring-System/blob/2b4f4e85/app.py#L21-L21" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4]"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>app.py</span><span class="flex flex-shrink-0 items-center rounded-r border-l px-2 py-1.5 border-[#dddddd] bg-[#d8d8d8] text-[#666666] dark:border-[#333333] dark:bg-[#2a2a2a] dark:text-[#888888]">21</span></a></li>
<li>Run continuous monitoring loop <a href="https://github.com/krishnakumaragrawal/Artificial-Intelligence-based-Online-Exam-Proctoring-System/blob/2b4f4e85/app.py#L47-L47" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4]"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>app.py</span><span class="flex flex-shrink-0 items-center rounded-r border-l px-2 py-1.5 border-[#dddddd] bg-[#d8d8d8] text-[#666666] dark:border-[#333333] dark:bg-[#2a2a2a] dark:text-[#888888]">47</span></a></li>
<li>Display real-time video feed via <code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">cv2.imshow(&#x27;Frame&#x27;, frame)</code> <a href="https://github.com/krishnakumaragrawal/Artificial-Intelligence-based-Online-Exam-Proctoring-System/blob/2b4f4e85/app.py#L109-L109" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4]"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>app.py</span><span class="flex flex-shrink-0 items-center rounded-r border-l px-2 py-1.5 border-[#dddddd] bg-[#d8d8d8] text-[#666666] dark:border-[#333333] dark:bg-[#2a2a2a] dark:text-[#888888]">109</span></a></li>
<li>Save activity data to <code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">activity.txt</code> <a href="https://github.com/krishnakumaragrawal/Artificial-Intelligence-based-Online-Exam-Proctoring-System/blob/2b4f4e85/app.py#L124-L125" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4]"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>app.py</span><span class="flex flex-shrink-0 items-center rounded-r border-l px-2 py-1.5 border-[#dddddd] bg-[#d8d8d8] text-[#666666] dark:border-[#333333] dark:bg-[#2a2a2a] dark:text-[#888888]">124-125</span></a></li>
</ol>
<h3 id="web-service-mode" class="group" data-header="true">Web Service Mode<button class="relative ml-2 cursor-pointer align-baseline text-gray-400 opacity-0 transition-opacity hover:text-blue-400 focus:opacity-100 group-hover:opacity-100" aria-label="Copy link to header"><svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" fill="currentColor" viewBox="0 0 256 256" class="h-4 w-4"><path d="M117.18,188.74a12,12,0,0,1,0,17l-5.12,5.12A58.26,58.26,0,0,1,70.6,228h0A58.62,58.62,0,0,1,29.14,127.92L63.89,93.17a58.64,58.64,0,0,1,98.56,28.11,12,12,0,1,1-23.37,5.44,34.65,34.65,0,0,0-58.22-16.58L46.11,144.89A34.62,34.62,0,0,0,70.57,204h0a34.41,34.41,0,0,0,24.49-10.14l5.11-5.12A12,12,0,0,1,117.18,188.74ZM226.83,45.17a58.65,58.65,0,0,0-82.93,0l-5.11,5.11a12,12,0,0,0,17,17l5.12-5.12a34.63,34.63,0,1,1,49,49L175.1,145.86A34.39,34.39,0,0,1,150.61,156h0a34.63,34.63,0,0,1-33.69-26.72,12,12,0,0,0-23.38,5.44A58.64,58.64,0,0,0,150.56,180h.05a58.28,58.28,0,0,0,41.47-17.17l34.75-34.75a58.62,58.62,0,0,0,0-82.91Z"></path></svg></button></h3>
<p>To run the system as a web service with authentication:</p>
<pre class="px-2 py-1.5 has-[code]:rounded-md has-[code]:!bg-[#e5e5e5] has-[div]:bg-transparent has-[div]:!p-0 has-[code]:text-stone-900 dark:has-[code]:!bg-[#242424] has-[code]:dark:text-white [&amp;_code]:block [&amp;_code]:border-none [&amp;_code]:bg-transparent [&amp;_code]:p-0"><div class="rounded-sm border border-[#8F8F8F]/30 p-2 text-xs font-normal leading-[15px]" style="background-color:transparent;min-height:2em"></div></pre>
<p>This starts the Flask development server on port 5000 <a href="https://github.com/krishnakumaragrawal/Artificial-Intelligence-based-Online-Exam-Proctoring-System/blob/2b4f4e85/server.py#L70-L70" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4]"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>server.py</span><span class="flex flex-shrink-0 items-center rounded-r border-l px-2 py-1.5 border-[#dddddd] bg-[#d8d8d8] text-[#666666] dark:border-[#333333] dark:bg-[#2a2a2a] dark:text-[#888888]">70</span></a> The web service provides these endpoints:</p>








































<table><thead><tr><th>Route</th><th>Purpose</th><th>Function</th></tr></thead><tbody><tr><td><code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">/</code></td><td>Main login page</td><td><code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">index_page()</code> <a href="https://github.com/krishnakumaragrawal/Artificial-Intelligence-based-Online-Exam-Proctoring-System/blob/2b4f4e85/server.py#L40-L40" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4]"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>server.py</span><span class="flex flex-shrink-0 items-center rounded-r border-l px-2 py-1.5 border-[#dddddd] bg-[#d8d8d8] text-[#666666] dark:border-[#333333] dark:bg-[#2a2a2a] dark:text-[#888888]">40</span></a></td></tr><tr><td><code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">/quiz_html</code></td><td>Exam interface</td><td><code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">quix_page()</code> <a href="https://github.com/krishnakumaragrawal/Artificial-Intelligence-based-Online-Exam-Proctoring-System/blob/2b4f4e85/server.py#L46-L46" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4]"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>server.py</span><span class="flex flex-shrink-0 items-center rounded-r border-l px-2 py-1.5 border-[#dddddd] bg-[#d8d8d8] text-[#666666] dark:border-[#333333] dark:bg-[#2a2a2a] dark:text-[#888888]">46</span></a></td></tr><tr><td><code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">/video_feed</code></td><td>Video streaming</td><td><code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">video_feed()</code> <a href="https://github.com/krishnakumaragrawal/Artificial-Intelligence-based-Online-Exam-Proctoring-System/blob/2b4f4e85/server.py#L52-L52" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4]"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>server.py</span><span class="flex flex-shrink-0 items-center rounded-r border-l px-2 py-1.5 border-[#dddddd] bg-[#d8d8d8] text-[#666666] dark:border-[#333333] dark:bg-[#2a2a2a] dark:text-[#888888]">52</span></a></td></tr><tr><td><code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">/signup_data</code></td><td>User registration</td><td><code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">signup_data()</code> <a href="https://github.com/krishnakumaragrawal/Artificial-Intelligence-based-Online-Exam-Proctoring-System/blob/2b4f4e85/server.py#L16-L16" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4]"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>server.py</span><span class="flex flex-shrink-0 items-center rounded-r border-l px-2 py-1.5 border-[#dddddd] bg-[#d8d8d8] text-[#666666] dark:border-[#333333] dark:bg-[#2a2a2a] dark:text-[#888888]">16</span></a></td></tr><tr><td><code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">/login_data</code></td><td>User authentication</td><td><code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">login_data()</code> <a href="https://github.com/krishnakumaragrawal/Artificial-Intelligence-based-Online-Exam-Proctoring-System/blob/2b4f4e85/server.py#L28-L28" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4]"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>server.py</span><span class="flex flex-shrink-0 items-center rounded-r border-l px-2 py-1.5 border-[#dddddd] bg-[#d8d8d8] text-[#666666] dark:border-[#333333] dark:bg-[#2a2a2a] dark:text-[#888888]">28</span></a></td></tr><tr><td><code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">/stop_camera</code></td><td>System shutdown</td><td><code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">stop_camera()</code> <a href="https://github.com/krishnakumaragrawal/Artificial-Intelligence-based-Online-Exam-Proctoring-System/blob/2b4f4e85/server.py#L58-L58" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4]"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>server.py</span><span class="flex flex-shrink-0 items-center rounded-r border-l px-2 py-1.5 border-[#dddddd] bg-[#d8d8d8] text-[#666666] dark:border-[#333333] dark:bg-[#2a2a2a] dark:text-[#888888]">58</span></a></td></tr></tbody></table>
<p><strong>Sources:</strong> <a href="https://github.com/krishnakumaragrawal/Artificial-Intelligence-based-Online-Exam-Proctoring-System/blob/2b4f4e85/app.py#L117-L118" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4]"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>app.py</span><span class="flex flex-shrink-0 items-center rounded-r border-l px-2 py-1.5 border-[#dddddd] bg-[#d8d8d8] text-[#666666] dark:border-[#333333] dark:bg-[#2a2a2a] dark:text-[#888888]">117-118</span></a> <a href="https://github.com/krishnakumaragrawal/Artificial-Intelligence-based-Online-Exam-Proctoring-System/blob/2b4f4e85/server.py#L38-L70" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4]"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>server.py</span><span class="flex flex-shrink-0 items-center rounded-r border-l px-2 py-1.5 border-[#dddddd] bg-[#d8d8d8] text-[#666666] dark:border-[#333333] dark:bg-[#2a2a2a] dark:text-[#888888]">38-70</span></a> <a href="https://github.com/krishnakumaragrawal/Artificial-Intelligence-based-Online-Exam-Proctoring-System/blob/2b4f4e85/main.py#L131-L140" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4]"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>main.py</span><span class="flex flex-shrink-0 items-center rounded-r border-l px-2 py-1.5 border-[#dddddd] bg-[#d8d8d8] text-[#666666] dark:border-[#333333] dark:bg-[#2a2a2a] dark:text-[#888888]">131-140</span></a></p>
<h2 id="basic-usage-workflow" class="group" data-header="true">Basic Usage Workflow<button class="relative ml-2 cursor-pointer align-baseline text-gray-400 opacity-0 transition-opacity hover:text-blue-400 focus:opacity-100 group-hover:opacity-100" aria-label="Copy link to header"><svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" fill="currentColor" viewBox="0 0 256 256" class="h-4 w-4"><path d="M117.18,188.74a12,12,0,0,1,0,17l-5.12,5.12A58.26,58.26,0,0,1,70.6,228h0A58.62,58.62,0,0,1,29.14,127.92L63.89,93.17a58.64,58.64,0,0,1,98.56,28.11,12,12,0,1,1-23.37,5.44,34.65,34.65,0,0,0-58.22-16.58L46.11,144.89A34.62,34.62,0,0,0,70.57,204h0a34.41,34.41,0,0,0,24.49-10.14l5.11-5.12A12,12,0,0,1,117.18,188.74ZM226.83,45.17a58.65,58.65,0,0,0-82.93,0l-5.11,5.11a12,12,0,0,0,17,17l5.12-5.12a34.63,34.63,0,1,1,49,49L175.1,145.86A34.39,34.39,0,0,1,150.61,156h0a34.63,34.63,0,0,1-33.69-26.72,12,12,0,0,0-23.38,5.44A58.64,58.64,0,0,0,150.56,180h.05a58.28,58.28,0,0,0,41.47-17.17l34.75-34.75a58.62,58.62,0,0,0,0-82.91Z"></path></svg></button></h2>
<h3 id="system-initialization-flow" class="group" data-header="true">System Initialization Flow<button class="relative ml-2 cursor-pointer align-baseline text-gray-400 opacity-0 transition-opacity hover:text-blue-400 focus:opacity-100 group-hover:opacity-100" aria-label="Copy link to header"><svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" fill="currentColor" viewBox="0 0 256 256" class="h-4 w-4"><path d="M117.18,188.74a12,12,0,0,1,0,17l-5.12,5.12A58.26,58.26,0,0,1,70.6,228h0A58.62,58.62,0,0,1,29.14,127.92L63.89,93.17a58.64,58.64,0,0,1,98.56,28.11,12,12,0,1,1-23.37,5.44,34.65,34.65,0,0,0-58.22-16.58L46.11,144.89A34.62,34.62,0,0,0,70.57,204h0a34.41,34.41,0,0,0,24.49-10.14l5.11-5.12A12,12,0,0,1,117.18,188.74ZM226.83,45.17a58.65,58.65,0,0,0-82.93,0l-5.11,5.11a12,12,0,0,0,17,17l5.12-5.12a34.63,34.63,0,1,1,49,49L175.1,145.86A34.39,34.39,0,0,1,150.61,156h0a34.63,34.63,0,0,1-33.69-26.72,12,12,0,0,0-23.38,5.44A58.64,58.64,0,0,0,150.56,180h.05a58.28,58.28,0,0,0,41.47-17.17l34.75-34.75a58.62,58.62,0,0,0,0-82.91Z"></path></svg></button></h3>
<pre class="px-2 py-1.5 has-[code]:rounded-md has-[code]:!bg-[#e5e5e5] has-[div]:bg-transparent has-[div]:!p-0 has-[code]:text-stone-900 dark:has-[code]:!bg-[#242424] has-[code]:dark:text-white [&amp;_code]:block [&amp;_code]:border-none [&amp;_code]:bg-transparent [&amp;_code]:p-0"><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><!--/$--></pre>
<h3 id="core-monitoring-loop" class="group" data-header="true">Core Monitoring Loop<button class="relative ml-2 cursor-pointer align-baseline text-gray-400 opacity-0 transition-opacity hover:text-blue-400 focus:opacity-100 group-hover:opacity-100" aria-label="Copy link to header"><svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" fill="currentColor" viewBox="0 0 256 256" class="h-4 w-4"><path d="M117.18,188.74a12,12,0,0,1,0,17l-5.12,5.12A58.26,58.26,0,0,1,70.6,228h0A58.62,58.62,0,0,1,29.14,127.92L63.89,93.17a58.64,58.64,0,0,1,98.56,28.11,12,12,0,1,1-23.37,5.44,34.65,34.65,0,0,0-58.22-16.58L46.11,144.89A34.62,34.62,0,0,0,70.57,204h0a34.41,34.41,0,0,0,24.49-10.14l5.11-5.12A12,12,0,0,1,117.18,188.74ZM226.83,45.17a58.65,58.65,0,0,0-82.93,0l-5.11,5.11a12,12,0,0,0,17,17l5.12-5.12a34.63,34.63,0,1,1,49,49L175.1,145.86A34.39,34.39,0,0,1,150.61,156h0a34.63,34.63,0,0,1-33.69-26.72,12,12,0,0,0-23.38,5.44A58.64,58.64,0,0,0,150.56,180h.05a58.28,58.28,0,0,0,41.47-17.17l34.75-34.75a58.62,58.62,0,0,0,0-82.91Z"></path></svg></button></h3>
<p>The heart of the system is the monitoring loop in <code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">proctoringAlgo()</code> <a href="https://github.com/krishnakumaragrawal/Artificial-Intelligence-based-Online-Exam-Proctoring-System/blob/2b4f4e85/app.py#L47-L47" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4]"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>app.py</span><span class="flex flex-shrink-0 items-center rounded-r border-l px-2 py-1.5 border-[#dddddd] bg-[#d8d8d8] text-[#666666] dark:border-[#333333] dark:bg-[#2a2a2a] dark:text-[#888888]">47</span></a> which performs these operations each frame:</p>
<ol>
<li><strong>Timestamp Recording</strong>: Captures current time using <code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">datetime.now().strftime(&quot;%H:%M:%S.%f&quot;)</code> <a href="https://github.com/krishnakumaragrawal/Artificial-Intelligence-based-Online-Exam-Proctoring-System/blob/2b4f4e85/app.py#L54-L54" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4]"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>app.py</span><span class="flex flex-shrink-0 items-center rounded-r border-l px-2 py-1.5 border-[#dddddd] bg-[#d8d8d8] text-[#666666] dark:border-[#333333] dark:bg-[#2a2a2a] dark:text-[#888888]">54</span></a></li>
<li><strong>Face Detection</strong>: Calls <code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">detectFace(frame)</code> returning <code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">faceCount</code> and <code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">faces</code> <a href="https://github.com/krishnakumaragrawal/Artificial-Intelligence-based-Online-Exam-Proctoring-System/blob/2b4f4e85/app.py#L59-L59" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4]"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>app.py</span><span class="flex flex-shrink-0 items-center rounded-r border-l px-2 py-1.5 border-[#dddddd] bg-[#d8d8d8] text-[#666666] dark:border-[#333333] dark:bg-[#2a2a2a] dark:text-[#888888]">59</span></a></li>
<li><strong>Face Validation</strong>: Processes face count through <code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">faceCount_detection()</code> <a href="https://github.com/krishnakumaragrawal/Artificial-Intelligence-based-Online-Exam-Proctoring-System/blob/2b4f4e85/app.py#L60-L60" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4]"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>app.py</span><span class="flex flex-shrink-0 items-center rounded-r border-l px-2 py-1.5 border-[#dddddd] bg-[#d8d8d8] text-[#666666] dark:border-[#333333] dark:bg-[#2a2a2a] dark:text-[#888888]">60</span></a></li>
<li><strong>Multi-modal Analysis</strong> (if exactly 1 face detected):
<ul>
<li>Blink detection via <code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">isBlinking(faces, frame)</code> <a href="https://github.com/krishnakumaragrawal/Artificial-Intelligence-based-Online-Exam-Proctoring-System/blob/2b4f4e85/app.py#L67-L67" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4]"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>app.py</span><span class="flex flex-shrink-0 items-center rounded-r border-l px-2 py-1.5 border-[#dddddd] bg-[#d8d8d8] text-[#666666] dark:border-[#333333] dark:bg-[#2a2a2a] dark:text-[#888888]">67</span></a></li>
<li>Gaze tracking via <code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">gazeDetection(faces, frame)</code> <a href="https://github.com/krishnakumaragrawal/Artificial-Intelligence-based-Online-Exam-Proctoring-System/blob/2b4f4e85/app.py#L78-L78" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4]"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>app.py</span><span class="flex flex-shrink-0 items-center rounded-r border-l px-2 py-1.5 border-[#dddddd] bg-[#d8d8d8] text-[#666666] dark:border-[#333333] dark:bg-[#2a2a2a] dark:text-[#888888]">78</span></a></li>
<li>Mouth tracking via <code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">mouthTrack(faces, frame)</code> <a href="https://github.com/krishnakumaragrawal/Artificial-Intelligence-based-Online-Exam-Proctoring-System/blob/2b4f4e85/app.py#L83-L83" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4]"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>app.py</span><span class="flex flex-shrink-0 items-center rounded-r border-l px-2 py-1.5 border-[#dddddd] bg-[#d8d8d8] text-[#666666] dark:border-[#333333] dark:bg-[#2a2a2a] dark:text-[#888888]">83</span></a></li>
<li>Object detection via <code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">detectObject(frame)</code> <a href="https://github.com/krishnakumaragrawal/Artificial-Intelligence-based-Online-Exam-Proctoring-System/blob/2b4f4e85/app.py#L87-L87" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4]"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>app.py</span><span class="flex flex-shrink-0 items-center rounded-r border-l px-2 py-1.5 border-[#dddddd] bg-[#d8d8d8] text-[#666666] dark:border-[#333333] dark:bg-[#2a2a2a] dark:text-[#888888]">87</span></a></li>
<li>Head pose estimation via <code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">head_pose_detection(faces, frame)</code> <a href="https://github.com/krishnakumaragrawal/Artificial-Intelligence-based-Online-Exam-Proctoring-System/blob/2b4f4e85/app.py#L97-L97" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4]"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>app.py</span><span class="flex flex-shrink-0 items-center rounded-r border-l px-2 py-1.5 border-[#dddddd] bg-[#d8d8d8] text-[#666666] dark:border-[#333333] dark:bg-[#2a2a2a] dark:text-[#888888]">97</span></a></li>
</ul>
</li>
</ol>
<p><strong>Sources:</strong> <a href="https://github.com/krishnakumaragrawal/Artificial-Intelligence-based-Online-Exam-Proctoring-System/blob/2b4f4e85/app.py#L43-L115" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4]"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>app.py</span><span class="flex flex-shrink-0 items-center rounded-r border-l px-2 py-1.5 border-[#dddddd] bg-[#d8d8d8] text-[#666666] dark:border-[#333333] dark:bg-[#2a2a2a] dark:text-[#888888]">43-115</span></a> <a href="https://github.com/krishnakumaragrawal/Artificial-Intelligence-based-Online-Exam-Proctoring-System/blob/2b4f4e85/main.py#L47-L127" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4]"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>main.py</span><span class="flex flex-shrink-0 items-center rounded-r border-l px-2 py-1.5 border-[#dddddd] bg-[#d8d8d8] text-[#666666] dark:border-[#333333] dark:bg-[#2a2a2a] dark:text-[#888888]">47-127</span></a></p>
<h2 id="understanding-system-output" class="group" data-header="true">Understanding System Output<button class="relative ml-2 cursor-pointer align-baseline text-gray-400 opacity-0 transition-opacity hover:text-blue-400 focus:opacity-100 group-hover:opacity-100" aria-label="Copy link to header"><svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" fill="currentColor" viewBox="0 0 256 256" class="h-4 w-4"><path d="M117.18,188.74a12,12,0,0,1,0,17l-5.12,5.12A58.26,58.26,0,0,1,70.6,228h0A58.62,58.62,0,0,1,29.14,127.92L63.89,93.17a58.64,58.64,0,0,1,98.56,28.11,12,12,0,1,1-23.37,5.44,34.65,34.65,0,0,0-58.22-16.58L46.11,144.89A34.62,34.62,0,0,0,70.57,204h0a34.41,34.41,0,0,0,24.49-10.14l5.11-5.12A12,12,0,0,1,117.18,188.74ZM226.83,45.17a58.65,58.65,0,0,0-82.93,0l-5.11,5.11a12,12,0,0,0,17,17l5.12-5.12a34.63,34.63,0,1,1,49,49L175.1,145.86A34.39,34.39,0,0,1,150.61,156h0a34.63,34.63,0,0,1-33.69-26.72,12,12,0,0,0-23.38,5.44A58.64,58.64,0,0,0,150.56,180h.05a58.28,58.28,0,0,0,41.47-17.17l34.75-34.75a58.62,58.62,0,0,0,0-82.91Z"></path></svg></button></h2>
<h3 id="activity-logging-structure" class="group" data-header="true">Activity Logging Structure<button class="relative ml-2 cursor-pointer align-baseline text-gray-400 opacity-0 transition-opacity hover:text-blue-400 focus:opacity-100 group-hover:opacity-100" aria-label="Copy link to header"><svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" fill="currentColor" viewBox="0 0 256 256" class="h-4 w-4"><path d="M117.18,188.74a12,12,0,0,1,0,17l-5.12,5.12A58.26,58.26,0,0,1,70.6,228h0A58.62,58.62,0,0,1,29.14,127.92L63.89,93.17a58.64,58.64,0,0,1,98.56,28.11,12,12,0,1,1-23.37,5.44,34.65,34.65,0,0,0-58.22-16.58L46.11,144.89A34.62,34.62,0,0,0,70.57,204h0a34.41,34.41,0,0,0,24.49-10.14l5.11-5.12A12,12,0,0,1,117.18,188.74ZM226.83,45.17a58.65,58.65,0,0,0-82.93,0l-5.11,5.11a12,12,0,0,0,17,17l5.12-5.12a34.63,34.63,0,1,1,49,49L175.1,145.86A34.39,34.39,0,0,1,150.61,156h0a34.63,34.63,0,0,1-33.69-26.72,12,12,0,0,0-23.38,5.44A58.64,58.64,0,0,0,150.56,180h.05a58.28,58.28,0,0,0,41.47-17.17l34.75-34.75a58.62,58.62,0,0,0,0-82.91Z"></path></svg></button></h3>
<p>The system maintains a global <code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">data_record</code> list <a href="https://github.com/krishnakumaragrawal/Artificial-Intelligence-based-Online-Exam-Proctoring-System/blob/2b4f4e85/app.py#L14-L14" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4]"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>app.py</span><span class="flex flex-shrink-0 items-center rounded-r border-l px-2 py-1.5 border-[#dddddd] bg-[#d8d8d8] text-[#666666] dark:border-[#333333] dark:bg-[#2a2a2a] dark:text-[#888888]">14</span></a> that stores timestamped activity entries. Each record contains:</p>
<pre class="px-2 py-1.5 has-[code]:rounded-md has-[code]:!bg-[#e5e5e5] has-[div]:bg-transparent has-[div]:!p-0 has-[code]:text-stone-900 dark:has-[code]:!bg-[#242424] has-[code]:dark:text-white [&amp;_code]:block [&amp;_code]:border-none [&amp;_code]:bg-transparent [&amp;_code]:p-0"><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><!--/$--></pre>
<h3 id="alert-system" class="group" data-header="true">Alert System<button class="relative ml-2 cursor-pointer align-baseline text-gray-400 opacity-0 transition-opacity hover:text-blue-400 focus:opacity-100 group-hover:opacity-100" aria-label="Copy link to header"><svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" fill="currentColor" viewBox="0 0 256 256" class="h-4 w-4"><path d="M117.18,188.74a12,12,0,0,1,0,17l-5.12,5.12A58.26,58.26,0,0,1,70.6,228h0A58.62,58.62,0,0,1,29.14,127.92L63.89,93.17a58.64,58.64,0,0,1,98.56,28.11,12,12,0,1,1-23.37,5.44,34.65,34.65,0,0,0-58.22-16.58L46.11,144.89A34.62,34.62,0,0,0,70.57,204h0a34.41,34.41,0,0,0,24.49-10.14l5.11-5.12A12,12,0,0,1,117.18,188.74ZM226.83,45.17a58.65,58.65,0,0,0-82.93,0l-5.11,5.11a12,12,0,0,0,17,17l5.12-5.12a34.63,34.63,0,1,1,49,49L175.1,145.86A34.39,34.39,0,0,1,150.61,156h0a34.63,34.63,0,0,1-33.69-26.72,12,12,0,0,0-23.38,5.44A58.64,58.64,0,0,0,150.56,180h.05a58.28,58.28,0,0,0,41.47-17.17l34.75-34.75a58.62,58.62,0,0,0,0-82.91Z"></path></svg></button></h3>
<p>The system uses audio alerts via <code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">winsound.Beep(frequency, duration)</code> with these parameters <a href="https://github.com/krishnakumaragrawal/Artificial-Intelligence-based-Online-Exam-Proctoring-System/blob/2b4f4e85/app.py#L17-L18" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4]"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>app.py</span><span class="flex flex-shrink-0 items-center rounded-r border-l px-2 py-1.5 border-[#dddddd] bg-[#d8d8d8] text-[#666666] dark:border-[#333333] dark:bg-[#2a2a2a] dark:text-[#888888]">17-18</span></a>:</p>
<ul>
<li><strong>Frequency</strong>: 2500 Hz</li>
<li><strong>Duration</strong>: 1000 ms</li>
</ul>
<p>Alerts are triggered when:</p>
<ul>
<li>Multiple faces detected (<code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">faceCount &gt; 1</code>) <a href="https://github.com/krishnakumaragrawal/Artificial-Intelligence-based-Online-Exam-Proctoring-System/blob/2b4f4e85/app.py#L29-L32" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4]"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>app.py</span><span class="flex flex-shrink-0 items-center rounded-r border-l px-2 py-1.5 border-[#dddddd] bg-[#d8d8d8] text-[#666666] dark:border-[#333333] dark:bg-[#2a2a2a] dark:text-[#888888]">29-32</span></a></li>
<li>No face detected (<code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">faceCount == 0</code>) <a href="https://github.com/krishnakumaragrawal/Artificial-Intelligence-based-Online-Exam-Proctoring-System/blob/2b4f4e85/app.py#L33-L36" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4]"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>app.py</span><span class="flex flex-shrink-0 items-center rounded-r border-l px-2 py-1.5 border-[#dddddd] bg-[#d8d8d8] text-[#666666] dark:border-[#333333] dark:bg-[#2a2a2a] dark:text-[#888888]">33-36</span></a></li>
<li>Multiple objects detected (<code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">len(objectName) &gt; 1</code>) <a href="https://github.com/krishnakumaragrawal/Artificial-Intelligence-based-Online-Exam-Proctoring-System/blob/2b4f4e85/app.py#L91-L94" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4]"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>app.py</span><span class="flex flex-shrink-0 items-center rounded-r border-l px-2 py-1.5 border-[#dddddd] bg-[#d8d8d8] text-[#666666] dark:border-[#333333] dark:bg-[#2a2a2a] dark:text-[#888888]">91-94</span></a></li>
</ul>
<h3 id="output-files" class="group" data-header="true">Output Files<button class="relative ml-2 cursor-pointer align-baseline text-gray-400 opacity-0 transition-opacity hover:text-blue-400 focus:opacity-100 group-hover:opacity-100" aria-label="Copy link to header"><svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" fill="currentColor" viewBox="0 0 256 256" class="h-4 w-4"><path d="M117.18,188.74a12,12,0,0,1,0,17l-5.12,5.12A58.26,58.26,0,0,1,70.6,228h0A58.62,58.62,0,0,1,29.14,127.92L63.89,93.17a58.64,58.64,0,0,1,98.56,28.11,12,12,0,1,1-23.37,5.44,34.65,34.65,0,0,0-58.22-16.58L46.11,144.89A34.62,34.62,0,0,0,70.57,204h0a34.41,34.41,0,0,0,24.49-10.14l5.11-5.12A12,12,0,0,1,117.18,188.74ZM226.83,45.17a58.65,58.65,0,0,0-82.93,0l-5.11,5.11a12,12,0,0,0,17,17l5.12-5.12a34.63,34.63,0,1,1,49,49L175.1,145.86A34.39,34.39,0,0,1,150.61,156h0a34.63,34.63,0,0,1-33.69-26.72,12,12,0,0,0-23.38,5.44A58.64,58.64,0,0,0,150.56,180h.05a58.28,58.28,0,0,0,41.47-17.17l34.75-34.75a58.62,58.62,0,0,0,0-82.91Z"></path></svg></button></h3>
<p>Upon system termination, activity data is written to <code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">activity.txt</code> <a href="https://github.com/krishnakumaragrawal/Artificial-Intelligence-based-Online-Exam-Proctoring-System/blob/2b4f4e85/app.py#L124-L125" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4]"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>app.py</span><span class="flex flex-shrink-0 items-center rounded-r border-l px-2 py-1.5 border-[#dddddd] bg-[#d8d8d8] text-[#666666] dark:border-[#333333] dark:bg-[#2a2a2a] dark:text-[#888888]">124-125</span></a> using:</p>
<pre class="px-2 py-1.5 has-[code]:rounded-md has-[code]:!bg-[#e5e5e5] has-[div]:bg-transparent has-[div]:!p-0 has-[code]:text-stone-900 dark:has-[code]:!bg-[#242424] has-[code]:dark:text-white [&amp;_code]:block [&amp;_code]:border-none [&amp;_code]:bg-transparent [&amp;_code]:p-0"><div class="rounded-sm border border-[#8F8F8F]/30 p-2 text-xs font-normal leading-[15px]" style="background-color:transparent;min-height:2em"></div></pre>
<p><strong>Sources:</strong> <a href="https://github.com/krishnakumaragrawal/Artificial-Intelligence-based-Online-Exam-Proctoring-System/blob/2b4f4e85/app.py#L13-L14" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4]"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>app.py</span><span class="flex flex-shrink-0 items-center rounded-r border-l px-2 py-1.5 border-[#dddddd] bg-[#d8d8d8] text-[#666666] dark:border-[#333333] dark:bg-[#2a2a2a] dark:text-[#888888]">13-14</span></a> <a href="https://github.com/krishnakumaragrawal/Artificial-Intelligence-based-Online-Exam-Proctoring-System/blob/2b4f4e85/app.py#L28-L39" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4]"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>app.py</span><span class="flex flex-shrink-0 items-center rounded-r border-l px-2 py-1.5 border-[#dddddd] bg-[#d8d8d8] text-[#666666] dark:border-[#333333] dark:bg-[#2a2a2a] dark:text-[#888888]">28-39</span></a> <a href="https://github.com/krishnakumaragrawal/Artificial-Intelligence-based-Online-Exam-Proctoring-System/blob/2b4f4e85/app.py#L120-L126" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4]"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>app.py</span><span class="flex flex-shrink-0 items-center rounded-r border-l px-2 py-1.5 border-[#dddddd] bg-[#d8d8d8] text-[#666666] dark:border-[#333333] dark:bg-[#2a2a2a] dark:text-[#888888]">120-126</span></a> <a href="https://github.com/krishnakumaragrawal/Artificial-Intelligence-based-Online-Exam-Proctoring-System/blob/2b4f4e85/main.py#L131-L140" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4]"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>main.py</span><span class="flex flex-shrink-0 items-center rounded-r border-l px-2 py-1.5 border-[#dddddd] bg-[#d8d8d8] text-[#666666] dark:border-[#333333] dark:bg-[#2a2a2a] dark:text-[#888888]">131-140</span></a></p>
<h2 id="quick-verification" class="group" data-header="true">Quick Verification<button class="relative ml-2 cursor-pointer align-baseline text-gray-400 opacity-0 transition-opacity hover:text-blue-400 focus:opacity-100 group-hover:opacity-100" aria-label="Copy link to header"><svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" fill="currentColor" viewBox="0 0 256 256" class="h-4 w-4"><path d="M117.18,188.74a12,12,0,0,1,0,17l-5.12,5.12A58.26,58.26,0,0,1,70.6,228h0A58.62,58.62,0,0,1,29.14,127.92L63.89,93.17a58.64,58.64,0,0,1,98.56,28.11,12,12,0,1,1-23.37,5.44,34.65,34.65,0,0,0-58.22-16.58L46.11,144.89A34.62,34.62,0,0,0,70.57,204h0a34.41,34.41,0,0,0,24.49-10.14l5.11-5.12A12,12,0,0,1,117.18,188.74ZM226.83,45.17a58.65,58.65,0,0,0-82.93,0l-5.11,5.11a12,12,0,0,0,17,17l5.12-5.12a34.63,34.63,0,1,1,49,49L175.1,145.86A34.39,34.39,0,0,1,150.61,156h0a34.63,34.63,0,0,1-33.69-26.72,12,12,0,0,0-23.38,5.44A58.64,58.64,0,0,0,150.56,180h.05a58.28,58.28,0,0,0,41.47-17.17l34.75-34.75a58.62,58.62,0,0,0,0-82.91Z"></path></svg></button></h2>
<p>To verify the system is working correctly:</p>
<ol>
<li><strong>Camera Access</strong>: Ensure the webcam indicator light activates</li>
<li><strong>Console Output</strong>: Monitor real-time detection results printed to console</li>
<li><strong>Audio Alerts</strong>: Test by covering/uncovering the camera or having multiple people in frame</li>
<li><strong>Activity Logging</strong>: Check that <code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">activity.txt</code> is created after system shutdown</li>
<li><strong>Web Interface</strong> (if using server mode): Navigate to <code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">http://localhost:5000</code> to access the web interface</li>
</ol>
<p>The system is functioning correctly when you see continuous timestamp outputs and detection results in the console, with appropriate audio alerts for suspicious activities.</p>
<p><strong>Sources:</strong> <a href="https://github.com/krishnakumaragrawal/Artificial-Intelligence-based-Online-Exam-Proctoring-System/blob/2b4f4e85/app.py#L55-L98" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4]"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>app.py</span><span class="flex flex-shrink-0 items-center rounded-r border-l px-2 py-1.5 border-[#dddddd] bg-[#d8d8d8] text-[#666666] dark:border-[#333333] dark:bg-[#2a2a2a] dark:text-[#888888]">55-98</span></a> <a href="https://github.com/krishnakumaragrawal/Artificial-Intelligence-based-Online-Exam-Proctoring-System/blob/2b4f4e85/server.py#L69-L70" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4]"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>server.py</span><span class="flex flex-shrink-0 items-center rounded-r border-l px-2 py-1.5 border-[#dddddd] bg-[#d8d8d8] text-[#666666] dark:border-[#333333] dark:bg-[#2a2a2a] dark:text-[#888888]">69-70</span></a></p></div></div></div></div></div></div><div class="hidden overflow-hidden transition-[border-radius] xl:sticky xl:right-0 xl:top-20 xl:block xl:h-[calc(100vh-82px)] xl:w-64 xl:flex-shrink-0 2xl:w-72" style="scrollbar-width:none"><div class="flex max-h-full w-full flex-shrink-0 flex-col py-6 pt-0 text-sm lg:pb-4 lg:pt-8 xl:w-64 2xl:w-72" style="scrollbar-color:var(--color-night) transparent"><div><div class="relative mx-4 my-4 rounded-md border border-neutral-200 bg-neutral-100 p-3 text-sm text-neutral-600 dark:border-neutral-800 dark:bg-neutral-900 dark:text-neutral-400"><button class="absolute right-2 top-2 rounded-sm p-1 opacity-70 transition-opacity hover:opacity-100 focus:outline-none focus:ring-2 focus:ring-neutral-400 focus:ring-offset-2"><svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" fill="currentColor" viewBox="0 0 256 256" class="h-4 w-4"><path d="M205.66,194.34a8,8,0,0,1-11.32,11.32L128,139.31,61.66,205.66a8,8,0,0,1-11.32-11.32L116.69,128,50.34,61.66A8,8,0,0,1,61.66,50.34L128,116.69l66.34-66.35a8,8,0,0,1,11.32,11.32L139.31,128Z"></path></svg><span class="sr-only">Dismiss</span></button><p class="text-sm font-medium">Refresh this wiki</p><button class="mt-2 flex items-center gap-1 rounded-md bg-neutral-200 px-2 py-1 text-sm font-medium text-neutral-700 transition-colors hover:bg-neutral-300 dark:bg-neutral-800 dark:text-neutral-300 dark:hover:bg-neutral-700">Enter email to refresh</button></div></div><h3 class="px-4 pb-5 text-lg font-medium leading-none">On this page</h3><ul style="scrollbar-width:none" class="min-h-0 flex-1 space-y-3 overflow-y-auto p-4 pt-0"><li class=""><a href="#getting-started" class="hover:text-primary pr-1 transition-all text-primary font-medium">Getting Started</a></li><li class="ml-3"><a href="#prerequisites-and-system-requirements" class="hover:text-primary pr-1 font-normal transition-all text-secondary">Prerequisites and System Requirements</a></li><li class="ml-6"><a href="#required-dependencies" class="hover:text-primary pr-1 font-normal transition-all text-secondary">Required Dependencies</a></li><li class="ml-6"><a href="#hardware-requirements" class="hover:text-primary pr-1 font-normal transition-all text-secondary">Hardware Requirements</a></li><li class="ml-3"><a href="#system-entry-points" class="hover:text-primary pr-1 font-normal transition-all text-secondary">System Entry Points</a></li><li class="ml-6"><a href="#entry-point-comparison" class="hover:text-primary pr-1 font-normal transition-all text-secondary">Entry Point Comparison</a></li><li class="ml-6"><a href="#standalone-desktop-application" class="hover:text-primary pr-1 font-normal transition-all text-secondary">Standalone Desktop Application</a></li><li class="ml-6"><a href="#web-service-mode" class="hover:text-primary pr-1 font-normal transition-all text-secondary">Web Service Mode</a></li><li class="ml-3"><a href="#basic-usage-workflow" class="hover:text-primary pr-1 font-normal transition-all text-secondary">Basic Usage Workflow</a></li><li class="ml-6"><a href="#system-initialization-flow" class="hover:text-primary pr-1 font-normal transition-all text-secondary">System Initialization Flow</a></li><li class="ml-6"><a href="#core-monitoring-loop" class="hover:text-primary pr-1 font-normal transition-all text-secondary">Core Monitoring Loop</a></li><li class="ml-3"><a href="#understanding-system-output" class="hover:text-primary pr-1 font-normal transition-all text-secondary">Understanding System Output</a></li><li class="ml-6"><a href="#activity-logging-structure" class="hover:text-primary pr-1 font-normal transition-all text-secondary">Activity Logging Structure</a></li><li class="ml-6"><a href="#alert-system" class="hover:text-primary pr-1 font-normal transition-all text-secondary">Alert System</a></li><li class="ml-6"><a href="#output-files" class="hover:text-primary pr-1 font-normal transition-all text-secondary">Output Files</a></li><li class="ml-3"><a href="#quick-verification" class="hover:text-primary pr-1 font-normal transition-all text-secondary">Quick Verification</a></li></ul></div></div><div class="pointer-events-none fixed bottom-2 left-2 right-2 mt-2 md:bottom-4 md:left-0 md:right-0"><div class="z-10 mx-auto max-w-3xl"><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><!--/$--></div></div></div></div></div><!--/$--></div><!--/$--><script src="/_next/static/chunks/webpack-acbbbb548492d4a6.js?dpl=dpl_3BvSF9SjLWU2C3NrwwXTj5194PnT" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:\"$Sreact.fragment\"\n2:I[51709,[\"9453\",\"static/chunks/b1298b8d-549c141f97a3b262.js?dpl=dpl_3BvSF9SjLWU2C3NrwwXTj5194PnT\",\"8970\",\"static/chunks/378e5a93-3b0f971d3611a8a5.js?dpl=dpl_3BvSF9SjLWU2C3NrwwXTj5194PnT\",\"1585\",\"static/chunks/f7f68e2d-40290491c524df5c.js?dpl=dpl_3BvSF9SjLWU2C3NrwwXTj5194PnT\",\"5009\",\"static/chunks/5009-cf1c1739f4eccbfa.js?dpl=dpl_3BvSF9SjLWU2C3NrwwXTj5194PnT\",\"3377\",\"static/chunks/3377-d302682beb4206f6.js?dpl=dpl_3BvSF9SjLWU2C3NrwwXTj5194PnT\",\"3224\",\"static/chunks/3224-7e9887ea92eab174.js?dpl=dpl_3BvSF9SjLWU2C3NrwwXTj5194PnT\",\"6967\",\"static/chunks/6967-c4b815e71e97ed18.js?dpl=dpl_3BvSF9SjLWU2C3NrwwXTj5194PnT\",\"7177\",\"static/chunks/app/layout-97dc749f3c752109.js?dpl=dpl_3BvSF9SjLWU2C3NrwwXTj5194PnT\"],\"RootProvider\"]\n3:I[87555,[],\"\"]\n4:I[31295,[],\"\"]\n6:I[90894,[],\"ClientPageRoot\"]\n7:I[87667,[\"9453\",\"static/chunks/b1298b8d-549c141f97a3b262.js?dpl=dpl_3BvSF9SjLWU2C3NrwwXTj5194PnT\",\"8970\",\"static/chunks/378e5a93-3b0f971d3611a8a5.js?dpl=dpl_3BvSF9SjLWU2C3NrwwXTj5194PnT\",\"1585\",\"static/chunks/f7f68e2d-40290491c524df5c.js?dpl=dpl_3BvSF9SjLWU2C3NrwwXTj5194PnT\",\"4129\",\"static/chunks/7bf36345-06f80506190927ed.js?dpl=dpl_3BvSF9SjLWU2C3NrwwXTj5194PnT\",\"2545\",\"static/chunks/c16f53c3-1a60b9b77b3e1d4b.js?dpl=dpl_3BvSF9SjLWU2C3NrwwXTj5194PnT\",\"537\",\"static/chunks/537-bef73e6675141c6f.js?dpl=dpl_3BvSF9SjLWU2C3NrwwXTj5194PnT\",\"5009\",\"static/chunks/5009-cf1c1739f4eccbfa.js?dpl=dpl_3BvSF9SjLWU2C3NrwwXTj5194PnT\",\"3377\",\"static/chunks/3377-d302682beb4206f6.js?dpl=dpl_3BvSF9SjLWU2C3NrwwXTj5194PnT\",\"3224\",\"static/chunks/3224-7e9887ea92eab174.js?dpl=dpl_3BvSF9SjLWU2C3NrwwXTj5194PnT\",\"8039\",\"static/chunks/8039-e53433e94d896525.js?dpl=dpl_3BvSF9SjLWU2C3NrwwXTj5194PnT\",\"2136\",\"static/chunks/2136-947db7298d61ada7.js?dpl=dpl_3BvSF9SjLWU2C3NrwwXTj5194PnT\",\"6967\",\"static/chunks/6967-c4b815e71e97ed18.js?dpl=dpl_3BvSF9SjLWU2C3NrwwXTj5194PnT\",\"9769\",\"static/chunks/9769-220603704fe7fef4.js?dpl=dpl_3BvSF9SjLWU2C3NrwwXTj5194PnT\",\"8674\",\"static/chunks/8674-a5b7b13d963b8a5d.js?dpl=dpl_3BvSF9SjLWU2C3NrwwXTj5194PnT\",\"3283\",\"static/ch"])</script><script>self.__next_f.push([1,"unks/3283-f8ea31617a625982.js?dpl=dpl_3BvSF9SjLWU2C3NrwwXTj5194PnT\",\"5104\",\"static/chunks/5104-74152de4b96231ef.js?dpl=dpl_3BvSF9SjLWU2C3NrwwXTj5194PnT\",\"5024\",\"static/chunks/5024-17a1979955106ff0.js?dpl=dpl_3BvSF9SjLWU2C3NrwwXTj5194PnT\",\"2853\",\"static/chunks/2853-5659559f5634fdac.js?dpl=dpl_3BvSF9SjLWU2C3NrwwXTj5194PnT\",\"3285\",\"static/chunks/app/%5Borg%5D/%5Brepo%5D/%5B%5B...wikiRoutes%5D%5D/page-1febda9ce73dd135.js?dpl=dpl_3BvSF9SjLWU2C3NrwwXTj5194PnT\"],\"default\"]\na:I[59665,[],\"OutletBoundary\"]\nd:I[59665,[],\"ViewportBoundary\"]\nf:I[59665,[],\"MetadataBoundary\"]\n11:I[26614,[],\"\"]\n:HL[\"/_next/static/media/4cf2300e9c8272f7-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n:HL[\"/_next/static/media/93f479601ee12b01-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n:HL[\"/_next/static/css/de70bee13400563f.css?dpl=dpl_3BvSF9SjLWU2C3NrwwXTj5194PnT\",\"style\"]\n:HL[\"/_next/static/css/fc1993f38795cea2.css?dpl=dpl_3BvSF9SjLWU2C3NrwwXTj5194PnT\",\"style\"]\n"])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"SAiO5vRNMlMp-wxpnPDkZ\",\"p\":\"\",\"c\":[\"\",\"krishnakumaragrawal\",\"Artificial-Intelligence-based-Online-Exam-Proctoring-System\",\"2-getting-started\"],\"i\":false,\"f\":[[[\"\",{\"children\":[[\"org\",\"krishnakumaragrawal\",\"d\"],{\"children\":[[\"repo\",\"Artificial-Intelligence-based-Online-Exam-Proctoring-System\",\"d\"],{\"children\":[[\"wikiRoutes\",\"2-getting-started\",\"oc\"],{\"children\":[\"__PAGE__\",{}]}]}]}]},\"$undefined\",\"$undefined\",true],[\"\",[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/de70bee13400563f.css?dpl=dpl_3BvSF9SjLWU2C3NrwwXTj5194PnT\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}],[\"$\",\"link\",\"1\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/fc1993f38795cea2.css?dpl=dpl_3BvSF9SjLWU2C3NrwwXTj5194PnT\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"suppressHydrationWarning\":true,\"children\":[[\"$\",\"head\",null,{}],[\"$\",\"body\",null,{\"className\":\"__variable_188709 font-geist-sans relative min-h-screen __variable_9a8899 bg-background antialiased\",\"children\":[\"$\",\"$L2\",null,{\"children\":[\"$\",\"$L3\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L4\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":404}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],[]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]}]}]]}]]}],{\"children\":[[\"org\",\"krishnakumaragrawal\",\"d\"],[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L3\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L4\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"repo\",\"Artificial-Intelligence-based-Online-Exam-Proctoring-System\",\"d\"],[\"$\",\"$1\",\"c\",{\"children\":[null,\"$L5\"]}],{\"children\":[[\"wikiRoutes\",\"2-getting-started\",\"oc\"],[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L3\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L4\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[\"__PAGE__\",[\"$\",\"$1\",\"c\",{\"children\":[[\"$\",\"$L6\",null,{\"Component\":\"$7\",\"searchParams\":{},\"params\":{\"org\":\"krishnakumaragrawal\",\"repo\":\"Artificial-Intelligence-based-Online-Exam-Proctoring-System\",\"wikiRoutes\":[\"2-getting-started\"]},\"promises\":[\"$@8\",\"$@9\"]}],\"$undefined\",null,[\"$\",\"$La\",null,{\"children\":[\"$Lb\",\"$Lc\",null]}]]}],{},null,false]},null,false]},null,false]},null,false]},null,false],[\"$\",\"$1\",\"h\",{\"children\":[null,[\"$\",\"$1\",\"qjbF95UvLegAreQVWfoKV\",{\"children\":[[\"$\",\"$Ld\",null,{\"children\":\"$Le\"}],[\"$\",\"meta\",null,{\"name\":\"next-size-adjust\",\"content\":\"\"}]]}],[\"$\",\"$Lf\",null,{\"children\":\"$L10\"}]]}],false]],\"m\":\"$undefined\",\"G\":[\"$11\",\"$undefined\"],\"s\":false,\"S\":true}\n"])</script><script>self.__next_f.push([1,"8:{}\n9:{\"org\":\"krishnakumaragrawal\",\"repo\":\"Artificial-Intelligence-based-Online-Exam-Proctoring-System\",\"wikiRoutes\":\"$0:f:0:1:2:children:2:children:2:children:2:children:1:props:children:0:props:params:wikiRoutes\"}\n"])</script><script>self.__next_f.push([1,"e:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"meta\",\"1\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}]]\nb:null\n"])</script><script>self.__next_f.push([1,"12:I[87437,[\"9453\",\"static/chunks/b1298b8d-549c141f97a3b262.js?dpl=dpl_3BvSF9SjLWU2C3NrwwXTj5194PnT\",\"8970\",\"static/chunks/378e5a93-3b0f971d3611a8a5.js?dpl=dpl_3BvSF9SjLWU2C3NrwwXTj5194PnT\",\"1585\",\"static/chunks/f7f68e2d-40290491c524df5c.js?dpl=dpl_3BvSF9SjLWU2C3NrwwXTj5194PnT\",\"537\",\"static/chunks/537-bef73e6675141c6f.js?dpl=dpl_3BvSF9SjLWU2C3NrwwXTj5194PnT\",\"5009\",\"static/chunks/5009-cf1c1739f4eccbfa.js?dpl=dpl_3BvSF9SjLWU2C3NrwwXTj5194PnT\",\"3377\",\"static/chunks/3377-d302682beb4206f6.js?dpl=dpl_3BvSF9SjLWU2C3NrwwXTj5194PnT\",\"8039\",\"static/chunks/8039-e53433e94d896525.js?dpl=dpl_3BvSF9SjLWU2C3NrwwXTj5194PnT\",\"2136\",\"static/chunks/2136-947db7298d61ada7.js?dpl=dpl_3BvSF9SjLWU2C3NrwwXTj5194PnT\",\"9769\",\"static/chunks/9769-220603704fe7fef4.js?dpl=dpl_3BvSF9SjLWU2C3NrwwXTj5194PnT\",\"3449\",\"static/chunks/3449-8ea8d12ec92ab7b3.js?dpl=dpl_3BvSF9SjLWU2C3NrwwXTj5194PnT\",\"3283\",\"static/chunks/3283-f8ea31617a625982.js?dpl=dpl_3BvSF9SjLWU2C3NrwwXTj5194PnT\",\"2015\",\"static/chunks/2015-d2f0d3f0cd483d40.js?dpl=dpl_3BvSF9SjLWU2C3NrwwXTj5194PnT\",\"736\",\"static/chunks/736-daf66278216745a2.js?dpl=dpl_3BvSF9SjLWU2C3NrwwXTj5194PnT\",\"2933\",\"static/chunks/app/%5Borg%5D/%5Brepo%5D/layout-3efb06fa04ccb94c.js?dpl=dpl_3BvSF9SjLWU2C3NrwwXTj5194PnT\"],\"HeaderWrapperWithSuspense\"]\n13:I[93403,[\"9453\",\"static/chunks/b1298b8d-549c141f97a3b262.js?dpl=dpl_3BvSF9SjLWU2C3NrwwXTj5194PnT\",\"8970\",\"static/chunks/378e5a93-3b0f971d3611a8a5.js?dpl=dpl_3BvSF9SjLWU2C3NrwwXTj5194PnT\",\"1585\",\"static/chunks/f7f68e2d-40290491c524df5c.js?dpl=dpl_3BvSF9SjLWU2C3NrwwXTj5194PnT\",\"537\",\"static/chunks/537-bef73e6675141c6f.js?dpl=dpl_3BvSF9SjLWU2C3NrwwXTj5194PnT\",\"5009\",\"static/chunks/5009-cf1c1739f4eccbfa.js?dpl=dpl_3BvSF9SjLWU2C3NrwwXTj5194PnT\",\"3377\",\"static/chunks/3377-d302682beb4206f6.js?dpl=dpl_3BvSF9SjLWU2C3NrwwXTj5194PnT\",\"8039\",\"static/chunks/8039-e53433e94d896525.js?dpl=dpl_3BvSF9SjLWU2C3NrwwXTj5194PnT\",\"2136\",\"static/chunks/2136-947db7298d61ada7.js?dpl=dpl_3BvSF9SjLWU2C3NrwwXTj5194PnT\",\"9769\",\"static/chunks/9769-220603704fe7fef4.js?dpl=dpl_3BvSF9SjLWU2C3NrwwXTj519"])</script><script>self.__next_f.push([1,"4PnT\",\"3449\",\"static/chunks/3449-8ea8d12ec92ab7b3.js?dpl=dpl_3BvSF9SjLWU2C3NrwwXTj5194PnT\",\"3283\",\"static/chunks/3283-f8ea31617a625982.js?dpl=dpl_3BvSF9SjLWU2C3NrwwXTj5194PnT\",\"2015\",\"static/chunks/2015-d2f0d3f0cd483d40.js?dpl=dpl_3BvSF9SjLWU2C3NrwwXTj5194PnT\",\"736\",\"static/chunks/736-daf66278216745a2.js?dpl=dpl_3BvSF9SjLWU2C3NrwwXTj5194PnT\",\"2933\",\"static/chunks/app/%5Borg%5D/%5Brepo%5D/layout-3efb06fa04ccb94c.js?dpl=dpl_3BvSF9SjLWU2C3NrwwXTj5194PnT\"],\"WikiContextProvider\"]\n14:T1e0b,"])</script><script>self.__next_f.push([1,"# Overview\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [Project Overview.txt](Project Overview.txt)\n- [README.md](README.md)\n\n\u003c/details\u003e\n\n\n\n## Purpose and Scope\n\nThis document provides a comprehensive overview of the AI-based Online Exam Proctoring System, a real-time monitoring solution designed to maintain academic integrity during remote examinations. The system combines multiple artificial intelligence detection modules with a web-based interface to continuously monitor test-takers and log suspicious activities.\n\nThis overview covers the system's architectural design, core components, and data flow patterns. For detailed implementation of specific AI detection modules, see [AI Detection Engine](#4). For web interface components and user authentication, see [Web Interface](#5). For database schema and data management details, see [Database and Data Management](#6).\n\n## System Architecture\n\nThe system follows a layered architecture with clear separation between the web interface, application logic, AI detection engine, and data persistence layers.\n\n### High-Level Component Architecture\n\n```mermaid\ngraph TB\n    subgraph \"Web Layer\"\n        templates[\"templates/index.html\u003cbr/\u003etemplates/quiz.html\"]\n        static[\"static/login_script.js\u003cbr/\u003estatic/style.css\"]\n        server[\"server.py\u003cbr/\u003eFlask Routes\"]\n    end\n    \n    subgraph \"Application Layer\"\n        app[\"app.py\u003cbr/\u003eWeb Application Entry\"]\n        main[\"main.py\u003cbr/\u003eCLI Entry Point\"]\n        proctoringAlgo[\"proctoringAlgo()\u003cbr/\u003eOrchestration Function\"]\n    end\n    \n    subgraph \"AI Detection Engine\"\n        facial_detections[\"facial_detections.py\u003cbr/\u003eFace Detection\"]\n        eye_tracker[\"eye_tracker.py\u003cbr/\u003eGaze Tracking\"]\n        blink_detection[\"blink_detection.py\u003cbr/\u003eBlink Analysis\"]\n        head_pose_estimation[\"head_pose_estimation.py\u003cbr/\u003ePose Detection\"]\n        mouth_tracking[\"mouth_tracking.py\u003cbr/\u003eSpeech Detection\"]\n        object_detection[\"object_detection.py\u003cbr/\u003eYOLO Implementation\"]\n        audio_detection[\"audio_detection.py\u003cbr/\u003eSound Analysis\"]\n    end\n    \n    subgraph \"Data Layer\"\n        db_helper[\"backend/db_helper.py\u003cbr/\u003eDatabase Interface\"]\n        database[\"database.sql\u003cbr/\u003eMySQL Schema\"]\n        activity_log[\"activity.txt\u003cbr/\u003eSession Logs\"]\n    end\n    \n    subgraph \"AI Models\"\n        shape_predictor[\"shape_predictor_model/\u003cbr/\u003e68 Point Landmarks\"]\n        yolo_model[\"object_detection_model/\u003cbr/\u003eYOLO Weights \u0026 Config\"]\n    end\n    \n    templates --\u003e server\n    static --\u003e server\n    server --\u003e app\n    server --\u003e db_helper\n    app --\u003e proctoringAlgo\n    main --\u003e proctoringAlgo\n    \n    proctoringAlgo --\u003e facial_detections\n    proctoringAlgo --\u003e eye_tracker\n    proctoringAlgo --\u003e blink_detection\n    proctoringAlgo --\u003e head_pose_estimation\n    proctoringAlgo --\u003e mouth_tracking\n    proctoringAlgo --\u003e object_detection\n    proctoringAlgo --\u003e audio_detection\n    proctoringAlgo --\u003e activity_log\n    \n    facial_detections --\u003e shape_predictor\n    eye_tracker --\u003e shape_predictor\n    blink_detection --\u003e shape_predictor\n    head_pose_estimation --\u003e shape_predictor\n    mouth_tracking --\u003e shape_predictor\n    object_detection --\u003e yolo_model\n    \n    db_helper --\u003e database\n```\n\nSources: [README.md:1-28](), [Project Overview.txt:1-40]()\n\n## Core Components\n\n### Application Entry Points\n\nThe system provides two primary entry points:\n\n| Entry Point | Purpose | Key Functions |\n|-------------|---------|---------------|\n| `app.py` | Web application interface | Flask integration, HTTP request handling |\n| `main.py` | Command-line interface | Direct proctoring execution |\n\nBoth entry points utilize the central `proctoringAlgo()` function to orchestrate the monitoring process.\n\n### AI Detection Modules\n\nThe system implements seven specialized detection modules:\n\n| Module | File | Detection Capability |\n|--------|------|---------------------|\n| Face Detection | `facial_detections.py` | Person identification and counting |\n| Eye Tracking | `eye_tracker.py` | Gaze direction analysis |\n| Blink Detection | `blink_detection.py` | Eye closure patterns |\n| Head Pose | `head_pose_estimation.py` | 3D head orientation |\n| Mouth Tracking | `mouth_tracking.py` | Speech detection |\n| Object Detection | `object_detection.py` | Unauthorized item detection |\n| Audio Detection | `audio_detection.py` | Sound level monitoring |\n\n### Data Management\n\nThe system maintains two primary data stores:\n\n- **MySQL Database**: User authentication and exam data via `backend/db_helper.py`\n- **Activity Logs**: Real-time behavior logging to `activity.txt`\n\nSources: [README.md:6-13](), [Project Overview.txt:15-27]()\n\n## Real-Time Monitoring Data Flow\n\nThe following diagram illustrates how data flows through the system during an active exam session:\n\n```mermaid\nsequenceDiagram\n    participant Student as \"Student Browser\"\n    participant FlaskServer as \"server.py\"\n    participant ProctoringAlgo as \"proctoringAlgo()\"\n    participant AIModules as \"AI Detection Modules\"\n    participant ActivityLog as \"activity.txt\"\n    \n    Student-\u003e\u003eFlaskServer: \"POST /login_data\"\n    FlaskServer-\u003e\u003eStudent: \"Redirect to quiz.html\"\n    \n    Student-\u003e\u003eFlaskServer: \"GET /video_feed\"\n    FlaskServer-\u003e\u003eProctoringAlgo: \"Initialize monitoring\"\n    \n    loop \"Continuous Monitoring\"\n        ProctoringAlgo-\u003e\u003eAIModules: \"Process video/audio frame\"\n        AIModules-\u003e\u003eAIModules: \"facial_detections.py\"\n        AIModules-\u003e\u003eAIModules: \"eye_tracker.py\"\n        AIModules-\u003e\u003eAIModules: \"blink_detection.py\"\n        AIModules-\u003e\u003eAIModules: \"head_pose_estimation.py\"\n        AIModules-\u003e\u003eAIModules: \"mouth_tracking.py\"\n        AIModules-\u003e\u003eAIModules: \"object_detection.py\"\n        AIModules-\u003e\u003eAIModules: \"audio_detection.py\"\n        AIModules-\u003e\u003eProctoringAlgo: \"Detection results\"\n        ProctoringAlgo-\u003e\u003eActivityLog: \"Log timestamp + activity\"\n        \n        alt \"Violation Detected\"\n            ProctoringAlgo-\u003e\u003eStudent: \"winsound.Beep() alert\"\n        end\n    end\n    \n    Student-\u003e\u003eFlaskServer: \"End exam session\"\n    ProctoringAlgo-\u003e\u003eActivityLog: \"Finalize session log\"\n```\n\nSources: [README.md:1-4](), [Project Overview.txt:23-28]()\n\n## Technology Stack\n\n### Core Technologies\n\n| Layer | Technologies | Purpose |\n|-------|-------------|---------|\n| **Frontend** | HTML, CSS, JavaScript, jQuery | User interface and interaction |\n| **Backend** | Python, Flask | Web server and application logic |\n| **Computer Vision** | OpenCV, dlib, NumPy | Image processing and analysis |\n| **AI Models** | YOLO, 68-point face landmarks | Object and facial feature detection |\n| **Audio Processing** | PyAudio, winsound | Sound capture and alert generation |\n| **Database** | MySQL | User data and exam information storage |\n\n### Key Dependencies\n\nThe system relies on several critical libraries:\n\n- **OpenCV**: Video capture and image processing operations\n- **dlib**: Facial landmark detection using HOG-based face detection\n- **YOLO**: Real-time object detection for unauthorized items\n- **PyAudio**: Audio stream processing for environmental monitoring\n- **Flask**: HTTP server for web interface delivery\n\nSources: [README.md:15-24](), [Project Overview.txt:33-39]()\n\n## Activity Logging and Monitoring\n\nThe system generates comprehensive audit trails through the `activity.txt` file, which records:\n\n- Timestamp-based activity logs\n- Detection results from all AI modules  \n- Violation alerts and system responses\n- Session start and end markers\n\nThis logging mechanism enables post-exam review and analysis of student behavior patterns, supporting academic integrity enforcement.\n\nSources: [README.md:4](), [Project Overview.txt:28]()"])</script><script>self.__next_f.push([1,"15:T1f6c,"])</script><script>self.__next_f.push([1,"# Getting Started\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [app.py](app.py)\n- [main.py](main.py)\n- [server.py](server.py)\n\n\u003c/details\u003e\n\n\n\nThis document provides a comprehensive guide to setting up and running the AI-based Online Exam Proctoring System. It covers the primary entry points, system initialization, and basic usage patterns to help you get the system operational quickly.\n\nThe system can be run in two modes: as a standalone desktop application for direct proctoring or as a web-based service with user authentication and exam management. For detailed information about the AI detection modules, see [AI Detection Engine](#4). For web interface configuration, see [Web Interface](#5).\n\n## Prerequisites and System Requirements\n\nBefore running the system, ensure you have the following components installed and configured:\n\n### Required Dependencies\nThe system requires several Python packages and external models:\n\n| Component | Purpose | Used By |\n|-----------|---------|---------|\n| OpenCV (`cv2`) | Video capture and image processing | [app.py:1](), [main.py:1]() |\n| dlib | Facial landmark detection | Multiple detection modules |\n| Flask | Web server framework | [server.py:1]() |\n| MySQL | Database backend | [server.py:3]() |\n| YOLO models | Object detection | `detectObject()` function |\n| Winsound | Audio alerts | [app.py:10](), [main.py:4]() |\n\n### Hardware Requirements\n- **Webcam**: Required for video capture via `cv2.VideoCapture(0)` [app.py:21]()\n- **Microphone**: Optional for audio detection capabilities\n- **Speakers**: Required for audio alerts using `winsound.Beep()` [app.py:32]()\n\n**Sources:** [app.py:1-26](), [main.py:1-29](), [server.py:1-9]()\n\n## System Entry Points\n\nThe system provides two primary entry points depending on your usage requirements:\n\n### Entry Point Comparison\n\n```mermaid\ngraph TB\n    subgraph \"Standalone Mode\"\n        AppPy[\"app.py\"]\n        AppProctoringAlgo[\"proctoringAlgo()\"]\n        AppOutput[\"activity.txt\"]\n    end\n    \n    subgraph \"Web Service Mode\"\n        ServerPy[\"server.py\"]\n        FlaskApp[\"Flask app\"]\n        WebRoutes[\"Web Routes\"]\n        MainPy[\"main.py\"]\n        MainProctoringAlgo[\"proctoringAlgo()\"]\n        MainOutput[\"activity.txt\"]\n    end\n    \n    AppPy --\u003e AppProctoringAlgo\n    AppProctoringAlgo --\u003e AppOutput\n    \n    ServerPy --\u003e FlaskApp\n    FlaskApp --\u003e WebRoutes\n    WebRoutes --\u003e MainPy\n    MainPy --\u003e MainProctoringAlgo\n    MainProctoringAlgo --\u003e MainOutput\n    \n    style AppPy fill:#f9f9f9\n    style ServerPy fill:#f9f9f9\n```\n\n### Standalone Desktop Application\n\nTo run the system as a standalone desktop application:\n\n```bash\npython app.py\n```\n\nThis mode initializes the camera directly and runs the proctoring algorithm with a local OpenCV window display. The `proctoringAlgo()` function [app.py:43]() will:\n\n1. Initialize `cv2.VideoCapture(0)` for webcam access [app.py:21]()\n2. Run continuous monitoring loop [app.py:47]()\n3. Display real-time video feed via `cv2.imshow('Frame', frame)` [app.py:109]()\n4. Save activity data to `activity.txt` [app.py:124-125]()\n\n### Web Service Mode\n\nTo run the system as a web service with authentication:\n\n```bash\npython server.py\n```\n\nThis starts the Flask development server on port 5000 [server.py:70](). The web service provides these endpoints:\n\n| Route | Purpose | Function |\n|-------|---------|----------|\n| `/` | Main login page | `index_page()` [server.py:40]() |\n| `/quiz_html` | Exam interface | `quix_page()` [server.py:46]() |\n| `/video_feed` | Video streaming | `video_feed()` [server.py:52]() |\n| `/signup_data` | User registration | `signup_data()` [server.py:16]() |\n| `/login_data` | User authentication | `login_data()` [server.py:28]() |\n| `/stop_camera` | System shutdown | `stop_camera()` [server.py:58]() |\n\n**Sources:** [app.py:117-118](), [server.py:38-70](), [main.py:131-140]()\n\n## Basic Usage Workflow\n\n### System Initialization Flow\n\n```mermaid\nsequenceDiagram\n    participant User as \"User\"\n    participant System as \"System Entry Point\"\n    participant Camera as \"cv2.VideoCapture(0)\"\n    participant Proctoring as \"proctoringAlgo()\"\n    participant Detection as \"AI Detection Modules\"\n    participant Logger as \"Activity Logger\"\n    \n    User-\u003e\u003eSystem: \"Start Application\"\n    System-\u003e\u003eCamera: \"Initialize Camera\"\n    \n    alt Camera Already Open\n        Camera-\u003e\u003eCamera: \"cam.isOpened() == False\"\n        Camera-\u003e\u003eCamera: \"cam.open()\"\n    end\n    \n    System-\u003e\u003eProctoring: \"Call proctoringAlgo()\"\n    \n    loop \"Continuous Monitoring\"\n        Proctoring-\u003e\u003eCamera: \"cam.read()\"\n        Camera-\u003e\u003eProctoring: \"Return frame\"\n        Proctoring-\u003e\u003eDetection: \"Process frame\"\n        Detection-\u003e\u003eProctoring: \"Return detection results\"\n        Proctoring-\u003e\u003eLogger: \"Append to data_record[]\"\n        \n        alt Suspicious Activity\n            Proctoring-\u003e\u003eUser: \"winsound.Beep(frequency, duration)\"\n        end\n    end\n    \n    User-\u003e\u003eSystem: \"Press 'q' or stop system\"\n    System-\u003e\u003eLogger: \"Write activity.txt\"\n    System-\u003e\u003eCamera: \"cam.release()\"\n```\n\n### Core Monitoring Loop\n\nThe heart of the system is the monitoring loop in `proctoringAlgo()` [app.py:47](), which performs these operations each frame:\n\n1. **Timestamp Recording**: Captures current time using `datetime.now().strftime(\"%H:%M:%S.%f\")` [app.py:54]()\n2. **Face Detection**: Calls `detectFace(frame)` returning `faceCount` and `faces` [app.py:59]()\n3. **Face Validation**: Processes face count through `faceCount_detection()` [app.py:60]()\n4. **Multi-modal Analysis** (if exactly 1 face detected):\n   - Blink detection via `isBlinking(faces, frame)` [app.py:67]()\n   - Gaze tracking via `gazeDetection(faces, frame)` [app.py:78]()\n   - Mouth tracking via `mouthTrack(faces, frame)` [app.py:83]()\n   - Object detection via `detectObject(frame)` [app.py:87]()\n   - Head pose estimation via `head_pose_detection(faces, frame)` [app.py:97]()\n\n**Sources:** [app.py:43-115](), [main.py:47-127]()\n\n## Understanding System Output\n\n### Activity Logging Structure\n\nThe system maintains a global `data_record` list [app.py:14]() that stores timestamped activity entries. Each record contains:\n\n```mermaid\ngraph LR\n    Record[\"Record Entry\"] --\u003e Timestamp[\"Current Time\"]\n    Record --\u003e FaceStatus[\"Face Detection Status\"]\n    Record --\u003e BlinkStatus[\"Blink Detection Result\"]\n    Record --\u003e EyeStatus[\"Gaze Detection Result\"]\n    Record --\u003e MouthStatus[\"Mouth Tracking Result\"]\n    Record --\u003e ObjectStatus[\"Object Detection Result\"]\n    Record --\u003e HeadStatus[\"Head Pose Result\"]\n```\n\n### Alert System\n\nThe system uses audio alerts via `winsound.Beep(frequency, duration)` with these parameters [app.py:17-18]():\n- **Frequency**: 2500 Hz\n- **Duration**: 1000 ms\n\nAlerts are triggered when:\n- Multiple faces detected (`faceCount \u003e 1`) [app.py:29-32]()\n- No face detected (`faceCount == 0`) [app.py:33-36]()\n- Multiple objects detected (`len(objectName) \u003e 1`) [app.py:91-94]()\n\n### Output Files\n\nUpon system termination, activity data is written to `activity.txt` [app.py:124-125]() using:\n```python\nactivityVal = \"\\n\".join(map(str, data_record))\nwith open('activity.txt', 'w') as file:\n    file.write(str(activityVal))\n```\n\n**Sources:** [app.py:13-14](), [app.py:28-39](), [app.py:120-126](), [main.py:131-140]()\n\n## Quick Verification\n\nTo verify the system is working correctly:\n\n1. **Camera Access**: Ensure the webcam indicator light activates\n2. **Console Output**: Monitor real-time detection results printed to console\n3. **Audio Alerts**: Test by covering/uncovering the camera or having multiple people in frame\n4. **Activity Logging**: Check that `activity.txt` is created after system shutdown\n5. **Web Interface** (if using server mode): Navigate to `http://localhost:5000` to access the web interface\n\nThe system is functioning correctly when you see continuous timestamp outputs and detection results in the console, with appropriate audio alerts for suspicious activities.\n\n**Sources:** [app.py:55-98](), [server.py:69-70]()"])</script><script>self.__next_f.push([1,"16:T290a,"])</script><script>self.__next_f.push([1,"# System Architecture\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [Project Overview.txt](Project Overview.txt)\n- [README.md](README.md)\n- [app.py](app.py)\n- [main.py](main.py)\n\n\u003c/details\u003e\n\n\n\n## Purpose and Scope\n\nThis document provides a comprehensive overview of the AI-based Online Exam Proctoring System's architecture, detailing how the various components interact to deliver real-time proctoring capabilities. The architecture encompasses web-based user interfaces, AI detection engines, data persistence mechanisms, and activity logging systems.\n\nFor detailed information about individual AI detection modules, see [AI Detection Engine](#4). For web interface implementation details, see [Web Interface](#5). For database schema and data management specifics, see [Database and Data Management](#6).\n\n## Architectural Overview\n\nThe system follows a layered architecture pattern with clear separation of concerns between the web interface, application logic, AI processing, and data persistence layers. The core orchestration is handled by the `proctoringAlgo()` function, which coordinates multiple AI detection modules in real-time.\n\n```mermaid\ngraph TB\n    subgraph \"Web_Layer\"\n        FlaskServer[\"Flask Server\u003cbr/\u003eserver.py\"]\n        HTMLTemplates[\"HTML Templates\u003cbr/\u003etemplates/\"]\n        StaticAssets[\"Static Assets\u003cbr/\u003estatic/\"]\n    end\n    \n    subgraph \"Application_Layer\"\n        MainApp[\"Main Application\u003cbr/\u003eapp.py\"]\n        ProctoringOrchestrator[\"proctoringAlgo()\u003cbr/\u003emain.py:47\"]\n        ActivityLogger[\"Activity Logger\u003cbr/\u003eactivity.txt writer\"]\n    end\n    \n    subgraph \"AI_Detection_Engine\"\n        FaceDetection[\"detectFace()\u003cbr/\u003efacial_detections.py\"]\n        BlinkDetection[\"isBlinking()\u003cbr/\u003eblink_detection.py\"]\n        EyeTracking[\"gazeDetection()\u003cbr/\u003eeye_tracker.py\"]\n        MouthTracking[\"mouthTrack()\u003cbr/\u003emouth_tracking.py\"]\n        ObjectDetection[\"detectObject()\u003cbr/\u003eobject_detection.py\"]\n        HeadPose[\"head_pose_detection()\u003cbr/\u003ehead_pose_estimation.py\"]\n        AudioDetection[\"audio_detection.py\"]\n    end\n    \n    subgraph \"Data_Layer\"\n        Database[\"MySQL Database\u003cbr/\u003esign_up table\"]\n        DBHelper[\"db_helper.py\"]\n        ActivityLog[\"activity.txt\u003cbr/\u003etimestamped logs\"]\n    end\n    \n    subgraph \"AI_Models\"\n        ShapePredictor[\"68 Face Landmarks\u003cbr/\u003eshape_predictor_model/\"]\n        YOLOModel[\"YOLO Object Detection\u003cbr/\u003eobject_detection_model/\"]\n    end\n    \n    FlaskServer --\u003e MainApp\n    FlaskServer --\u003e DBHelper\n    MainApp --\u003e ProctoringOrchestrator\n    ProctoringOrchestrator --\u003e FaceDetection\n    ProctoringOrchestrator --\u003e BlinkDetection\n    ProctoringOrchestrator --\u003e EyeTracking\n    ProctoringOrchestrator --\u003e MouthTracking\n    ProctoringOrchestrator --\u003e ObjectDetection\n    ProctoringOrchestrator --\u003e HeadPose\n    ProctoringOrchestrator --\u003e AudioDetection\n    ProctoringOrchestrator --\u003e ActivityLogger\n    ActivityLogger --\u003e ActivityLog\n    DBHelper --\u003e Database\n    FaceDetection --\u003e ShapePredictor\n    BlinkDetection --\u003e ShapePredictor\n    EyeTracking --\u003e ShapePredictor\n    MouthTracking --\u003e ShapePredictor\n    HeadPose --\u003e ShapePredictor\n    ObjectDetection --\u003e YOLOModel\n```\n\n**Sources:** [app.py:1-126](), [main.py:1-140](), [Project Overview.txt:1-40](), [README.md:1-28]()\n\n## Core Architectural Layers\n\n### Web Layer\nThe web layer implements the user-facing interface using Flask as the web framework. The `server.py` module handles HTTP routing, user authentication, and serves both static content and dynamic video streams.\n\n| Component | File | Purpose |\n|-----------|------|---------|\n| Flask Server | `server.py` | HTTP request handling, routing, authentication |\n| HTML Templates | `templates/` | User interface markup |\n| Static Assets | `static/` | CSS, JavaScript, images |\n\n### Application Layer\nThe application layer contains the core business logic, centered around the `proctoringAlgo()` function which orchestrates the entire proctoring workflow. This layer manages the video capture pipeline and coordinates all AI detection modules.\n\n```mermaid\nflowchart TD\n    VideoCapture[\"cv2.VideoCapture(0)\u003cbr/\u003emain.py:25\"]\n    ProctoringAlgo[\"proctoringAlgo()\u003cbr/\u003emain.py:47\"]\n    FrameProcessing[\"Frame Processing Loop\u003cbr/\u003emain.py:51\"]\n    \n    TimeStamp[\"current_time\u003cbr/\u003edatetime.now()\"]\n    FaceCountCheck[\"faceCount_detection()\u003cbr/\u003emain.py:32\"]\n    RecordBuilder[\"record = []\u003cbr/\u003emain.py:55\"]\n    DataRecord[\"data_record.append()\u003cbr/\u003emain.py:107\"]\n    \n    VideoCapture --\u003e ProctoringAlgo\n    ProctoringAlgo --\u003e FrameProcessing\n    FrameProcessing --\u003e TimeStamp\n    FrameProcessing --\u003e FaceCountCheck\n    FrameProcessing --\u003e RecordBuilder\n    RecordBuilder --\u003e DataRecord\n    \n    subgraph \"AI_Module_Calls\"\n        FaceDetect[\"detectFace()\u003cbr/\u003emain.py:63\"]\n        BlinkDetect[\"isBlinking()\u003cbr/\u003emain.py:71\"]\n        GazeDetect[\"gazeDetection()\u003cbr/\u003emain.py:82\"]\n        MouthDetect[\"mouthTrack()\u003cbr/\u003emain.py:87\"]\n        ObjectDetect[\"detectObject()\u003cbr/\u003emain.py:92\"]\n        HeadPoseDetect[\"head_pose_detection()\u003cbr/\u003emain.py:102\"]\n    end\n    \n    FrameProcessing --\u003e FaceDetect\n    FaceDetect --\u003e BlinkDetect\n    BlinkDetect --\u003e GazeDetect\n    GazeDetect --\u003e MouthDetect\n    MouthDetect --\u003e ObjectDetect\n    ObjectDetect --\u003e HeadPoseDetect\n```\n\n**Sources:** [main.py:47-127](), [app.py:43-118]()\n\n### AI Detection Engine Layer\nThe AI detection engine consists of specialized modules, each responsible for monitoring different aspects of student behavior. All modules operate on video frames captured from the webcam and return structured detection results.\n\n| Module | Function | Detection Target | Alert Condition |\n|--------|----------|------------------|-----------------|\n| Facial Detection | `detectFace()` | Face presence | 0 or \u003e1 faces |\n| Blink Detection | `isBlinking()` | Eye blink patterns | Abnormal blink rates |\n| Eye Tracking | `gazeDetection()` | Gaze direction | Looking away from screen |\n| Mouth Tracking | `mouthTrack()` | Mouth movement | Speaking detection |\n| Object Detection | `detectObject()` | Unauthorized objects | \u003e1 objects detected |\n| Head Pose | `head_pose_detection()` | Head orientation | Unusual head positions |\n\n### Data Persistence Layer\nThe system maintains two primary data storage mechanisms: a MySQL database for user management and persistent activity logs for exam monitoring data.\n\n**Sources:** [main.py:32-43](), [main.py:63-103](), [README.md:7-13]()\n\n## Component Interaction Patterns\n\n### Real-time Processing Pipeline\nThe system implements a continuous processing pipeline where each video frame undergoes sequential analysis through multiple AI detection modules. The `proctoringAlgo()` function manages this pipeline with frame-by-frame processing.\n\n```mermaid\nsequenceDiagram\n    participant VC as \"cv2.VideoCapture\"\n    participant PA as \"proctoringAlgo()\"\n    participant FD as \"detectFace()\"\n    participant BD as \"isBlinking()\"\n    participant GD as \"gazeDetection()\"\n    participant MT as \"mouthTrack()\"\n    participant OD as \"detectObject()\"\n    participant HP as \"head_pose_detection()\"\n    participant AL as \"Activity Logger\"\n    participant AS as \"Alert System\"\n    \n    loop \"Continuous Processing\"\n        VC-\u003e\u003ePA: \"ret, frame = cam.read()\"\n        PA-\u003e\u003ePA: \"current_time = datetime.now()\"\n        PA-\u003e\u003eFD: \"detectFace(frame)\"\n        FD-\u003e\u003ePA: \"faceCount, faces\"\n        \n        alt \"faceCount == 1\"\n            PA-\u003e\u003eBD: \"isBlinking(faces, frame)\"\n            BD-\u003e\u003ePA: \"blinkStatus\"\n            PA-\u003e\u003eGD: \"gazeDetection(faces, frame)\"\n            GD-\u003e\u003ePA: \"eyeStatus\"\n            PA-\u003e\u003eMT: \"mouthTrack(faces, frame)\"\n            MT-\u003e\u003ePA: \"mouthStatus\"\n            PA-\u003e\u003eOD: \"detectObject(frame)\"\n            OD-\u003e\u003ePA: \"objectName\"\n            PA-\u003e\u003eHP: \"head_pose_detection(faces, frame)\"\n            HP-\u003e\u003ePA: \"headPoseStatus\"\n        else \"faceCount != 1\"\n            PA-\u003e\u003eAS: \"winsound.Beep(frequency, duration)\"\n        end\n        \n        PA-\u003e\u003eAL: \"data_record.append(record)\"\n        \n        alt \"Suspicious Activity\"\n            PA-\u003e\u003eAS: \"winsound.Beep(frequency, duration)\"\n            PA-\u003e\u003ePA: \"time.sleep(4-5)\"\n        end\n    end\n    \n    PA-\u003e\u003eAL: \"Write to activity.txt\"\n```\n\n**Sources:** [main.py:47-127](), [app.py:43-118]()\n\n### Alert and Logging System\nThe system implements a dual-mechanism approach for handling suspicious activities: immediate audio alerts using `winsound.Beep()` and comprehensive activity logging to `activity.txt`.\n\n```mermaid\nflowchart TD\n    DetectionResult[\"AI Detection Result\"]\n    AlertCheck{\"Suspicious Activity?\"}\n    AudioAlert[\"winsound.Beep()\u003cbr/\u003efrequency=2500, duration=1000\"]\n    TimeDelay[\"time.sleep(4-5)\"]\n    ActivityRecord[\"record.append()\u003cbr/\u003etimestamped entry\"]\n    DataRecord[\"data_record.append()\u003cbr/\u003eglobal list\"]\n    ActivityFile[\"activity.txt\u003cbr/\u003epersistent log\"]\n    \n    DetectionResult --\u003e AlertCheck\n    AlertCheck --\u003e|\"Yes\"| AudioAlert\n    AlertCheck --\u003e|\"No\"| ActivityRecord\n    AudioAlert --\u003e TimeDelay\n    TimeDelay --\u003e ActivityRecord\n    ActivityRecord --\u003e DataRecord\n    DataRecord --\u003e ActivityFile\n```\n\n**Alert Conditions:**\n- Multiple faces detected (`faceCount \u003e 1`)\n- No face detected (`faceCount == 0`)\n- Multiple objects detected (`len(objectName) \u003e 1`)\n\n**Sources:** [main.py:32-43](), [main.py:96-99](), [main.py:134-139]()\n\n## Data Flow Architecture\n\n### Input Data Flow\nThe system processes multiple input streams simultaneously:\n\n| Input Type | Source | Processing Module | Output Format |\n|------------|--------|-------------------|---------------|\n| Video Frame | `cv2.VideoCapture(0)` | All AI modules | Detection results |\n| Timestamp | `datetime.now()` | Activity logger | String format |\n| User Input | Web interface | Flask routes | HTTP responses |\n\n### Output Data Flow\nThe system generates multiple output streams:\n\n| Output Type | Destination | Format | Purpose |\n|-------------|-------------|--------|---------|\n| Activity Log | `activity.txt` | Timestamped text | Post-exam review |\n| Video Stream | Web interface | JPEG frames | Real-time monitoring |\n| Audio Alerts | System speakers | Beep sounds | Immediate feedback |\n| Database Records | MySQL | Structured data | User management |\n\n**Sources:** [main.py:118-123](), [main.py:134-139](), [app.py:121-125]()\n\nThis architecture ensures scalable, real-time processing of multiple AI detection algorithms while maintaining comprehensive activity logging and immediate alert capabilities for suspicious behavior detection."])</script><script>self.__next_f.push([1,"17:T2c8f,"])</script><script>self.__next_f.push([1,"# Core Application Components\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [app.py](app.py)\n- [main.py](main.py)\n- [server.py](server.py)\n\n\u003c/details\u003e\n\n\n\n## Purpose and Scope\n\nThis document covers the core application components that form the backbone of the AI-based Online Exam Proctoring System. These components include the main proctoring orchestrator, the Flask web server, and their integration patterns. The core components coordinate between the web interface, AI detection modules, and data persistence layers to deliver real-time exam monitoring capabilities.\n\nFor information about the specific AI detection algorithms, see [AI Detection Engine](#4). For details about the web interface templates and user interactions, see [Web Interface](#5). For database operations and data management, see [Database and Data Management](#6).\n\n## Main Application Orchestrator\n\nThe system's core logic is implemented through the `proctoringAlgo()` function, which exists in two variants depending on the deployment context. This function serves as the central orchestrator that coordinates all AI detection modules and manages the continuous monitoring loop during exam sessions.\n\n### Proctoring Algorithm Architecture\n\n```mermaid\nflowchart TD\n    Start[\"`proctoringAlgo() Start`\"] --\u003e CameraInit[\"`cam = cv2.VideoCapture(0)`\"]\n    CameraInit --\u003e MainLoop[\"`while True/running:`\"]\n    \n    MainLoop --\u003e FrameCapture[\"`ret, frame = cam.read()`\"]\n    FrameCapture --\u003e TimeStamp[\"`current_time = datetime.now()`\"]\n    TimeStamp --\u003e CreateRecord[\"`record = []`\"]\n    \n    CreateRecord --\u003e FaceDetect[\"`faceCount, faces = detectFace(frame)`\"]\n    FaceDetect --\u003e FaceCountCheck{\"`faceCount == 1`\"}\n    \n    FaceCountCheck --\u003e|Yes| BlinkDetect[\"`blinkStatus = isBlinking(faces, frame)`\"]\n    FaceCountCheck --\u003e|No| FaceCountHandler[\"`faceCount_detection(faceCount)`\"]\n    \n    BlinkDetect --\u003e GazeDetect[\"`eyeStatus = gazeDetection(faces, frame)`\"]\n    GazeDetect --\u003e MouthTrack[\"`mouthTrack(faces, frame)`\"]\n    MouthTrack --\u003e ObjectDetect[\"`objectName = detectObject(frame)`\"]\n    ObjectDetect --\u003e HeadPose[\"`head_pose_detection(faces, frame)`\"]\n    \n    FaceCountHandler --\u003e RecordAppend[\"`data_record.append(record)`\"]\n    HeadPose --\u003e RecordAppend\n    \n    RecordAppend --\u003e MainLoop\n    \n    MainLoop --\u003e|Exit| Cleanup[\"`cam.release()`\"]\n    Cleanup --\u003e SaveActivity[\"`activity.txt write`\"]\n    \n    style Start fill:#e1f5fe\n    style MainLoop fill:#f3e5f5\n    style FaceCountCheck fill:#fff3e0\n    style SaveActivity fill:#ffebee\n```\n\n**Main Proctoring Loop Components**\n\nThe `proctoringAlgo()` function implements a continuous monitoring loop with the following key components:\n\n| Component | Function Call | Purpose |\n|-----------|--------------|---------|\n| Face Detection | `detectFace(frame)` | Detects and counts faces in the video frame |\n| Face Count Validation | `faceCount_detection(faceCount)` | Triggers alerts for multiple or missing faces |\n| Blink Detection | `isBlinking(faces, frame)` | Monitors natural blinking patterns |\n| Gaze Tracking | `gazeDetection(faces, frame)` | Tracks eye movement and gaze direction |\n| Mouth Monitoring | `mouthTrack(faces, frame)` | Detects mouth opening/closing |\n| Object Detection | `detectObject(frame)` | Identifies unauthorized objects |\n| Head Pose Analysis | `head_pose_detection(faces, frame)` | Monitors head orientation |\n\nSources: [app.py:43-118](), [main.py:47-127]()\n\n### Alert System Integration\n\nThe core orchestrator includes an integrated alert system that triggers audio warnings when suspicious activities are detected:\n\n```mermaid\nsequenceDiagram\n    participant Orchestrator as \"proctoringAlgo()\"\n    participant FaceCount as \"faceCount_detection()\"\n    participant WinSound as \"winsound.Beep()\"\n    participant Activity as \"data_record\"\n    \n    Orchestrator-\u003e\u003eFaceCount: \"faceCount parameter\"\n    \n    alt \"faceCount \u003e 1\"\n        FaceCount-\u003e\u003eFaceCount: \"time.sleep(5)\"\n        FaceCount-\u003e\u003eWinSound: \"Beep(2500, 1000)\"\n        FaceCount-\u003e\u003eOrchestrator: \"Multiple faces detected\"\n    else \"faceCount == 0\"\n        FaceCount-\u003e\u003eFaceCount: \"time.sleep(3-5)\"\n        FaceCount-\u003e\u003eWinSound: \"Beep(2500, 1000)\"\n        FaceCount-\u003e\u003eOrchestrator: \"No face detected\"\n    else \"faceCount == 1\"\n        FaceCount-\u003e\u003eOrchestrator: \"Face detecting properly\"\n    end\n    \n    Orchestrator-\u003e\u003eActivity: \"record.append(remark)\"\n```\n\nSources: [app.py:28-39](), [main.py:32-43](), [app.py:16-18](), [main.py:20-22]()\n\n## Flask Server Component\n\nThe Flask server component in `server.py` provides the web interface and coordinates between the web layer and the proctoring system. It implements multiple endpoints that handle different aspects of the exam proctoring workflow.\n\n### Server Route Architecture\n\n```mermaid\ngraph TD\n    FlaskApp[\"`Flask(__name__)`\"] --\u003e AuthRoutes[\"`Authentication Routes`\"]\n    FlaskApp --\u003e UIRoutes[\"`UI Routes`\"]\n    FlaskApp --\u003e VideoRoutes[\"`Video Routes`\"]\n    FlaskApp --\u003e ControlRoutes[\"`Control Routes`\"]\n    \n    AuthRoutes --\u003e SignupRoute[\"`@app.route('/signup_data')`\"]\n    AuthRoutes --\u003e LoginRoute[\"`@app.route('/login_data')`\"]\n    \n    UIRoutes --\u003e IndexRoute[\"`@app.route('/')`\"]\n    UIRoutes --\u003e QuizRoute[\"`@app.route('/quiz_html')`\"]\n    \n    VideoRoutes --\u003e VideoFeed[\"`@app.route('/video_feed')`\"]\n    \n    ControlRoutes --\u003e StopCamera[\"`@app.route('/stop_camera')`\"]\n    \n    SignupRoute --\u003e DBHelper[\"`insert_signup()`\"]\n    LoginRoute --\u003e DBHelper[\"`search_login_credentials()`\"]\n    \n    IndexRoute --\u003e IndexTemplate[\"`render_template('index.html')`\"]\n    QuizRoute --\u003e QuizTemplate[\"`render_template('quiz.html')`\"]\n    \n    VideoFeed --\u003e ProctoringAlgo[\"`proctoringAlgo()`\"]\n    VideoFeed --\u003e Response[\"`Response(mimetype='multipart/x-mixed-replace')`\"]\n    \n    StopCamera --\u003e GlobalRunning[\"`global running = False`\"]\n    StopCamera --\u003e MainApp[\"`main_app()`\"]\n    StopCamera --\u003e OSExit[\"`os._exit(0)`\"]\n```\n\n### Video Streaming Integration\n\nThe Flask server integrates with the proctoring system through the `/video_feed` endpoint, which streams video frames while simultaneously running AI detection:\n\n```mermaid\nsequenceDiagram\n    participant Client as \"Web Browser\"\n    participant FlaskServer as \"server.py\"\n    participant ProctoringMain as \"main.py proctoringAlgo()\"\n    participant AIModules as \"AI Detection Modules\"\n    participant DataRecord as \"data_record\"\n    \n    Client-\u003e\u003eFlaskServer: \"GET /video_feed\"\n    FlaskServer-\u003e\u003eProctoringMain: \"proctoringAlgo() generator\"\n    \n    loop \"Continuous Streaming\"\n        ProctoringMain-\u003e\u003eAIModules: \"Process frame\"\n        AIModules-\u003e\u003eProctoringMain: \"Detection results\"\n        ProctoringMain-\u003e\u003eDataRecord: \"Append record\"\n        ProctoringMain-\u003e\u003eProctoringMain: \"cv2.imencode('.jpg', frame)\"\n        ProctoringMain-\u003e\u003eFlaskServer: \"yield frame bytes\"\n        FlaskServer-\u003e\u003eClient: \"multipart/x-mixed-replace response\"\n    end\n    \n    Client-\u003e\u003eFlaskServer: \"GET /stop_camera\"\n    FlaskServer-\u003e\u003eProctoringMain: \"running = False\"\n    FlaskServer-\u003e\u003eFlaskServer: \"main_app()\"\n    FlaskServer-\u003e\u003eFlaskServer: \"os._exit(0)\"\n```\n\nSources: [server.py:50-53](), [server.py:57-64](), [main.py:118-123]()\n\n## Application Variants and Entry Points\n\nThe system provides two main entry points depending on the deployment scenario:\n\n### Standalone Application (`app.py`)\n\nThe standalone version is designed for testing and development purposes:\n\n| Feature | Implementation | Purpose |\n|---------|----------------|---------|\n| Direct execution | `if __name__ == '__main__': proctoringAlgo()` | Command-line testing |\n| OpenCV display | `cv2.imshow('Frame', frame)` | Local frame visualization |\n| Manual termination | `cv2.waitKey(1) \u0026 0xFF == ord('q')` | Keyboard-controlled exit |\n| Activity logging | Direct file write to `activity.txt` | Immediate data persistence |\n\n### Web-Integrated Application (`main.py`)\n\nThe web-integrated version is optimized for Flask streaming:\n\n| Feature | Implementation | Purpose |\n|---------|----------------|---------|\n| Generator function | `yield (b'--frame\\r\\n'...)` | HTTP streaming compatibility |\n| Global control | `global running` variable | Server-controlled execution |\n| Frame encoding | `cv2.imencode('.jpg', frame)` | Web-compatible image format |\n| Deferred logging | `main_app()` function | Cleanup on session end |\n\nSources: [app.py:117-125](), [main.py:131-139](), [main.py:51](), [main.py:118-123]()\n\n## Data Flow and Control Mechanisms\n\n### Global State Management\n\nThe system uses several global variables and control mechanisms to coordinate between components:\n\n```mermaid\nstateDiagram-v2\n    [*] --\u003e CameraInit : \"cam = cv2.VideoCapture(0)\"\n    \n    CameraInit --\u003e MonitoringActive : \"running = True\"\n    \n    MonitoringActive --\u003e ProcessingFrame : \"Frame capture\"\n    ProcessingFrame --\u003e AIDetection : \"Multiple AI modules\"\n    AIDetection --\u003e RecordData : \"data_record.append()\"\n    RecordData --\u003e ProcessingFrame : \"Continue loop\"\n    \n    ProcessingFrame --\u003e MonitoringStop : \"/stop_camera endpoint\"\n    MonitoringStop --\u003e DataPersistence : \"main_app()\"\n    DataPersistence --\u003e ServerShutdown : \"os._exit(0)\"\n    \n    ServerShutdown --\u003e [*]\n```\n\n### Activity Recording Structure\n\nThe system maintains a global `data_record` list that accumulates timestamped activity data:\n\n| Data Element | Source Function | Format |\n|--------------|-----------------|--------|\n| Timestamp | `datetime.now().strftime(\"%H:%M:%S.%f\")` | String |\n| Face Status | `faceCount_detection()` | Descriptive string |\n| Blink Status | `isBlinking()` | Status with count |\n| Gaze Status | `gazeDetection()` | Direction string |\n| Mouth Status | `mouthTrack()` | Position string |\n| Objects | `detectObject()` | Object list |\n| Head Pose | `head_pose_detection()` | Orientation string |\n\nSources: [app.py:13-14](), [main.py:14-15](), [app.py:51-104](), [main.py:55-110]()\n\n## Integration Patterns\n\nThe core application components follow a layered integration pattern where the Flask server acts as the coordination layer between the web interface and the proctoring engine:\n\n```mermaid\ngraph TB\n    subgraph \"Web Layer\"\n        Templates[\"`HTML Templates`\"]\n        StaticAssets[\"`CSS/JS Assets`\"]\n    end\n    \n    subgraph \"Server Layer\"\n        FlaskRoutes[\"`Flask Routes`\"]\n        CORS[\"`CORS(app)`\"]\n        DBIntegration[\"`db_helper imports`\"]\n    end\n    \n    subgraph \"Application Layer\"\n        ProctoringAlgo[\"`proctoringAlgo()`\"]\n        MainApp[\"`main_app()`\"]\n        GlobalState[\"`global running, data_record`\"]\n    end\n    \n    subgraph \"AI Layer\"\n        FacialDetections[\"`facial_detections`\"]\n        BlinkDetection[\"`blink_detection`\"]\n        ObjectDetection[\"`object_detection`\"]\n        EyeTracker[\"`eye_tracker`\"]\n        MouthTracking[\"`mouth_tracking`\"]\n        HeadPose[\"`head_pose_estimation`\"]\n    end\n    \n    Templates --\u003e FlaskRoutes\n    StaticAssets --\u003e FlaskRoutes\n    FlaskRoutes --\u003e ProctoringAlgo\n    FlaskRoutes --\u003e DBIntegration\n    ProctoringAlgo --\u003e GlobalState\n    MainApp --\u003e GlobalState\n    \n    ProctoringAlgo --\u003e FacialDetections\n    ProctoringAlgo --\u003e BlinkDetection\n    ProctoringAlgo --\u003e ObjectDetection\n    ProctoringAlgo --\u003e EyeTracker\n    ProctoringAlgo --\u003e MouthTracking\n    ProctoringAlgo --\u003e HeadPose\n```\n\nSources: [server.py:1-9](), [server.py:39-70](), [app.py:4-11](), [main.py:5-11]()"])</script><script>self.__next_f.push([1,"18:T23c4,"])</script><script>self.__next_f.push([1,"# Activity Logging System\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [activity.txt](activity.txt)\n- [app.py](app.py)\n- [main.py](main.py)\n\n\u003c/details\u003e\n\n\n\n## Purpose and Scope\n\nThe Activity Logging System is responsible for collecting, timestamping, and persisting real-time monitoring data from all AI detection modules during exam sessions. This system creates a comprehensive audit trail of student behavior that can be reviewed post-exam for proctoring violations and assessment integrity.\n\nFor information about the core proctoring algorithm that generates this data, see [Core Application Components](#3.1). For details about the individual AI detection modules that feed into this system, see [AI Detection Engine](#4).\n\n## System Overview\n\nThe Activity Logging System operates as a centralized data collection service that aggregates outputs from multiple AI detection modules into timestamped records. Each record represents a single frame of analysis and contains detection results from facial analysis, object detection, and behavioral monitoring systems.\n\n### Activity Logging Architecture\n\n```mermaid\ngraph TB\n    subgraph \"AI Detection Modules\"\n        FaceDetect[detectFace]\n        BlinkDetect[isBlinking]\n        GazeDetect[gazeDetection]\n        MouthTrack[mouthTrack]\n        ObjectDetect[detectObject]\n        HeadPose[head_pose_detection]\n    end\n    \n    subgraph \"Data Collection Layer\"\n        DataRecord[data_record]\n        RecordBuilder[record]\n        TimeStamp[datetime.now]\n    end\n    \n    subgraph \"Persistence Layer\"\n        ActivityFile[activity.txt]\n        FileWriter[file.write]\n    end\n    \n    subgraph \"Main Processing Loop\"\n        ProctoringAlgo[proctoringAlgo]\n        MainApp[main_app]\n    end\n    \n    ProctoringAlgo --\u003e TimeStamp\n    TimeStamp --\u003e RecordBuilder\n    \n    ProctoringAlgo --\u003e FaceDetect\n    ProctoringAlgo --\u003e BlinkDetect\n    ProctoringAlgo --\u003e GazeDetect\n    ProctoringAlgo --\u003e MouthTrack\n    ProctoringAlgo --\u003e ObjectDetect\n    ProctoringAlgo --\u003e HeadPose\n    \n    FaceDetect --\u003e RecordBuilder\n    BlinkDetect --\u003e RecordBuilder\n    GazeDetect --\u003e RecordBuilder\n    MouthTrack --\u003e RecordBuilder\n    ObjectDetect --\u003e RecordBuilder\n    HeadPose --\u003e RecordBuilder\n    \n    RecordBuilder --\u003e DataRecord\n    DataRecord --\u003e MainApp\n    MainApp --\u003e FileWriter\n    FileWriter --\u003e ActivityFile\n```\n\n**Sources:** [app.py:13-14](), [app.py:51](), [app.py:104](), [app.py:121-125](), [main.py:14-15](), [main.py:135-139]()\n\n## Data Collection Process\n\nThe logging system operates within the main processing loop of the proctoring algorithm, capturing data from each AI detection module on every frame iteration.\n\n### Data Flow During Frame Processing\n\n```mermaid\nsequenceDiagram\n    participant Loop as \"proctoringAlgo()\"\n    participant Record as \"record[]\"\n    participant Time as \"datetime.now()\"\n    participant Face as \"detectFace()\"\n    participant Blink as \"isBlinking()\"\n    participant Gaze as \"gazeDetection()\"\n    participant Mouth as \"mouthTrack()\"\n    participant Object as \"detectObject()\"\n    participant Head as \"head_pose_detection()\"\n    participant DataRec as \"data_record[]\"\n    participant File as \"activity.txt\"\n    \n    Loop-\u003e\u003eRecord: Initialize empty record\n    Loop-\u003e\u003eTime: Get current timestamp\n    Time-\u003e\u003eRecord: Append timestamp\n    \n    Loop-\u003e\u003eFace: Process frame\n    Face-\u003e\u003eRecord: Append face detection result\n    \n    alt Face detected (faceCount == 1)\n        Loop-\u003e\u003eBlink: Process facial landmarks\n        Blink-\u003e\u003eRecord: Append blink status\n        \n        Loop-\u003e\u003eGaze: Process eye regions\n        Gaze-\u003e\u003eRecord: Append gaze direction\n        \n        Loop-\u003e\u003eMouth: Process mouth landmarks\n        Mouth-\u003e\u003eRecord: Append mouth status\n        \n        Loop-\u003e\u003eObject: Process full frame\n        Object-\u003e\u003eRecord: Append detected objects\n        \n        Loop-\u003e\u003eHead: Process facial landmarks\n        Head-\u003e\u003eRecord: Append head pose\n    end\n    \n    Record-\u003e\u003eDataRec: Append complete record\n    \n    Note over Loop,DataRec: Process continues for each frame\n    \n    Loop-\u003e\u003eFile: Write complete log at session end\n```\n\n**Sources:** [app.py:47-104](), [main.py:47-110]()\n\n## Log Format and Structure\n\nEach activity record follows a consistent 7-element structure that captures comprehensive behavioral data for a single frame analysis.\n\n### Record Structure\n\n| Index | Field | Type | Description | Example |\n|-------|-------|------|-------------|---------|\n| 0 | Timestamp | String | Current time with microseconds | `'21:37:01.965395'` |\n| 1 | Face Status | String | Face detection result | `'Face detecting properly.'` |\n| 2 | Blink Status | String | Blink detection or count | `'No Blink'` or `'Blink count: 1'` |\n| 3 | Gaze Direction | String | Eye gaze direction | `'center'`, `'left'`, `'right'` |\n| 4 | Mouth Status | String | Mouth position | `'Mouth Close'`, `'Mouth Open'` |\n| 5 | Object Detection | List | Detected objects with confidence | `[('person', 0.6870696544647217)]` |\n| 6 | Head Pose | String/Integer | Head orientation status | `'Head Down'`, `'Head Up'`, `-1` |\n\n### Sample Log Entries\n\n```\n['21:37:01.965395', 'Face detecting properly.', 'No Blink', 'center', 'Mouth Close', [('person', 0.6870696544647217)], -1]\n['21:37:20.276865', 'Face detecting properly.', 'Blink count: 1', 'center', 'Mouth Close', [('person', 0.8016493916511536)], -1]\n['21:37:14.285137', 'Face detecting properly.', 'No Blink', 'left', 'Mouth Close', [('person', 0.7348140478134155)], 'Head Down']\n```\n\n**Sources:** [activity.txt:1-55](), [app.py:54-98](), [main.py:58-103]()\n\n## File Output Mechanism\n\nThe logging system persists activity data to a text file using a simple but effective write-once strategy at the end of each proctoring session.\n\n### File Writing Process\n\n```mermaid\nflowchart TD\n    DataRecord[data_record list] --\u003e JoinOperation[\"\\n\".join(map(str, data_record))]\n    JoinOperation --\u003e ActivityVal[activityVal string]\n    ActivityVal --\u003e FileOpen[\"open('activity.txt', 'w')\"]\n    FileOpen --\u003e FileWrite[\"file.write(str(activityVal))\"]\n    FileWrite --\u003e ActivityTxt[activity.txt]\n    \n    note1[\"Each record becomes one line\"]\n    note2[\"Overwrites existing file\"]\n    \n    JoinOperation -.-\u003e note1\n    FileOpen -.-\u003e note2\n```\n\nThe file writing implementation uses Python's built-in string operations to convert the collected data structure into a human-readable format:\n\n1. **Data Conversion**: The `data_record` list is converted to a string using `\"\\n\".join(map(str, data_record))`\n2. **File Creation**: Opens `activity.txt` in write mode, overwriting any existing content\n3. **Data Persistence**: Writes the complete activity log as a single operation\n\n**Sources:** [app.py:121-125](), [main.py:135-139]()\n\n## Integration with Detection Modules\n\nThe Activity Logging System serves as the central integration point for all AI detection modules, collecting their outputs in a standardized format.\n\n### Detection Module Integration Map\n\n| Detection Module | Function Call | Data Captured | Record Position |\n|------------------|---------------|---------------|-----------------|\n| Facial Detection | `detectFace(frame)` | Face count and detection status | Index 1 |\n| Blink Detection | `isBlinking(faces, frame)` | Blink status and count | Index 2 |\n| Gaze Detection | `gazeDetection(faces, frame)` | Eye gaze direction | Index 3 |\n| Mouth Tracking | `mouthTrack(faces, frame)` | Mouth open/close status | Index 4 |\n| Object Detection | `detectObject(frame)` | Detected objects with confidence | Index 5 |\n| Head Pose Estimation | `head_pose_detection(faces, frame)` | Head orientation status | Index 6 |\n\n### Module Data Processing Flow\n\n```mermaid\ngraph LR\n    subgraph \"Frame Processing\"\n        Frame[Video Frame]\n        Faces[Detected Faces]\n    end\n    \n    subgraph \"Detection Modules\"\n        FD[facial_detections.py]\n        BD[blink_detection.py]\n        GD[eye_tracker.py]\n        MT[mouth_tracking.py]\n        OD[object_detection.py]\n        HP[head_pose_estimation.py]\n    end\n    \n    subgraph \"Logging Integration\"\n        Record[record list]\n        Append[append operations]\n        DataRec[data_record]\n    end\n    \n    Frame --\u003e FD\n    Frame --\u003e OD\n    FD --\u003e Faces\n    \n    Faces --\u003e BD\n    Faces --\u003e GD\n    Faces --\u003e MT\n    Faces --\u003e HP\n    \n    FD --\u003e Record\n    BD --\u003e Record\n    GD --\u003e Record\n    MT --\u003e Record\n    OD --\u003e Record\n    HP --\u003e Record\n    \n    Record --\u003e Append\n    Append --\u003e DataRec\n```\n\n**Sources:** [app.py:59-98](), [main.py:63-103](), [app.py:4-9](), [main.py:5-10]()\n\n## Error Handling and Edge Cases\n\nThe logging system handles various edge cases in the detection pipeline:\n\n- **No Face Detected**: Records face status as \"No face has been detected.\" and continues logging\n- **Multiple Faces**: Records face status as \"Multiple faces has been detected.\" with audio alert\n- **Missing Objects**: Empty object detection list `[]` is recorded when no objects are detected\n- **Detection Failures**: Invalid or `-1` values are recorded for failed head pose detection\n\n**Sources:** [app.py:28-39](), [main.py:32-43](), [activity.txt:25]()"])</script><script>self.__next_f.push([1,"19:T2550,"])</script><script>self.__next_f.push([1,"# AI Detection Engine\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [blink_detection.py](blink_detection.py)\n- [eye_tracker.py](eye_tracker.py)\n- [facial_detections.py](facial_detections.py)\n- [object_detection.py](object_detection.py)\n\n\u003c/details\u003e\n\n\n\n## Purpose and Scope\n\nThe AI Detection Engine is the core component responsible for real-time behavioral analysis during proctored exams. This system combines multiple computer vision and audio processing modules to continuously monitor student behavior and detect potential violations. The engine processes video frames and audio input to identify suspicious activities such as looking away from the screen, presence of unauthorized objects, or unusual sounds.\n\nFor detailed information about specific facial analysis capabilities, see [Facial Analysis Modules](#4.1). For object and audio detection specifics, see [Object and Audio Detection](#4.2).\n\n## Architecture Overview\n\nThe AI Detection Engine operates as a modular pipeline where each detection module processes the same input frame independently and returns analysis results to the main proctoring system.\n\n### Core Detection Pipeline\n\n```mermaid\nflowchart TD\n    VideoInput[\"Video Input\"] --\u003e FrameProcessor[\"Frame Processing\"]\n    AudioInput[\"Audio Input\"] --\u003e AudioProcessor[\"Audio Processing\"]\n    \n    FrameProcessor --\u003e FaceDetection[\"detectFace()\"]\n    FrameProcessor --\u003e ObjectDetection[\"detectObject()\"]\n    \n    FaceDetection --\u003e FaceCount{Face Count Check}\n    FaceCount --\u003e|\"1 Face\"| FacialAnalysis[\"Facial Analysis Pipeline\"]\n    FaceCount --\u003e|\"≠ 1 Face\"| SuspiciousAlert[\"Suspicious Activity Alert\"]\n    \n    FacialAnalysis --\u003e BlinkDetection[\"isBlinking()\"]\n    FacialAnalysis --\u003e GazeTracking[\"gazeDetection()\"]\n    FacialAnalysis --\u003e HeadPoseEstimation[\"Head Pose Analysis\"]\n    FacialAnalysis --\u003e MouthTracking[\"Mouth Movement Analysis\"]\n    \n    ObjectDetection --\u003e ObjectCount{Object Count Check}\n    ObjectCount --\u003e|\"\u003e 1 Object\"| ObjectAlert[\"Unauthorized Object Alert\"]\n    ObjectCount --\u003e|\"≤ 1 Object\"| ObjectNormal[\"Normal State\"]\n    \n    AudioProcessor --\u003e VolumeAnalysis[\"Volume Threshold Check\"]\n    VolumeAnalysis --\u003e|\"Above Threshold\"| AudioAlert[\"Suspicious Sound Alert\"]\n    VolumeAnalysis --\u003e|\"Below Threshold\"| AudioNormal[\"Normal State\"]\n    \n    BlinkDetection --\u003e ActivityLogger[\"Activity Logging\"]\n    GazeTracking --\u003e ActivityLogger\n    HeadPoseEstimation --\u003e ActivityLogger\n    MouthTracking --\u003e ActivityLogger\n    ObjectNormal --\u003e ActivityLogger\n    AudioNormal --\u003e ActivityLogger\n    SuspiciousAlert --\u003e ActivityLogger\n    ObjectAlert --\u003e ActivityLogger\n    AudioAlert --\u003e ActivityLogger\n```\n\n**Sources:** `facial_detections.py`, `blink_detection.py`, `eye_tracker.py`, `object_detection.py`\n\n## Module Dependencies and Shared Resources\n\nAll facial analysis modules share common dependencies and models, creating an efficient resource utilization pattern.\n\n### Shared Model Architecture\n\n```mermaid\ngraph TB\n    subgraph \"Shared Resources\"\n        ShapePredictor[\"shape_predictor_68_face_landmarks.dat\"]\n        DlibDetector[\"dlib.get_frontal_face_detector()\"]\n    end\n    \n    subgraph \"Facial Analysis Modules\"\n        FacialDetections[\"facial_detections.py\"]\n        BlinkDetection[\"blink_detection.py\"] \n        EyeTracker[\"eye_tracker.py\"]\n        HeadPose[\"head_pose_estimation.py\"]\n        MouthTracking[\"mouth_tracking.py\"]\n    end\n    \n    subgraph \"Object Detection Module\"\n        ObjectDetectionModule[\"object_detection.py\"]\n        YOLOWeights[\"yolov3-tiny.weights\"]\n        YOLOConfig[\"yolov3-tiny.cfg\"]\n        COCOLabels[\"coco.names\"]\n    end\n    \n    ShapePredictor --\u003e FacialDetections\n    ShapePredictor --\u003e BlinkDetection\n    ShapePredictor --\u003e EyeTracker\n    ShapePredictor --\u003e HeadPose\n    ShapePredictor --\u003e MouthTracking\n    \n    DlibDetector --\u003e FacialDetections\n    \n    YOLOWeights --\u003e ObjectDetectionModule\n    YOLOConfig --\u003e ObjectDetectionModule\n    COCOLabels --\u003e ObjectDetectionModule\n```\n\n**Sources:** `facial_detections.py:6-8`, `blink_detection.py:6-7`, `eye_tracker.py:6-7`, `object_detection.py:6,11-12`\n\n## Detection Module Functions\n\nEach detection module exposes a primary function that processes input frames and returns analysis results.\n\n| Module | Primary Function | Input Parameters | Return Value | Purpose |\n|--------|------------------|------------------|--------------|---------|\n| Facial Detection | `detectFace(frame)` | Video frame | `(faceCount, faces)` | Detects and localizes faces |\n| Blink Detection | `isBlinking(faces, frame)` | Detected faces, frame | Eye aspect ratios and blink status | Monitors eye closure patterns |\n| Gaze Tracking | `gazeDetection(faces, frame)` | Detected faces, frame | Gaze direction string | Determines where eyes are looking |\n| Object Detection | `detectObject(frame)` | Video frame | List of detected objects | Identifies unauthorized items |\n\n**Sources:** `facial_detections.py:11-65`, `blink_detection.py:26-73`, `eye_tracker.py:41-103`, `object_detection.py:26-81`\n\n## Facial Landmark Processing\n\nThe facial analysis modules utilize a standardized 68-point facial landmark system for consistent feature extraction.\n\n### Landmark Point Mapping\n\n```mermaid\ngraph LR\n    subgraph \"68-Point Facial Landmarks\"\n        LeftEye[\"Left Eye Points\u003cbr/\u003e[36,37,38,39,40,41]\"]\n        RightEye[\"Right Eye Points\u003cbr/\u003e[42,43,44,45,46,47]\"]\n        Jawline[\"Jawline Points\u003cbr/\u003e[0-16]\"]\n        Eyebrows[\"Eyebrow Points\u003cbr/\u003e[17-26]\"]\n        Nose[\"Nose Points\u003cbr/\u003e[27-35]\"]\n        Mouth[\"Mouth Points\u003cbr/\u003e[48-67]\"]\n    end\n    \n    subgraph \"Processing Functions\"\n        BlinkAnalysis[\"isBlinking()\u003cbr/\u003eEye Aspect Ratio\"]\n        GazeAnalysis[\"gazeDetection()\u003cbr/\u003ePupil Position\"]\n        LandmarkExtraction[\"shapePredictor()\u003cbr/\u003eFeature Extraction\"]\n    end\n    \n    LandmarkExtraction --\u003e LeftEye\n    LandmarkExtraction --\u003e RightEye\n    LandmarkExtraction --\u003e Mouth\n    \n    LeftEye --\u003e BlinkAnalysis\n    RightEye --\u003e BlinkAnalysis\n    LeftEye --\u003e GazeAnalysis\n    RightEye --\u003e GazeAnalysis\n```\n\n**Sources:** `blink_detection.py:33-34`, `eye_tracker.py:49-50`, `facial_detections.py:54-62`\n\n## Object Detection Configuration\n\nThe YOLO-based object detection system uses a lightweight YOLOv3-tiny model for real-time performance.\n\n### YOLO Model Configuration\n\n| Component | File Path | Purpose |\n|-----------|-----------|---------|\n| Model Weights | `object_detection_model/weights/yolov3-tiny.weights` | Pre-trained neural network parameters |\n| Network Config | `object_detection_model/config/yolov3-tiny.cfg` | Network architecture definition |\n| Object Labels | `object_detection_model/objectLabels/coco.names` | COCO dataset class names |\n\nThe detection pipeline processes frames with the following parameters:\n- **Input Size**: 220x220 pixels [object_detection.py:32]()\n- **Confidence Threshold**: 0.5 [object_detection.py:51]()\n- **NMS Threshold**: 0.4 [object_detection.py:68]()\n\n**Sources:** `object_detection.py:6,11-12,32,51,68`\n\n## Detection Thresholds and Parameters\n\nEach detection module uses carefully tuned thresholds to balance sensitivity and false positive rates.\n\n### Critical Detection Parameters\n\n```mermaid\ngraph TD\n    subgraph \"Blink Detection Thresholds\"\n        BlinkRatio[\"Eye Aspect Ratio ≥ 3.6\u003cbr/\u003eIndicates Blink\"]\n        BlinkOptimal[\"Optimal Threshold: 5.1\u003cbr/\u003e(Comment Reference)\"]\n    end\n    \n    subgraph \"Gaze Detection Thresholds\"\n        GazeRatio[\"Trial Ratio: 1.2\u003cbr/\u003eSide Detection Sensitivity\"]\n        GazeLeftRight[\"Left/Right Detection\u003cbr/\u003eBased on White Pixel Ratio\"]\n    end\n    \n    subgraph \"Object Detection Thresholds\"\n        ConfidenceThresh[\"Confidence \u003e 0.5\u003cbr/\u003eObject Detection Threshold\"]\n        NMSThresh[\"NMS Threshold: 0.4\u003cbr/\u003eDuplicate Suppression\"]\n    end\n    \n    BlinkRatio --\u003e ActivityDecision[\"Log Blink Activity\"]\n    GazeLeftRight --\u003e ActivityDecision\n    ConfidenceThresh --\u003e ObjectDecision[\"Log Object Detection\"]\n    NMSThresh --\u003e ObjectDecision\n```\n\n**Sources:** `blink_detection.py:65-66`, `eye_tracker.py:45,94-99`, `object_detection.py:51,68`\n\n## Integration with Main System\n\nThe AI Detection Engine operates within the broader proctoring system through standardized interfaces that allow the main orchestrator to coordinate all detection activities.\n\n### System Integration Flow\n\n```mermaid\nsequenceDiagram\n    participant MainApp as \"Main Application\"\n    participant AIEngine as \"AI Detection Engine\"\n    participant FaceModule as \"facial_detections.py\"\n    participant BlinkModule as \"blink_detection.py\"\n    participant EyeModule as \"eye_tracker.py\"\n    participant ObjectModule as \"object_detection.py\"\n    participant Logger as \"Activity Logger\"\n    \n    MainApp-\u003e\u003eAIEngine: \"Process Frame\"\n    AIEngine-\u003e\u003eFaceModule: \"detectFace(frame)\"\n    FaceModule-\u003e\u003eAIEngine: \"(faceCount, faces)\"\n    \n    alt Face Count == 1\n        AIEngine-\u003e\u003eBlinkModule: \"isBlinking(faces, frame)\"\n        AIEngine-\u003e\u003eEyeModule: \"gazeDetection(faces, frame)\"\n        BlinkModule-\u003e\u003eAIEngine: \"Blink Status\"\n        EyeModule-\u003e\u003eAIEngine: \"Gaze Direction\"\n    else Face Count != 1\n        AIEngine-\u003e\u003eLogger: \"Log Suspicious Activity\"\n    end\n    \n    AIEngine-\u003e\u003eObjectModule: \"detectObject(frame)\"\n    ObjectModule-\u003e\u003eAIEngine: \"Object List\"\n    \n    AIEngine-\u003e\u003eLogger: \"Log All Detection Results\"\n    AIEngine-\u003e\u003eMainApp: \"Analysis Complete\"\n```\n\n**Sources:** `facial_detections.py:11-65`, `blink_detection.py:26-73`, `eye_tracker.py:41-103`, `object_detection.py:26-81`"])</script><script>self.__next_f.push([1,"1a:T2c53,"])</script><script>self.__next_f.push([1,"# Facial Analysis Modules\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [blink_detection.py](blink_detection.py)\n- [eye_tracker.py](eye_tracker.py)\n- [facial_detections.py](facial_detections.py)\n- [head_pose_estimation.py](head_pose_estimation.py)\n- [mouth_tracking.py](mouth_tracking.py)\n\n\u003c/details\u003e\n\n\n\n## Purpose and Scope\n\nThe Facial Analysis Modules comprise the core computer vision components responsible for monitoring student behavior during proctored exams. These modules analyze facial features, head positioning, eye movements, blink patterns, and mouth activity to detect potential violations of exam protocols. This documentation covers the five primary analysis modules: face detection, eye tracking, blink detection, head pose estimation, and mouth tracking.\n\nFor information about object detection and audio monitoring capabilities, see [Object and Audio Detection](#4.2). For details on how these modules integrate with the main proctoring system, see [Core Application Components](#3.1).\n\n## Module Architecture Overview\n\nThe facial analysis system consists of five interconnected modules that work together to provide comprehensive facial behavior monitoring:\n\n**Facial Analysis Module Architecture**\n```mermaid\ngraph TD\n    Input[\"Video Frame Input\"] --\u003e FaceDetector[\"detectFace()\"]\n    \n    FaceDetector --\u003e FaceCount{Face Count}\n    FaceCount --\u003e|\"== 1\"| FacialLandmarks[\"68-Point Facial Landmarks\u003cbr/\u003eshape_predictor_68_face_landmarks.dat\"]\n    FaceCount --\u003e|\"!= 1\"| Alert[\"Suspicious Activity Alert\"]\n    \n    FacialLandmarks --\u003e EyeTracker[\"gazeDetection()\"]\n    FacialLandmarks --\u003e BlinkDetector[\"isBlinking()\"]\n    FacialLandmarks --\u003e HeadPose[\"head_pose_detection()\"]\n    FacialLandmarks --\u003e MouthTracker[\"mouthTrack()\"]\n    \n    EyeTracker --\u003e GazeResult[\"Gaze Direction\u003cbr/\u003e(left/right/center)\"]\n    BlinkDetector --\u003e BlinkResult[\"Blink Status\u003cbr/\u003e(Blink/No Blink)\"]\n    HeadPose --\u003e PoseResult[\"Head Orientation\u003cbr/\u003e(Up/Down/Left/Right)\"]\n    MouthTracker --\u003e MouthResult[\"Mouth State\u003cbr/\u003e(Open/Close)\"]\n    \n    GazeResult --\u003e ActivityLogger[\"Activity Logging System\"]\n    BlinkResult --\u003e ActivityLogger\n    PoseResult --\u003e ActivityLogger\n    MouthResult --\u003e ActivityLogger\n    Alert --\u003e ActivityLogger\n```\n\nSources: [facial_detections.py:11-65](), [eye_tracker.py:41-103](), [blink_detection.py:26-73](), [head_pose_estimation.py:105-178](), [mouth_tracking.py:15-36]()\n\n## Face Detection Foundation\n\nThe `detectFace()` function in `facial_detections.py` serves as the foundation for all other facial analysis modules. It uses dlib's HOG-based face detector to locate faces in video frames and extracts 68 facial landmarks.\n\n| Component | Implementation | Purpose |\n|-----------|----------------|---------|\n| Face Detector | `dlib.get_frontal_face_detector()` | Locates faces in grayscale frames |\n| Shape Predictor | `dlib.shape_predictor()` | Extracts 68 facial landmark points |\n| Landmark Model | `shape_predictor_68_face_landmarks.dat` | Pre-trained model for facial landmarks |\n\n**Face Detection Process Flow**\n```mermaid\nflowchart TD\n    VideoFrame[\"Video Frame (BGR)\"] --\u003e GrayConvert[\"cv2.cvtColor()\u003cbr/\u003eBGR to Grayscale\"]\n    GrayConvert --\u003e FaceDetector[\"dlib.get_frontal_face_detector()\"]\n    FaceDetector --\u003e FaceObjects[\"Face Objects Array\"]\n    \n    FaceObjects --\u003e FaceLoop[\"For each detected face\"]\n    FaceLoop --\u003e ExtractCoords[\"Extract (x,y,w,h) coordinates\"]\n    FaceLoop --\u003e DrawCorners[\"Draw corner markers\u003cbr/\u003ecv2.line()\"]\n    FaceLoop --\u003e ShapePredictor[\"shapePredictor(gray, face)\"]\n    \n    ShapePredictor --\u003e Landmarks[\"68 Facial Landmarks\"]\n    Landmarks --\u003e DrawLandmarks[\"cv2.circle() for each point\"]\n    \n    DrawCorners --\u003e Output[\"Annotated Frame + Face Count\"]\n    DrawLandmarks --\u003e Output\n```\n\nThe function returns a tuple containing the face count and face objects array, which are used by all subsequent analysis modules.\n\nSources: [facial_detections.py:11-65]()\n\n## Eye Analysis Components\n\n### Eye Gaze Detection\n\nThe `gazeDetection()` function in `eye_tracker.py` determines the direction of the student's gaze by analyzing eye regions and pupil positioning.\n\n**Eye Gaze Detection Pipeline**\n```mermaid\ngraph TD\n    FaceObjects[\"Face Objects from detectFace()\"] --\u003e ExtractRegions[\"Extract Eye Regions\u003cbr/\u003eLeft: landmarks 36-41\u003cbr/\u003eRight: landmarks 42-47\"]\n    \n    ExtractRegions --\u003e CreateMask[\"createMask()\u003cbr/\u003eBlack mask with frame dimensions\"]\n    CreateMask --\u003e ExtractEye[\"extractEye()\u003cbr/\u003ePolylines + fillPoly on mask\"]\n    ExtractEye --\u003e EyeRegions[\"Masked Eye Regions\"]\n    \n    EyeRegions --\u003e CropEyes[\"Crop individual eye rectangles\"]\n    CropEyes --\u003e GrayConvert[\"Convert to grayscale\"]\n    GrayConvert --\u003e AdaptiveThresh[\"cv2.adaptiveThreshold()\u003cbr/\u003eADAPTIVE_THRESH_MEAN_C\"]\n    \n    AdaptiveThresh --\u003e Segment[\"eyeSegmentationAndReturnWhite()\u003cbr/\u003eLeft/Right pixel counting\"]\n    Segment --\u003e TrialRatio{\"Ratio \u003e= 1.2\"}\n    \n    TrialRatio --\u003e|\"Right \u003e Left\"| GazeLeft[\"Gaze: left\"]\n    TrialRatio --\u003e|\"Left \u003e Right\"| GazeRight[\"Gaze: right\"]  \n    TrialRatio --\u003e|\"Balanced\"| GazeCenter[\"Gaze: center\"]\n```\n\nKey parameters:\n- **Eye Landmark Indices**: Left eye (36-41), Right eye (42-47)\n- **Threshold Method**: Adaptive threshold with mean C\n- **Gaze Ratio**: 1.2 threshold for directional detection\n\nSources: [eye_tracker.py:41-103](), [eye_tracker.py:10-37]()\n\n### Blink Detection\n\nThe `isBlinking()` function calculates the Eye Aspect Ratio (EAR) to determine if the student is blinking, which helps detect attention and potential communication attempts.\n\n**Blink Detection Algorithm**\n```mermaid\ngraph TD\n    Landmarks[\"68 Facial Landmarks\"] --\u003e LeftEye[\"Left Eye Points\u003cbr/\u003e36,37,38,39,40,41\"]\n    Landmarks --\u003e RightEye[\"Right Eye Points\u003cbr/\u003e42,43,44,45,46,47\"]\n    \n    LeftEye --\u003e LeftCalc[\"Calculate Left EAR\"]\n    RightEye --\u003e RightCalc[\"Calculate Right EAR\"]\n    \n    LeftCalc --\u003e LeftHor[\"findDist(leftPoint, rightPoint)\"]\n    LeftCalc --\u003e LeftVer[\"findDist(topMidpoint, bottomMidpoint)\"]\n    LeftHor --\u003e LeftRatio[\"leftHorLen / leftVerLen\"]\n    LeftVer --\u003e LeftRatio\n    \n    RightCalc --\u003e RightHor[\"findDist(leftPoint, rightPoint)\"]\n    RightCalc --\u003e RightVer[\"findDist(topMidpoint, bottomMidpoint)\"]\n    RightHor --\u003e RightRatio[\"rightHorLen / rightVerLen\"]\n    RightVer --\u003e RightRatio\n    \n    LeftRatio --\u003e ThresholdCheck{\"Ratio \u003e= 3.6\"}\n    RightRatio --\u003e ThresholdCheck\n    ThresholdCheck --\u003e|\"Yes\"| BlinkDetected[\"Blink Detected\"]\n    ThresholdCheck --\u003e|\"No\"| NoBlinkDetected[\"No Blink\"]\n```\n\nThe algorithm uses:\n- **Midpoint Calculation**: `midPoint(pointA, pointB)` for top/bottom eye positions\n- **Distance Calculation**: `findDist(pointA, pointB)` using Euclidean distance\n- **Blink Threshold**: 3.6 ratio threshold (optimal value determined empirically)\n\nSources: [blink_detection.py:26-73](), [blink_detection.py:10-23]()\n\n## Head Pose Estimation\n\nThe `head_pose_detection()` function uses 3D pose estimation to determine head orientation, detecting when students look away from the screen or attempt to communicate.\n\n**Head Pose Estimation Components**\n```mermaid\ngraph TD\n    ModelPoints[\"3D Model Points\u003cbr/\u003eNose, Chin, Eye corners\u003cbr/\u003eMouth corners\"] --\u003e ImagePoints[\"2D Image Points\u003cbr/\u003eLandmarks 30,8,36,45,48,54\"]\n    \n    ImagePoints --\u003e SolvePnP[\"cv2.solvePnP()\u003cbr/\u003eSOLVEPNP_UPNP\"]\n    ModelPoints --\u003e SolvePnP\n    CameraMatrix[\"Camera Matrix\u003cbr/\u003efocal_length, center\"] --\u003e SolvePnP\n    \n    SolvePnP --\u003e RotationVector[\"Rotation Vector\"]\n    SolvePnP --\u003e TranslationVector[\"Translation Vector\"]\n    \n    RotationVector --\u003e ProjectPoints[\"cv2.projectPoints()\u003cbr/\u003eProject 3D to 2D\"]\n    TranslationVector --\u003e ProjectPoints\n    \n    ProjectPoints --\u003e CalculateAngles[\"Calculate Angles\u003cbr/\u003emath.atan(slope)\"]\n    CalculateAngles --\u003e AngleThreshold{\"Angle \u003e= 45° or \u003c= -45°\"}\n    \n    AngleThreshold --\u003e|\"ang1 \u003e= 45\"| HeadUp[\"Head Up\"]\n    AngleThreshold --\u003e|\"ang1 \u003c= -45\"| HeadDown[\"Head Down\"]\n    AngleThreshold --\u003e|\"ang2 \u003e= 45\"| HeadRight[\"Head Right\"]\n    AngleThreshold --\u003e|\"ang2 \u003c= -45\"| HeadLeft[\"Head Left\"]\n```\n\nKey technical details:\n- **3D Model Points**: 6 key facial features in 3D space coordinates\n- **Camera Matrix**: Calculated using focal length and image center\n- **Angle Thresholds**: ±45 degrees for detecting significant head movements\n- **PnP Solver**: Uses UPNP (Unified Perspective-n-Point) algorithm\n\nSources: [head_pose_estimation.py:105-178](), [head_pose_estimation.py:72-99]()\n\n## Mouth Tracking\n\nThe `mouthTrack()` function monitors mouth movement to detect speaking, which may indicate communication attempts during the exam.\n\n**Mouth Tracking Implementation**\n```mermaid\ngraph TD\n    FacialLandmarks[\"Facial Landmarks\"] --\u003e OuterTop[\"Outer Lip Top\u003cbr/\u003eLandmark 51\"]\n    FacialLandmarks --\u003e OuterBottom[\"Outer Lip Bottom\u003cbr/\u003eLandmark 57\"]\n    \n    OuterTop --\u003e TopCoords[\"(outerTopX, outerTopY)\"]\n    OuterBottom --\u003e BottomCoords[\"(outerBottomX, outerBottomY)\"]\n    \n    TopCoords --\u003e CalcDistance[\"calcDistance()\u003cbr/\u003eEuclidean distance\"]\n    BottomCoords --\u003e CalcDistance\n    \n    CalcDistance --\u003e Threshold{\"Distance \u003e 23 pixels\"}\n    Threshold --\u003e|\"Yes\"| MouthOpen[\"Mouth Open\"]\n    Threshold --\u003e|\"No\"| MouthClose[\"Mouth Close\"]\n```\n\nThe implementation uses:\n- **Key Landmarks**: Point 51 (outer lip top) and Point 57 (outer lip bottom)\n- **Distance Threshold**: 23 pixels to distinguish open/closed states\n- **Distance Calculation**: Euclidean distance using `hypot()` function\n\nSources: [mouth_tracking.py:15-36](), [mouth_tracking.py:8-12]()\n\n## Common Dependencies and Integration\n\nAll facial analysis modules share common dependencies and integration patterns:\n\n| Dependency | Usage | Configuration |\n|------------|-------|---------------|\n| `dlib` | Face detection and landmark extraction | HOG face detector + shape predictor |\n| `cv2` (OpenCV) | Image processing and drawing | Video capture, color conversion, drawing |\n| `numpy` | Numerical computations | Array operations, mathematical calculations |\n| Shape Predictor Model | `shape_predictor_68_face_landmarks.dat` | 68-point facial landmark model |\n\n**Module Integration Pattern**\n```mermaid\nsequenceDiagram\n    participant Main as \"Main Application\"\n    participant FD as \"facial_detections.py\"\n    participant ET as \"eye_tracker.py\"\n    participant BD as \"blink_detection.py\"\n    participant HP as \"head_pose_estimation.py\"\n    participant MT as \"mouth_tracking.py\"\n    \n    Main-\u003e\u003eFD: detectFace(frame)\n    FD-\u003e\u003eMain: (faceCount, faces)\n    \n    alt faceCount == 1\n        Main-\u003e\u003eET: gazeDetection(faces, frame)\n        Main-\u003e\u003eBD: isBlinking(faces, frame)\n        Main-\u003e\u003eHP: head_pose_detection(faces, frame)\n        Main-\u003e\u003eMT: mouthTrack(faces, frame)\n        \n        ET-\u003e\u003eMain: gaze_result\n        BD-\u003e\u003eMain: blink_result\n        HP-\u003e\u003eMain: pose_result\n        MT-\u003e\u003eMain: mouth_result\n    else faceCount != 1\n        Main-\u003e\u003eMain: Log suspicious activity\n    end\n```\n\nEach module operates independently but relies on the face detection results as input. The system processes all modules in parallel for each video frame, aggregating results for activity logging and violation detection.\n\nSources: [facial_detections.py:6-8](), [eye_tracker.py:6-7](), [blink_detection.py:6-7](), [head_pose_estimation.py:97-98](), [mouth_tracking.py:5-6]()"])</script><script>self.__next_f.push([1,"1b:T1ee4,"])</script><script>self.__next_f.push([1,"# Object and Audio Detection\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [audio_detection.py](audio_detection.py)\n- [object_detection.py](object_detection.py)\n\n\u003c/details\u003e\n\n\n\n## Purpose and Scope\n\nThis document covers the object and audio detection modules that form part of the AI Detection Engine. These modules monitor the exam environment for unauthorized objects and suspicious audio activity during proctored exams. The object detection system uses YOLO neural networks to identify physical objects in the video feed, while the audio detection system monitors ambient sound levels to detect potential violations.\n\nFor information about facial analysis capabilities, see [Facial Analysis Modules](#4.1). For the overall AI detection architecture, see [AI Detection Engine](#4).\n\n## Object Detection System\n\n### Architecture Overview\n\nThe object detection system implements real-time object recognition using the YOLOv3-tiny model. The system processes video frames to identify and classify objects, returning detection results with confidence scores.\n\n**Object Detection Pipeline**\n```mermaid\nflowchart TD\n    VideoFrame[\"Video Frame Input\"] --\u003e detectObject[\"detectObject()\"]\n    detectObject --\u003e BlobPrep[\"cv2.dnn.blobFromImage()\"]\n    BlobPrep --\u003e YOLONet[\"YOLO Network Inference\"]\n    YOLONet --\u003e PostProcess[\"Non-Maximum Suppression\"]\n    PostProcess --\u003e Results[\"Detection Results List\"]\n    \n    subgraph \"Model Components\"\n        Weights[\"yolov3-tiny.weights\"]\n        Config[\"yolov3-tiny.cfg\"] \n        Labels[\"coco.names\"]\n    end\n    \n    subgraph \"Detection Pipeline\"\n        ClassIDs[\"class_ids[]\"]\n        Confidences[\"confidences[]\"]\n        Boxes[\"boxes[]\"]\n        NMS[\"cv2.dnn.NMSBoxes()\"]\n    end\n    \n    Weights --\u003e detectObject\n    Config --\u003e detectObject\n    Labels --\u003e detectObject\n    \n    YOLONet --\u003e ClassIDs\n    YOLONet --\u003e Confidences\n    YOLONet --\u003e Boxes\n    ClassIDs --\u003e NMS\n    Confidences --\u003e NMS\n    Boxes --\u003e NMS\n    NMS --\u003e Results\n```\n\n**Sources:** [object_detection.py:1-81]()\n\n### Model Configuration\n\nThe system loads the YOLOv3-tiny model with the following configuration:\n\n| Component | File Path | Purpose |\n|-----------|-----------|---------|\n| Model Weights | `object_detection_model/weights/yolov3-tiny.weights` | Pre-trained neural network parameters |\n| Model Config | `object_detection_model/config/yolov3-tiny.cfg` | Network architecture definition |\n| Object Labels | `object_detection_model/objectLabels/coco.names` | COCO dataset class names |\n\nThe model initialization occurs at module load time using `cv2.dnn.readNet()` and processes the COCO label classes for object classification.\n\n**Sources:** [object_detection.py:6-12]()\n\n### Detection Algorithm\n\nThe core detection function `detectObject(frame)` implements the following workflow:\n\n**Detection Process Flow**\n```mermaid\nsequenceDiagram\n    participant Frame as \"Input Frame\"\n    participant Blob as \"Blob Preprocessing\"\n    participant Net as \"YOLO Network\"\n    participant NMS as \"Non-Max Suppression\"\n    participant Results as \"Detection Results\"\n    \n    Frame-\u003e\u003eBlob: cv2.dnn.blobFromImage(frame, 0.00392, (220,220))\n    Blob-\u003e\u003eNet: net.setInput(blob)\n    Net-\u003e\u003eNet: net.forward(output_layers)\n    Net-\u003e\u003eNMS: Raw detections with confidence \u003e 0.5\n    NMS-\u003e\u003eNMS: cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n    NMS-\u003e\u003eResults: Filtered detection list [(label, confidence)]\n```\n\n**Sources:** [object_detection.py:26-81]()\n\n### Detection Parameters\n\nThe system uses specific thresholds and parameters for object detection:\n\n- **Confidence Threshold**: 0.5 - Minimum confidence score for object detection [object_detection.py:51]()\n- **NMS Threshold**: 0.4 - Non-maximum suppression overlap threshold [object_detection.py:68]()\n- **Input Size**: 220x220 pixels - Network input dimensions [object_detection.py:32]()\n- **Scale Factor**: 0.00392 - Pixel normalization factor [object_detection.py:32]()\n\n**Sources:** [object_detection.py:32-68]()\n\n## Audio Detection System\n\n### Real-time Audio Monitoring\n\nThe audio detection system continuously monitors ambient sound levels using PyAudio for real-time audio capture and analysis.\n\n**Audio Detection Architecture**\n```mermaid\nflowchart TD\n    AudioInput[\"Microphone Input\"] --\u003e PyAudioStream[\"pyaudio.Stream\"]\n    PyAudioStream --\u003e AudioBuffer[\"Audio Buffer (CHUNK=1024)\"]\n    AudioBuffer --\u003e NumPyConvert[\"np.frombuffer()\"]\n    NumPyConvert --\u003e ThresholdCheck[\"Amplitude \u003e THRESHOLD\"]\n    \n    ThresholdCheck --\u003e|\"Above 2000\"| SuspiciousFlag[\"suspicious_audio_detected = True\"]\n    ThresholdCheck --\u003e|\"Below 2000\"| NormalFlag[\"suspicious_audio_detected = False\"]\n    \n    SuspiciousFlag --\u003e BeepAlert[\"winsound.Beep(2500, 1000)\"]\n    SuspiciousFlag --\u003e FrameCapture[\"capture_and_save_frame()\"]\n    \n    subgraph \"Audio Configuration\"\n        Format[\"FORMAT = paInt16\"]\n        Channels[\"CHANNELS = 1\"]\n        Rate[\"RATE = 44100\"]\n        Threshold[\"THRESHOLD = 2000\"]\n    end\n    \n    Format --\u003e PyAudioStream\n    Channels --\u003e PyAudioStream\n    Rate --\u003e PyAudioStream\n```\n\n**Sources:** [audio_detection.py:9-55]()\n\n### Audio Configuration Parameters\n\nThe audio detection system uses the following configuration:\n\n| Parameter | Value | Purpose |\n|-----------|-------|---------|\n| `CHUNK` | 1024 | Audio buffer size in frames |\n| `FORMAT` | `paInt16` | 16-bit integer audio format |\n| `CHANNELS` | 1 | Mono audio capture |\n| `RATE` | 44100 | Sample rate in Hz |\n| `THRESHOLD` | 2000 | Amplitude threshold for suspicious audio |\n\n**Sources:** [audio_detection.py:10-14]()\n\n### Alert System\n\nWhen suspicious audio is detected, the system triggers multiple response mechanisms:\n\n1. **Audio Alert**: Plays a beep sound at 2500Hz for 1000ms using `winsound.Beep()` [audio_detection.py:38]()\n2. **Frame Capture**: Captures a video frame through `capture_and_save_frame()` function [audio_detection.py:44]()\n3. **State Management**: Uses `suspicious_audio_detected` flag to prevent repeated alerts [audio_detection.py:26]()\n\n**Suspicious Audio Detection Logic**\n```mermaid\nstateDiagram-v2\n    [*] --\u003e Listening\n    Listening --\u003e AudioCapture: Read audio chunk\n    AudioCapture --\u003e AmplitudeCheck: Convert to numpy array\n    \n    AmplitudeCheck --\u003e SuspiciousDetected: np.max(np.abs(audio_data)) \u003e THRESHOLD\n    AmplitudeCheck --\u003e NormalAudio: Below threshold\n    \n    SuspiciousDetected --\u003e AlertTriggered: First detection\n    AlertTriggered --\u003e BeepSound: winsound.Beep()\n    AlertTriggered --\u003e FrameCapture: Save suspicious frame\n    AlertTriggered --\u003e FlagSet: suspicious_audio_detected = True\n    \n    SuspiciousDetected --\u003e AlertSupressed: Subsequent detections\n    AlertSupressed --\u003e FlagSet\n    \n    NormalAudio --\u003e FlagReset: suspicious_audio_detected = False\n    FlagReset --\u003e Listening\n    FlagSet --\u003e Listening\n```\n\n**Sources:** [audio_detection.py:34-47]()\n\n### Frame Capture Integration\n\nThe `capture_and_save_frame()` function provides visual evidence when audio violations occur:\n\n- Opens camera using `cv2.VideoCapture(0)` [audio_detection.py:58]()\n- Captures single frame and saves as \"suspicious_frame.jpg\" [audio_detection.py:68]()\n- Handles camera errors gracefully [audio_detection.py:60-62]()\n- Releases camera resources after capture [audio_detection.py:71]()\n\n**Sources:** [audio_detection.py:57-71]()\n\n## Integration with Proctoring System\n\nBoth detection systems operate as independent modules within the larger proctoring orchestrator. The `detectObject()` function returns a list of detected objects with confidence scores, while the `audio_detection()` function runs continuously in the background monitoring for audio violations.\n\nThe detection results feed into the activity logging system for comprehensive exam monitoring and post-exam review capabilities.\n\n**Sources:** [object_detection.py:26-81](), [audio_detection.py:9-55]()"])</script><script>self.__next_f.push([1,"1c:T219e,"])</script><script>self.__next_f.push([1,"# Web Interface\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [static/login_script.js](static/login_script.js)\n- [static/login_style.css](static/login_style.css)\n- [templates/index.html](templates/index.html)\n\n\u003c/details\u003e\n\n\n\nThe Web Interface provides the primary user-facing components of the AI-based Online Exam Proctoring System. It consists of a Flask-based web server that serves HTML templates, handles user authentication, and provides the exam interface with integrated proctoring capabilities.\n\nThis document covers the overall web interface architecture and components. For detailed information about user authentication and session management, see [Authentication System](#5.1). For information about the quiz presentation and interaction components, see [Quiz Interface](#5.2).\n\n## System Overview\n\nThe web interface operates as a multi-layered system with distinct client-side and server-side components that work together to provide a seamless user experience from initial authentication through exam completion.\n\n### Web Interface Architecture\n\n```mermaid\ngraph TB\n    subgraph \"Client Layer\"\n        IndexHTML[\"index.html\u003cbr/\u003eLanding Page\"]\n        LoginScript[\"login_script.js\u003cbr/\u003eClient Logic\"]\n        LoginStyle[\"login_style.css\u003cbr/\u003eStyling\"]\n        QuizHTML[\"quiz.html\u003cbr/\u003eExam Interface\"]\n    end\n    \n    subgraph \"Server Layer\"\n        FlaskServer[\"server.py\u003cbr/\u003eFlask Application\"]\n        LoginDataRoute[\"/login_data\u003cbr/\u003ePOST Endpoint\"]\n        SignupDataRoute[\"/signup_data\u003cbr/\u003ePOST Endpoint\"]\n        QuizHTMLRoute[\"/quiz_html\u003cbr/\u003eGET Endpoint\"]\n        VideoFeedRoute[\"/video_feed\u003cbr/\u003eStreaming Endpoint\"]\n    end\n    \n    subgraph \"Authentication Flow\"\n        FormValidation[\"Form Validation\u003cbr/\u003eJavaScript\"]\n        AjaxRequest[\"fetch() API\u003cbr/\u003eHTTP Requests\"]\n        ServerValidation[\"Credential Validation\u003cbr/\u003eDatabase Lookup\"]\n    end\n    \n    subgraph \"Proctoring Integration\"\n        VideoStream[\"Video Streaming\u003cbr/\u003eReal-time Feed\"]\n        ProctoringEngine[\"Proctoring Algorithm\u003cbr/\u003eAI Detection\"]\n        ActivityLogging[\"Activity Logging\u003cbr/\u003eUser Monitoring\"]\n    end\n    \n    IndexHTML --\u003e LoginScript\n    IndexHTML --\u003e LoginStyle\n    LoginScript --\u003e FormValidation\n    FormValidation --\u003e AjaxRequest\n    AjaxRequest --\u003e LoginDataRoute\n    AjaxRequest --\u003e SignupDataRoute\n    \n    LoginDataRoute --\u003e ServerValidation\n    SignupDataRoute --\u003e ServerValidation\n    ServerValidation --\u003e QuizHTMLRoute\n    QuizHTMLRoute --\u003e QuizHTML\n    \n    QuizHTML --\u003e VideoFeedRoute\n    VideoFeedRoute --\u003e VideoStream\n    VideoStream --\u003e ProctoringEngine\n    ProctoringEngine --\u003e ActivityLogging\n    \n    FlaskServer --\u003e LoginDataRoute\n    FlaskServer --\u003e SignupDataRoute\n    FlaskServer --\u003e QuizHTMLRoute\n    FlaskServer --\u003e VideoFeedRoute\n```\n\nSources: [templates/index.html:1-143](), [static/login_script.js:1-103](), [static/login_style.css:1-429]()\n\n## Client-Side Components\n\nThe client-side interface consists of HTML templates, JavaScript logic, and CSS styling that handle user interactions and form processing.\n\n### Primary Interface Elements\n\n| Component | File | Purpose |\n|-----------|------|---------|\n| Landing Page | `templates/index.html` | Main entry point with loading animation and authentication forms |\n| Client Logic | `static/login_script.js` | Form validation, AJAX requests, and user interaction handling |\n| Styling | `static/login_style.css` | Responsive design, animations, and visual presentation |\n\n### Authentication Request Flow\n\n```mermaid\nsequenceDiagram\n    participant User as \"User Browser\"\n    participant IndexHTML as \"index.html\"\n    participant LoginScript as \"login_script.js\"\n    participant FlaskServer as \"Flask Server\"\n    participant Database as \"Database\"\n    \n    User-\u003e\u003eIndexHTML: \"Load Page\"\n    IndexHTML-\u003e\u003eLoginScript: \"Initialize Event Listeners\"\n    \n    User-\u003e\u003eLoginScript: \"Submit Login Form\"\n    LoginScript-\u003e\u003eLoginScript: \"validateCredentials()\"\n    \n    alt \"Validation Successful\"\n        LoginScript-\u003e\u003eFlaskServer: \"POST /login_data\"\n        FlaskServer-\u003e\u003eDatabase: \"Query user credentials\"\n        Database-\u003e\u003eFlaskServer: \"Return match result\"\n        \n        alt \"Credentials Valid\"\n            FlaskServer-\u003e\u003eLoginScript: \"Response: true\"\n            LoginScript-\u003e\u003eUser: \"window.location.replace('./quiz_html')\"\n        else \"Invalid Credentials\"\n            FlaskServer-\u003e\u003eLoginScript: \"Response: false\"\n            LoginScript-\u003e\u003eUser: \"Alert: Login credentials don't match\"\n        end\n    else \"Validation Failed\"\n        LoginScript-\u003e\u003eUser: \"Alert: Fill all required fields\"\n    end\n```\n\nSources: [static/login_script.js:62-98](), [static/login_script.js:23-60]()\n\n## Server-Side Integration\n\nThe Flask server provides RESTful endpoints that handle authentication requests and serve the exam interface with integrated proctoring capabilities.\n\n### Key Server Endpoints\n\n| Endpoint | Method | Purpose | Handler |\n|----------|--------|---------|---------|\n| `/login_data` | POST | User authentication | Login credential validation |\n| `/signup_data` | POST | User registration | New account creation |\n| `/quiz_html` | GET | Exam interface | Quiz page serving |\n| `/video_feed` | GET | Video streaming | Real-time proctoring feed |\n\n### Form Processing Logic\n\nThe client-side JavaScript implements comprehensive form validation and server communication:\n\n- **Login Processing**: The `submitButton` event listener in [static/login_script.js:62-98]() validates email and password fields, then sends a POST request to `/login_data`\n- **Registration Processing**: The `createacctbtn` event listener in [static/login_script.js:23-60]() validates signup fields including password confirmation\n- **Response Handling**: Successful authentication redirects to `./quiz_html`, while failures display appropriate error messages\n\n### User Interface States\n\n```mermaid\nstateDiagram-v2\n    [*] --\u003e LoadingScreen: \"Page Load\"\n    LoadingScreen --\u003e AuthenticationForm: \"5 second timeout\"\n    \n    state AuthenticationForm {\n        [*] --\u003e SignIn: \"Default State\"\n        SignIn --\u003e SignUp: \"toggle() function\"\n        SignUp --\u003e SignIn: \"toggle() function\"\n        \n        state SignIn {\n            [*] --\u003e EmailInput\n            EmailInput --\u003e PasswordInput\n            PasswordInput --\u003e SubmitButton\n            SubmitButton --\u003e ValidationCheck\n            ValidationCheck --\u003e ServerRequest: \"Valid Input\"\n            ValidationCheck --\u003e ErrorAlert: \"Invalid Input\"\n            ServerRequest --\u003e QuizRedirect: \"Success\"\n            ServerRequest --\u003e ErrorAlert: \"Failure\"\n        }\n        \n        state SignUp {\n            [*] --\u003e SignupEmailInput\n            SignupEmailInput --\u003e UsernameInput\n            UsernameInput --\u003e SignupPasswordInput\n            SignupPasswordInput --\u003e ConfirmPasswordInput\n            ConfirmPasswordInput --\u003e CreateAccountButton\n            CreateAccountButton --\u003e SignupValidation\n            SignupValidation --\u003e SignupRequest: \"Valid Input\"\n            SignupValidation --\u003e ErrorAlert: \"Invalid Input\"\n        }\n    }\n    \n    QuizRedirect --\u003e ExamInterface\n    ExamInterface --\u003e ProctoringActive\n    ErrorAlert --\u003e AuthenticationForm\n```\n\nSources: [templates/index.html:122-138](), [static/login_script.js:1-103]()\n\n## Visual Design and User Experience\n\nThe interface implements a modern, responsive design with smooth animations and transitions to provide an engaging user experience.\n\n### Design Features\n\n- **Loading Animation**: Initial 5-second loading screen with rotating dots animation [templates/index.html:12-22]()\n- **Dual-Panel Layout**: Animated transition between sign-in and sign-up forms [static/login_style.css:189-282]()\n- **Responsive Design**: Mobile-optimized layout with media queries [static/login_style.css:286-351]()\n- **Form Validation**: Real-time client-side validation with user feedback [static/login_script.js:30-39](), [static/login_script.js:67-70]()\n\n### CSS Architecture\n\nThe styling system uses CSS custom properties for consistent theming and implements complex animations for form transitions:\n\n- **Color Scheme**: Primary color `rgb(101, 105, 38)` and secondary color `#990073` [static/login_style.css:1-8]()\n- **Animation System**: Scale and translate transforms for smooth transitions [static/login_style.css:59-61](), [static/login_style.css:124-127]()\n- **Responsive Breakpoints**: Mobile-first design with 425px breakpoint [static/login_style.css:286]()\n\nSources: [templates/index.html:1-143](), [static/login_script.js:1-103](), [static/login_style.css:1-429]()"])</script><script>self.__next_f.push([1,"1d:T285c,"])</script><script>self.__next_f.push([1,"# Authentication System\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [server.py](server.py)\n- [static/login_script.js](static/login_script.js)\n- [static/login_style.css](static/login_style.css)\n- [templates/index.html](templates/index.html)\n\n\u003c/details\u003e\n\n\n\n## Purpose and Scope\n\nThe Authentication System handles user registration, login, and session management for the AI-based Online Exam Proctoring System. This system provides secure access control before users can enter the proctored exam environment. It includes both frontend user interfaces and backend API endpoints for credential validation and user account management.\n\nFor information about the quiz interface that users access after authentication, see [Quiz Interface](#5.2). For database schema and data persistence details, see [Database and Data Management](#6).\n\n## System Overview\n\nThe authentication system consists of a web-based frontend with dual login/signup forms, client-side JavaScript for form validation and API communication, and Flask backend routes for processing authentication requests. The system integrates with a MySQL database to store and validate user credentials.\n\n```mermaid\ngraph TB\n    subgraph \"Frontend Components\"\n        IndexHTML[\"index.html\u003cbr/\u003eLogin/Signup Forms\"]\n        LoginScript[\"login_script.js\u003cbr/\u003eForm Validation \u0026 API Calls\"]\n        LoginCSS[\"login_style.css\u003cbr/\u003eUI Styling\"]\n    end\n    \n    subgraph \"Backend Routes\"\n        SignupRoute[\"/signup_data\u003cbr/\u003ePOST endpoint\"]\n        LoginRoute[\"/login_data\u003cbr/\u003ePOST endpoint\"]\n        IndexRoute[\"/\u003cbr/\u003eServes index.html\"]\n        QuizRoute[\"/quiz_html\u003cbr/\u003eServes quiz.html\"]\n    end\n    \n    subgraph \"Database Layer\"\n        DBHelper[\"db_helper.py\u003cbr/\u003einsert_signup()\u003cbr/\u003esearch_login_credentials()\"]\n        SignupTable[\"sign_up table\u003cbr/\u003eMySQL Database\"]\n    end\n    \n    IndexHTML --\u003e LoginScript\n    LoginScript --\u003e|\"POST /signup_data\"| SignupRoute\n    LoginScript --\u003e|\"POST /login_data\"| LoginRoute\n    SignupRoute --\u003e DBHelper\n    LoginRoute --\u003e DBHelper\n    DBHelper \u003c--\u003e SignupTable\n    LoginRoute --\u003e|\"Success\"| QuizRoute\n    \n    style IndexHTML fill:#e1f5fe\n    style SignupRoute fill:#f3e5f5\n    style LoginRoute fill:#f3e5f5\n    style DBHelper fill:#fff3e0\n```\n\nSources: [server.py:1-70](), [static/login_script.js:1-103](), [templates/index.html:1-143]()\n\n## User Registration Process\n\nThe user registration process allows new users to create accounts by providing email, username, and password credentials. The system includes client-side validation and server-side processing.\n\n### Registration Flow\n\n```mermaid\nsequenceDiagram\n    participant User as \"User Browser\"\n    participant IndexHTML as \"index.html\"\n    participant LoginScript as \"login_script.js\"\n    participant Server as \"Flask Server\"\n    participant DBHelper as \"db_helper.py\"\n    participant Database as \"MySQL Database\"\n    \n    User-\u003e\u003eIndexHTML: \"Fill signup form\"\n    User-\u003e\u003eIndexHTML: \"Click Create Account\"\n    IndexHTML-\u003e\u003eLoginScript: \"createacctbtn click event\"\n    LoginScript-\u003e\u003eLoginScript: \"Validate form fields\"\n    LoginScript-\u003e\u003eLoginScript: \"Check password confirmation\"\n    \n    alt \"Validation successful\"\n        LoginScript-\u003e\u003eServer: \"POST /signup_data\u003cbr/\u003e{signupEmail, username, signupPassword}\"\n        Server-\u003e\u003eDBHelper: \"insert_signup(email, username, password)\"\n        DBHelper-\u003e\u003eDatabase: \"INSERT INTO sign_up table\"\n        Database--\u003e\u003eDBHelper: \"Success/Failure\"\n        DBHelper--\u003e\u003eServer: \"Return status (1 or 0)\"\n        Server--\u003e\u003eLoginScript: \"JSON response\"\n        LoginScript-\u003e\u003eUser: \"Display success/error alert\"\n    else \"Validation failed\"\n        LoginScript-\u003e\u003eUser: \"Display validation error\"\n    end\n```\n\nSources: [static/login_script.js:23-60](), [server.py:15-24]()\n\n### Registration Form Fields\n\nThe signup form in [templates/index.html:28-58]() contains the following input fields:\n\n| Field ID | Input Type | Placeholder | Validation |\n|----------|------------|-------------|------------|\n| `email-signup` | email | \"Email *\" | Required, email format |\n| `username-signup` | name | \"Username *\" | Required |\n| `password-signup` | password | \"Password *\" | Required |\n| `confirm-password-signup` | password | \"Confirm Password *\" | Must match password |\n\n### Client-Side Validation\n\nThe registration validation logic in [static/login_script.js:24-39]() performs:\n\n- Password confirmation matching check\n- Required field validation\n- Form data preparation for API submission\n\n## User Login Process\n\nThe login process authenticates existing users by validating their email and password credentials against the database.\n\n### Login Flow\n\n```mermaid\nsequenceDiagram\n    participant User as \"User Browser\"\n    participant IndexHTML as \"index.html\"\n    participant LoginScript as \"login_script.js\"\n    participant Server as \"Flask Server\"\n    participant DBHelper as \"db_helper.py\"\n    participant Database as \"MySQL Database\"\n    \n    User-\u003e\u003eIndexHTML: \"Fill login form\"\n    User-\u003e\u003eIndexHTML: \"Click Submit\"\n    IndexHTML-\u003e\u003eLoginScript: \"submit button click event\"\n    LoginScript-\u003e\u003eLoginScript: \"Validate required fields\"\n    \n    alt \"Fields validated\"\n        LoginScript-\u003e\u003eServer: \"POST /login_data\u003cbr/\u003e{email, password}\"\n        Server-\u003e\u003eDBHelper: \"search_login_credentials(email, password)\"\n        DBHelper-\u003e\u003eDatabase: \"SELECT from sign_up table\"\n        Database--\u003e\u003eDBHelper: \"User record or null\"\n        DBHelper--\u003e\u003eServer: \"Boolean result\"\n        \n        alt \"Credentials valid\"\n            Server--\u003e\u003eLoginScript: \"true\"\n            LoginScript-\u003e\u003eUser: \"Redirect to ./quiz_html\"\n        else \"Credentials invalid\"\n            Server--\u003e\u003eLoginScript: \"false\"\n            LoginScript-\u003e\u003eUser: \"Display error alert\"\n        end\n    else \"Required fields empty\"\n        LoginScript-\u003e\u003eUser: \"Display validation error\"\n    end\n```\n\nSources: [static/login_script.js:62-98](), [server.py:28-35]()\n\n### Login Form Components\n\nThe login form in [templates/index.html:61-90]() includes:\n\n- Email input field (`email`)\n- Password input field (`password`)\n- Submit button (`submit`)\n- \"Forgot password?\" link (currently non-functional)\n- Toggle link to switch to signup form\n\n## Frontend Interface Components\n\nThe authentication interface provides a modern, responsive design with animated transitions between login and signup modes.\n\n### HTML Structure\n\nThe main interface in [templates/index.html]() consists of:\n\n- Loading screen with logo and animation ([templates/index.html:12-22]())\n- Dual-panel container with signup and login forms ([templates/index.html:24-120]())\n- JavaScript toggle functionality for switching between modes ([templates/index.html:130-137]())\n\n### Form Toggle Mechanism\n\nThe interface uses CSS classes and JavaScript to switch between signup and login modes:\n\n```javascript\ntoggle = () =\u003e {\n    container.classList.toggle('sign-in')\n    container.classList.toggle('sign-up')\n}\n```\n\nSources: [templates/index.html:131-134]()\n\n### Styling and Animations\n\nThe CSS in [static/login_style.css]() provides:\n\n- Responsive design with breakpoints for mobile devices\n- Animated form transitions and background effects\n- Color scheme using CSS custom properties\n- Loading animation with animated dots\n\n## Backend API Endpoints\n\nThe Flask server in [server.py]() provides RESTful endpoints for authentication operations.\n\n### Signup Endpoint\n\n**Route:** `POST /signup_data`\n\n**Function:** `signup_data()` ([server.py:15-24]())\n\n**Request Format:**\n```json\n{\n    \"signupEmail\": \"user@example.com\",\n    \"username\": \"username\",\n    \"signupPassword\": \"password\"\n}\n```\n\n**Response Format:**\n```json\n{\n    \"message\": \"Data inserted successfully!\"\n}\n```\n\n### Login Endpoint\n\n**Route:** `POST /login_data`\n\n**Function:** `login_data()` ([server.py:28-35]())\n\n**Request Format:**\n```json\n{\n    \"email\": \"user@example.com\",\n    \"password\": \"password\"\n}\n```\n\n**Response:** Boolean value indicating authentication success\n\n### Static Routes\n\nAdditional routes serve the web interface:\n\n| Route | Function | Purpose |\n|-------|----------|---------|\n| `/` | `index_page()` | Serves login/signup interface |\n| `/quiz_html` | `quix_page()` | Serves quiz interface after login |\n\nSources: [server.py:39-47]()\n\n## Database Integration\n\nThe authentication system integrates with a MySQL database through helper functions in the `db_helper.py` module.\n\n### Database Operations\n\n```mermaid\ngraph LR\n    subgraph \"Authentication Functions\"\n        InsertSignup[\"insert_signup()\u003cbr/\u003eCreates new user\"]\n        SearchLogin[\"search_login_credentials()\u003cbr/\u003eValidates credentials\"]\n    end\n    \n    subgraph \"Database Table\"\n        SignupTable[\"sign_up table\u003cbr/\u003eStores user credentials\"]\n    end\n    \n    InsertSignup --\u003e|\"INSERT\"| SignupTable\n    SearchLogin --\u003e|\"SELECT\"| SignupTable\n    \n    SignupTable --\u003e|\"Return 1/0\"| InsertSignup\n    SignupTable --\u003e|\"Return Boolean\"| SearchLogin\n```\n\nSources: [server.py:20](), [server.py:32]()\n\n### Database Helper Functions\n\nThe system relies on two key database functions:\n\n1. **`insert_signup(email, username, password)`** - Returns 1 for success, 0 for failure\n2. **`search_login_credentials(email, password)`** - Returns boolean indicating if credentials exist\n\n## Session Management and Redirects\n\nThe authentication system manages user sessions through HTTP redirects and client-side navigation.\n\n### Post-Authentication Flow\n\nUpon successful login, the system:\n\n1. Validates credentials via `/login_data` endpoint\n2. Receives boolean response from server\n3. Redirects to `/quiz_html` route on success ([static/login_script.js:90]())\n4. Serves quiz interface from `quiz.html` template\n\n### Error Handling\n\nThe system handles authentication errors through:\n\n- Client-side form validation alerts\n- Server-side error responses for invalid credentials\n- User-friendly error messages displayed via `window.alert()`\n\nSources: [static/login_script.js:88-94](), [server.py:33-35]()\n\n### Security Considerations\n\nThe current authentication implementation:\n\n- Stores passwords in plain text (security concern)\n- Uses HTTP for credential transmission\n- Lacks session tokens or JWT implementation\n- Does not implement password strength requirements\n- Missing CSRF protection\n\nSources: [server.py:15-35](), [static/login_script.js:41-98]()"])</script><script>self.__next_f.push([1,"1e:T2325,"])</script><script>self.__next_f.push([1,"# Quiz Interface\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [static/questions.js](static/questions.js)\n- [static/script.js](static/script.js)\n- [static/style.css](static/style.css)\n\n\u003c/details\u003e\n\n\n\n## Purpose and Scope\n\nThe Quiz Interface provides the frontend presentation layer for exam questions and manages user interactions during the assessment process. This system handles question rendering, answer selection, timer management, and score calculation while integrating with the AI-based proctoring system for continuous monitoring.\n\nFor information about user authentication and login functionality, see [Authentication System](#5.1). For details about the AI detection modules that monitor during quiz sessions, see [AI Detection Engine](#4).\n\n## Quiz Data Structure\n\nThe quiz system is built around a structured question format defined in [static/questions.js:1-51](). Each question follows a standardized schema:\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `numb` | Number | Question sequence number |\n| `question` | String | Question text content |\n| `answer` | String | Correct answer text |\n| `options` | Array | Four answer options |\n\nThe system currently contains 6 predefined questions covering Python programming, databases, algorithms, and user feedback. Questions are stored in the global `questions` array and accessed by index during quiz progression.\n\n**Sources:** [static/questions.js:1-51]()\n\n## Quiz Flow and State Management\n\nThe quiz interface operates through a series of distinct states managed by CSS classes and JavaScript event handlers. The system transitions through four main phases:\n\n```mermaid\nstateDiagram-v2\n    [*] --\u003e StartScreen\n    StartScreen --\u003e InfoBox : \"start_btn.onclick\"\n    InfoBox --\u003e QuizActive : \"continue_btn.onclick\"  \n    InfoBox --\u003e StartScreen : \"exit_btn.onclick\"\n    QuizActive --\u003e NextQuestion : \"next_btn.onclick\"\n    NextQuestion --\u003e QuizActive : \"que_count \u003c questions.length-1\"\n    NextQuestion --\u003e ResultsScreen : \"que_count \u003e= questions.length-1\"\n    ResultsScreen --\u003e [*] : \"setTimeout redirect\"\n    \n    state QuizActive {\n        [*] --\u003e DisplayQuestion\n        DisplayQuestion --\u003e TimerRunning : \"startTimer(15)\"\n        TimerRunning --\u003e AnswerSelected : \"optionSelected()\"\n        TimerRunning --\u003e TimeExpired : \"time \u003c 0\"\n        AnswerSelected --\u003e ReadyNext\n        TimeExpired --\u003e ReadyNext\n        ReadyNext --\u003e [*] : \"next_btn.show\"\n    }\n```\n\n### State Transitions\n\nThe quiz states are controlled by CSS classes that toggle visibility and interaction:\n\n- **Start State**: `.start_btn` button visible\n- **Info State**: `.info_box.activeInfo` displays rules and instructions  \n- **Quiz State**: `.quiz_box.activeQuiz` shows active question interface\n- **Result State**: `.result_box.activeResult` presents final score\n\n**Sources:** [static/script.js:16-36](), [static/style.css:43-50]()\n\n## Question Presentation System\n\nThe `showQuetions(index)` function handles dynamic question rendering by manipulating DOM elements:\n\n```mermaid\nflowchart TD\n    showQuetions[\"showQuetions(index)\"] --\u003e GetElements[\"Get DOM elements:\\n.que_text, .option_list\"]\n    GetElements --\u003e BuildQuestion[\"Build que_tag HTML:\\nquestions[index].numb + question\"]\n    BuildQuestion --\u003e BuildOptions[\"Build option_tag HTML:\\nLoop through options[0-3]\"]\n    BuildOptions --\u003e InjectHTML[\"Inject HTML:\\nque_text.innerHTML = que_tag\\noption_list.innerHTML = option_tag\"]\n    InjectHTML --\u003e AttachEvents[\"Attach click events:\\noption.setAttribute('onclick', 'optionSelected(this)')\"]\n    \n    optionSelected[\"optionSelected(answer)\"] --\u003e StopTimers[\"clearInterval(counter)\\nclearInterval(counterLine)\"]\n    StopTimers --\u003e CheckAnswer[\"Compare userAns == correcAns\\nfrom questions[que_count].answer\"]\n    CheckAnswer --\u003e UpdateScore[\"userScore += 1 (if correct)\"]\n    UpdateScore --\u003e ApplyStyles[\"Add .correct/.incorrect classes\\nAdd tick/cross icons\"]\n    ApplyStyles --\u003e DisableOptions[\"Add .disabled class\\nto all options\"]\n    DisableOptions --\u003e ShowNext[\"next_btn.classList.add('show')\"]\n```\n\nThe system generates HTML dynamically using template strings and injects them into the DOM. Answer validation compares user selection against the `questions[que_count].answer` field.\n\n**Sources:** [static/script.js:38-71](), [static/script.js:84-116]()\n\n## Timer and Scoring System\n\nEach question operates with a 15-second countdown timer implemented through `setInterval()` functions:\n\n### Timer Components\n\n| Component | Function | Description |\n|-----------|----------|-------------|\n| `startTimer(time)` | Main countdown | Decrements time display, handles timeout |\n| `startTimerLine(time)` | Visual progress | Updates progress bar width |\n| `timeCount` | Display element | Shows remaining seconds |\n| `time_line` | Progress bar | Visual timer representation |\n\n### Scoring Logic\n\nThe scoring system tracks correct answers in the global `userScore` variable:\n\n- **Correct Answer**: Increment `userScore`, apply green styling, show tick icon\n- **Incorrect Answer**: Apply red styling, show cross icon, highlight correct answer\n- **Timeout**: Auto-select correct answer, disable all options\n\nResults are categorized based on performance:\n- Score \u003e 3: \"Congrats!\" message\n- Score ≤ 3: \"Nice\" message\n\n**Sources:** [static/script.js:154-185](), [static/script.js:187-196](), [static/script.js:198-223]()\n\n## User Interaction Handling\n\nThe interface manages user interactions through event handlers and DOM manipulation:\n\n```mermaid\nflowchart LR\n    UserActions[\"User Actions\"] --\u003e StartQuiz[\"start_btn.onclick:\\nShow info_box\"]\n    UserActions --\u003e ExitQuiz[\"exit_btn.onclick:\\nRedirect to quiz.html\"]\n    UserActions --\u003e ContinueQuiz[\"continue_btn.onclick:\\nStart quiz session\"]\n    UserActions --\u003e SelectAnswer[\"option.onclick:\\noptionSelected(this)\"]\n    UserActions --\u003e NextQuestion[\"next_btn.onclick:\\nAdvance to next\"]\n    \n    StartQuiz --\u003e InfoDisplay[\"info_box.classList.add('activeInfo')\"]\n    ContinueQuiz --\u003e QuizDisplay[\"quiz_box.classList.add('activeQuiz')\"]\n    ContinueQuiz --\u003e InitializeQuiz[\"showQuetions(0)\\nqueCounter(1)\\nstartTimer(15)\"]\n    \n    SelectAnswer --\u003e ProcessAnswer[\"Process answer validation\\nUpdate score and styling\"]\n    NextQuestion --\u003e CheckProgress[\"que_count \u003c questions.length-1\"]\n    CheckProgress --\u003e NextQ[\"Load next question\"]\n    CheckProgress --\u003e ShowResults[\"showResult()\"]\n```\n\n### Button State Management\n\nThe `next_btn` button visibility is controlled by the `.show` class, which is added after answer selection or timeout. This prevents users from advancing without answering or waiting for timeout.\n\n**Sources:** [static/script.js:17-36](), [static/script.js:122-141]()\n\n## Security and Anti-Cheating Features\n\nThe quiz interface implements several security measures to prevent cheating:\n\n### Interaction Restrictions\n\n- **Right-click disabled**: `user-select: none` CSS property\n- **Text selection disabled**: Multiple vendor prefixes for compatibility\n- **Screenshot prevention**: Event listener for `screenshotTaken`\n- **Screen recording detection**: `beforeunload` event handling\n\n### CSS Security Classes\n\n```css\nbody {\n  -webkit-touch-callout: none;\n  -webkit-user-select: none;\n  -khtml-user-select: none;\n  -moz-user-select: none;\n  -ms-user-select: none;\n  user-select: none;\n}\n\n.no-interaction {\n  pointer-events: none;\n}\n```\n\n### JavaScript Security Events\n\nThe system monitors for unauthorized activities:\n- Screenshot detection through custom event listeners\n- Screen recording prevention via MediaRecorder API monitoring\n- Page unload detection to prevent tab switching\n\n**Sources:** [static/style.css:12-24](), [static/script.js:225-241]()\n\n## Integration with Proctoring System\n\nThe quiz interface integrates with the AI-based proctoring system through the camera element and video feed:\n\n```mermaid\nflowchart TD\n    QuizInterface[\"Quiz Interface\\nstatic/script.js\"] --\u003e CameraElement[\"Camera Element\\n.camera CSS class\"]\n    CameraElement --\u003e VideoElement[\"#videoElement\\n280x200px display\"]\n    VideoElement --\u003e VideoFeed[\"Video Feed\\n/video_feed endpoint\"]\n    VideoFeed --\u003e ProctoringSystem[\"Proctoring System\\nReal-time monitoring\"]\n    \n    QuizInterface --\u003e TimerSystem[\"Timer System\\n15 seconds per question\"]\n    TimerSystem --\u003e ActivityLogging[\"Activity Logging\\nTimestamped events\"]\n    \n    ProctoringSystem --\u003e AlertSystem[\"Alert System\\nAudio beeps for violations\"]\n    AlertSystem --\u003e QuizInterface\n```\n\nThe camera element is positioned at coordinates specified by the `.camera` CSS class and displays the video feed at 280x200 pixels. The proctoring system operates continuously during quiz sessions, logging activities and triggering alerts for suspicious behavior.\n\nAfter quiz completion, the system waits 10 seconds before redirecting to `http://127.0.0.1:5000` (Flask server home page).\n\n**Sources:** [static/style.css:480-494](), [static/script.js:220-222]()"])</script><script>self.__next_f.push([1,"1f:T26e1,"])</script><script>self.__next_f.push([1,"# Database and Data Management\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [backend/database.sql](backend/database.sql)\n- [backend/db_helper.py](backend/db_helper.py)\n- [server.py](server.py)\n\n\u003c/details\u003e\n\n\n\nThis document covers the database schema, data persistence mechanisms, and data management utilities used in the AI-based Online Exam Proctoring System. The system employs a hybrid approach combining MySQL database storage for user authentication with file-based logging for activity tracking.\n\nFor information about the web interface authentication flow, see [Authentication System](#5.1). For details about activity logging during proctoring sessions, see [Activity Logging System](#3.2).\n\n## Database Architecture Overview\n\nThe system uses a minimal MySQL database design focused primarily on user authentication and account management. The database stores user credentials and registration information, while exam session data and proctoring activities are logged to text files rather than being persisted in the database.\n\n```mermaid\ngraph TB\n    subgraph \"Data Storage Layer\"\n        MySQL[(\"MySQL Database\u003cbr/\u003equizo\")]\n        ActivityFiles[\"Activity Log Files\u003cbr/\u003eactivity.txt\"]\n    end\n    \n    subgraph \"Data Access Layer\"\n        DBHelper[\"db_helper.py\u003cbr/\u003eDatabase Operations\"]\n        FileLogger[\"Activity Logger\u003cbr/\u003eFile-based Storage\"]\n    end\n    \n    subgraph \"Application Layer\"\n        FlaskRoutes[\"Flask Routes\u003cbr/\u003eserver.py\"]\n        ProctoringEngine[\"Proctoring Engine\u003cbr/\u003emain.py\"]\n    end\n    \n    FlaskRoutes --\u003e DBHelper\n    DBHelper --\u003e MySQL\n    ProctoringEngine --\u003e FileLogger\n    FileLogger --\u003e ActivityFiles\n    \n    MySQL --\u003e SignUpTable[\"sign_up table\u003cbr/\u003eUser Credentials\"]\n```\n\n**Database Storage Architecture**\n\nSources: [backend/database.sql:1-56](), [backend/db_helper.py:1-73](), [server.py:1-70]()\n\n## Database Schema\n\nThe MySQL database `quizo` contains a single table for user management. The schema is deliberately simple, focusing on essential authentication requirements.\n\n### sign_up Table Structure\n\n| Column | Type | Constraints | Description |\n|--------|------|-------------|-------------|\n| `email` | `varchar(45)` | PRIMARY KEY, NOT NULL | User's email address, serves as unique identifier |\n| `username` | `varchar(10)` | UNIQUE, NOT NULL | Display name with 10 character limit |\n| `password` | `varchar(15)` | NOT NULL | Plain text password with 15 character limit |\n\n```mermaid\nerDiagram\n    sign_up {\n        varchar(45) email PK \"Primary Key\"\n        varchar(10) username UK \"Unique Key\"\n        varchar(15) password \"Plain Text\"\n    }\n```\n\n**User Authentication Schema**\n\nThe table includes sample data for five users, including an administrative account with email `quizo_admin@gmail.com` and username `ExamPortalTeam`. All passwords are stored in plain text, which represents a security limitation in the current implementation.\n\nSources: [backend/database.sql:27-33](), [backend/database.sql:42-42]()\n\n## Database Helper Functions\n\nThe `db_helper.py` module provides a centralized interface for all database operations using the `mysql.connector` library. The module establishes a global connection object that is reused across all database operations.\n\n### Connection Configuration\n\nThe database connection is established using hardcoded credentials:\n\n```python\ncnx = mysql.connector.connect(\n    host=\"localhost\", \n    user=\"root\", \n    password=\"root\", \n    database=\"quizo\"\n)\n```\n\n### Core Database Operations\n\nThe helper module provides three primary functions:\n\n#### get_all_details()\nRetrieves all user records from the `sign_up` table for debugging and administrative purposes. This function prints all rows to console and is primarily used during development.\n\n#### insert_signup(email, username, password)\nHandles new user registration with comprehensive error handling:\n- Returns `1` on successful insertion\n- Returns `-1` on database errors or constraint violations\n- Includes rollback functionality for transaction safety\n- Prints success/error messages to console\n\n#### search_login_credentials(email, password)\nValidates user login attempts by querying the database with provided credentials:\n- Returns `True` if matching credentials are found\n- Returns `False` if no matching record exists\n- Uses parameterized queries to prevent SQL injection\n\n```mermaid\nflowchart TD\n    DBConnection[\"mysql.connector.connect()\u003cbr/\u003eGlobal Connection: cnx\"]\n    \n    GetAllDetails[\"get_all_details()\u003cbr/\u003eSELECT * FROM sign_up\"]\n    InsertSignup[\"insert_signup(email, username, password)\u003cbr/\u003eINSERT INTO sign_up\"]\n    SearchLogin[\"search_login_credentials(email, password)\u003cbr/\u003eSELECT email,password FROM sign_up\"]\n    \n    DBConnection --\u003e GetAllDetails\n    DBConnection --\u003e InsertSignup\n    DBConnection --\u003e SearchLogin\n    \n    InsertSignup --\u003e Success[\"Return 1\u003cbr/\u003eSuccess\"]\n    InsertSignup --\u003e Error[\"Return -1\u003cbr/\u003eError + Rollback\"]\n    \n    SearchLogin --\u003e Found[\"Return True\u003cbr/\u003eCredentials Valid\"]\n    SearchLogin --\u003e NotFound[\"Return False\u003cbr/\u003eInvalid Credentials\"]\n```\n\n**Database Helper Function Flow**\n\nSources: [backend/db_helper.py:6-11](), [backend/db_helper.py:13-24](), [backend/db_helper.py:26-51](), [backend/db_helper.py:53-66]()\n\n## Flask Integration\n\nThe Flask server integrates with the database helper functions through two main routes that handle user authentication workflows.\n\n### Registration Endpoint\n\nThe `/signup_data` route processes POST requests containing user registration data:\n\n```python\n@app.route('/signup_data', methods=['POST'])\ndef signup_data():\n    data = request.get_json()\n    if((insert_signup(data['signupEmail'], data['username'], data['signupPassword'])) == 1):\n        response_data = {'message': 'Data inserted successfully!'}\n    else:\n        response_data = {'message': 'Error in inserting the Data!'}\n    return jsonify(response_data)\n```\n\n### Login Endpoint\n\nThe `/login_data` route validates user credentials and returns authentication status:\n\n```python\n@app.route('/login_data', methods=['POST'])\ndef login_data():\n    data = request.get_json()\n    response_data = search_login_credentials(data['email'], data['password'])\n    if response_data:\n        return jsonify(response_data)   \n    return jsonify(response_data = {'message': 'Data not found!'})\n```\n\n```mermaid\nsequenceDiagram\n    participant Client as \"Client Browser\"\n    participant Flask as \"Flask Server\u003cbr/\u003eserver.py\"\n    participant DBHelper as \"db_helper.py\"\n    participant MySQL as \"MySQL Database\u003cbr/\u003equizo.sign_up\"\n    \n    Note over Client,MySQL: User Registration Flow\n    Client-\u003e\u003eFlask: \"POST /signup_data\u003cbr/\u003e{email, username, password}\"\n    Flask-\u003e\u003eDBHelper: \"insert_signup(email, username, password)\"\n    DBHelper-\u003e\u003eMySQL: \"INSERT INTO sign_up VALUES (%s, %s, %s)\"\n    MySQL--\u003e\u003eDBHelper: \"Success/Error\"\n    DBHelper--\u003e\u003eFlask: \"Return 1/-1\"\n    Flask--\u003e\u003eClient: \"JSON Response\u003cbr/\u003e{message: success/error}\"\n    \n    Note over Client,MySQL: User Login Flow\n    Client-\u003e\u003eFlask: \"POST /login_data\u003cbr/\u003e{email, password}\"\n    Flask-\u003e\u003eDBHelper: \"search_login_credentials(email, password)\"\n    DBHelper-\u003e\u003eMySQL: \"SELECT email,password FROM sign_up\u003cbr/\u003eWHERE email=%s AND password=%s\"\n    MySQL--\u003e\u003eDBHelper: \"Matching Records\"\n    DBHelper--\u003e\u003eFlask: \"Return True/False\"\n    Flask--\u003e\u003eClient: \"JSON Response\u003cbr/\u003e{authentication result}\"\n```\n\n**Database Integration Sequence**\n\nSources: [server.py:15-24](), [server.py:28-35]()\n\n## Activity Logging System\n\nWhile user authentication data is stored in MySQL, the system uses file-based logging for tracking proctoring activities during exam sessions. This represents a separation of concerns between persistent user data and session-specific monitoring data.\n\n### File-Based Activity Storage\n\nThe proctoring system generates activity logs that are written to `activity.txt` files rather than being stored in the database. This approach allows for:\n\n- Real-time logging without database overhead\n- Simple text-based log analysis\n- Reduced database complexity\n- Separation of authentication and monitoring data\n\n### Data Persistence Strategy\n\nThe system employs a hybrid data persistence approach:\n\n| Data Type | Storage Method | Purpose |\n|-----------|---------------|---------|\n| User Credentials | MySQL Database | Persistent authentication |\n| User Registration | MySQL Database | Account management |\n| Proctoring Activities | Text Files | Session monitoring |\n| AI Detection Results | Text Files | Violation tracking |\n| System Alerts | Text Files | Audit trail |\n\n```mermaid\ngraph LR\n    subgraph \"Persistent Data\"\n        UserAuth[\"User Authentication\u003cbr/\u003eMySQL Database\"]\n    end\n    \n    subgraph \"Session Data\"\n        ActivityLogs[\"Activity Logs\u003cbr/\u003eactivity.txt\"]\n        DetectionResults[\"AI Detection Results\u003cbr/\u003eFile-based\"]\n    end\n    \n    subgraph \"Application Components\"\n        AuthSystem[\"Authentication System\u003cbr/\u003eFlask + db_helper\"]\n        ProctoringSystem[\"Proctoring System\u003cbr/\u003eAI Detection Engine\"]\n    end\n    \n    AuthSystem --\u003e UserAuth\n    ProctoringSystem --\u003e ActivityLogs\n    ProctoringSystem --\u003e DetectionResults\n```\n\n**Data Storage Strategy**\n\nSources: [backend/database.sql:1-56](), [backend/db_helper.py:1-73]()\n\n## Security Considerations\n\nThe current database implementation has several security limitations that should be addressed in production environments:\n\n- **Plain Text Passwords**: User passwords are stored without encryption or hashing\n- **Hardcoded Credentials**: Database connection uses hardcoded root credentials\n- **Limited Input Validation**: Minimal validation on user input data\n- **No Session Management**: No database-level session tracking or token management\n\nThese limitations indicate that the current implementation is designed for demonstration purposes rather than production deployment.\n\nSources: [backend/database.sql:27-33](), [backend/db_helper.py:6-11]()"])</script><script>self.__next_f.push([1,"20:T1e1a,"])</script><script>self.__next_f.push([1,"# AI Models and Configuration\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [object_detection_model/config/yolov3-tiny.cfg](object_detection_model/config/yolov3-tiny.cfg)\n- [object_detection_model/config/yolov3.cfg](object_detection_model/config/yolov3.cfg)\n- [object_detection_model/objectLabels/coco.names](object_detection_model/objectLabels/coco.names)\n- [object_detection_model/weights/yolov3-tiny.weights](object_detection_model/weights/yolov3-tiny.weights)\n\n\u003c/details\u003e\n\n\n\nThis document covers the pre-trained AI models and their configuration files used throughout the proctoring system. It provides an overview of model architecture, configuration parameters, and integration points with the detection modules.\n\nFor detailed implementation of specific detection algorithms that use these models, see [AI Detection Engine](#4). For YOLO-specific model details, see [YOLO Object Detection Models](#7.1). For facial landmark model specifications, see [Facial Landmark Models](#7.2).\n\n## Overview\n\nThe proctoring system relies on two primary categories of pre-trained models to perform real-time behavioral analysis:\n\n| Model Type | Purpose | Primary Detection Modules |\n|------------|---------|---------------------------|\n| YOLO Object Detection | Identify objects and people in video frames | `object_detection.py` |\n| dlib Shape Predictor | Extract 68 facial landmarks | `facial_detections.py`, `eye_tracker.py`, `blink_detection.py`, `head_pose_estimation.py`, `mouth_tracking.py` |\n\n## Model Architecture Integration\n\nThe following diagram shows how AI models integrate with the detection pipeline:\n\n**Model-to-Detection Module Integration**\n```mermaid\ngraph TB\n    subgraph \"Model Files\"\n        YOLOConfig[\"yolov3.cfg\u003cbr/\u003eyolov3-tiny.cfg\"]\n        YOLOWeights[\"YOLO Weights\u003cbr/\u003e(Binary Files)\"]\n        COCONames[\"coco.names\u003cbr/\u003eClass Labels\"]\n        ShapePredictor[\"Shape Predictor\u003cbr/\u003e68 Landmarks Model\"]\n    end\n    \n    subgraph \"Detection Modules\"\n        ObjectDet[\"object_detection.py\u003cbr/\u003ecv2.dnn.readNet()\"]\n        FacialDet[\"facial_detections.py\u003cbr/\u003edlib.get_frontal_face_detector()\"]\n        EyeTracker[\"eye_tracker.py\u003cbr/\u003eshape_predictor()\"]\n        BlinkDet[\"blink_detection.py\u003cbr/\u003eEye Aspect Ratio\"]\n        HeadPose[\"head_pose_estimation.py\u003cbr/\u003esolvePnP()\"]\n        MouthTrack[\"mouth_tracking.py\u003cbr/\u003eMouth Aspect Ratio\"]\n    end\n    \n    subgraph \"Core Application\"\n        ProctoringAlgo[\"proctoringAlgo()\u003cbr/\u003eMain Detection Orchestrator\"]\n    end\n    \n    YOLOConfig --\u003e ObjectDet\n    YOLOWeights --\u003e ObjectDet\n    COCONames --\u003e ObjectDet\n    ShapePredictor --\u003e FacialDet\n    ShapePredictor --\u003e EyeTracker\n    ShapePredictor --\u003e BlinkDet\n    ShapePredictor --\u003e HeadPose\n    ShapePredictor --\u003e MouthTrack\n    \n    ObjectDet --\u003e ProctoringAlgo\n    FacialDet --\u003e ProctoringAlgo\n    EyeTracker --\u003e ProctoringAlgo\n    BlinkDet --\u003e ProctoringAlgo\n    HeadPose --\u003e ProctoringAlgo\n    MouthTrack --\u003e ProctoringAlgo\n```\n\nSources: [object_detection_model/config/yolov3.cfg](), [object_detection_model/config/yolov3-tiny.cfg](), [object_detection_model/objectLabels/coco.names]()\n\n## YOLO Configuration Structure\n\nThe system provides two YOLO model configurations optimized for different performance requirements:\n\n**YOLO Model Configuration Comparison**\n```mermaid\ngraph LR\n    subgraph \"yolov3.cfg\"\n        YOLOFull[\"Full YOLO v3\u003cbr/\u003eInput: 608x608\u003cbr/\u003eClasses: 80\u003cbr/\u003eBatch: 64\"]\n        YOLOFullLayers[\"106 Layers\u003cbr/\u003e3 YOLO Detection Heads\u003cbr/\u003e9 Anchor Boxes\"]\n    end\n    \n    subgraph \"yolov3-tiny.cfg\"  \n        YOLOTiny[\"YOLO v3 Tiny\u003cbr/\u003eInput: 416x416\u003cbr/\u003eClasses: 80\u003cbr/\u003eBatch: 1\"]\n        YOLOTinyLayers[\"23 Layers\u003cbr/\u003e2 YOLO Detection Heads\u003cbr/\u003e6 Anchor Boxes\"]\n    end\n    \n    subgraph \"Shared Configuration\"\n        COCOClasses[\"coco.names\u003cbr/\u003e80 Object Classes\u003cbr/\u003eperson, laptop, cell phone...\"]\n        Anchors[\"Anchor Boxes\u003cbr/\u003eMulti-scale Detection\"]\n    end\n    \n    YOLOFull --\u003e COCOClasses\n    YOLOTiny --\u003e COCOClasses\n    YOLOFullLayers --\u003e Anchors\n    YOLOTinyLayers --\u003e Anchors\n```\n\nSources: [object_detection_model/config/yolov3.cfg:1-790](), [object_detection_model/config/yolov3-tiny.cfg:1-183](), [object_detection_model/objectLabels/coco.names:1-81]()\n\n## Configuration Parameters\n\n### YOLO Network Configuration\n\nThe YOLO configuration files define network architecture through key parameters:\n\n| Parameter | yolov3.cfg | yolov3-tiny.cfg | Description |\n|-----------|------------|------------------|-------------|\n| `width` | 608 | 416 | Input image width |\n| `height` | 608 | 416 | Input image height |\n| `batch` | 64 | 1 | Training batch size |\n| `classes` | 80 | 80 | Number of object classes |\n| `learning_rate` | 0.001 | 0.001 | Training learning rate |\n| `momentum` | 0.9 | 0.9 | SGD momentum |\n\nSources: [object_detection_model/config/yolov3.cfg:8-18](), [object_detection_model/config/yolov3-tiny.cfg:8-18]()\n\n### COCO Class Labels\n\nThe system uses the standard COCO dataset with 80 object classes for detection. Key classes relevant to proctoring include:\n\n```\nperson (class 0)\nlaptop (class 63)  \nmouse (class 64)\nkeyboard (class 66)\ncell phone (class 67)\nbook (class 73)\n```\n\nSources: [object_detection_model/objectLabels/coco.names:1-80]()\n\n## Model Initialization Flow\n\nThe following diagram illustrates how models are loaded and initialized during system startup:\n\n**Model Loading and Initialization Process**\n```mermaid\nsequenceDiagram\n    participant App as \"Main Application\"\n    participant ObjectDet as \"object_detection.py\"\n    participant FacialMod as \"Facial Analysis Modules\"\n    participant FileSystem as \"Model Files\"\n    \n    App-\u003e\u003eObjectDet: Initialize YOLO Detection\n    ObjectDet-\u003e\u003eFileSystem: Load yolov3.cfg\n    ObjectDet-\u003e\u003eFileSystem: Load YOLO weights\n    ObjectDet-\u003e\u003eFileSystem: Load coco.names\n    ObjectDet-\u003e\u003eObjectDet: cv2.dnn.readNet()\n    ObjectDet--\u003e\u003eApp: YOLO Net Ready\n    \n    App-\u003e\u003eFacialMod: Initialize Facial Detection\n    FacialMod-\u003e\u003eFileSystem: Load shape_predictor_68.dat\n    FacialMod-\u003e\u003eFacialMod: dlib.shape_predictor()\n    FacialMod-\u003e\u003eFacialMod: dlib.get_frontal_face_detector()\n    FacialMod--\u003e\u003eApp: Facial Models Ready\n    \n    App-\u003e\u003eApp: Start proctoringAlgo()\n```\n\nSources: [object_detection_model/config/yolov3.cfg](), [object_detection_model/objectLabels/coco.names]()\n\n## File Organization\n\nThe AI models and configuration files are organized in the following directory structure:\n\n```\nobject_detection_model/\n├── config/\n│   ├── yolov3.cfg          # Full YOLO v3 network configuration\n│   └── yolov3-tiny.cfg     # Lightweight YOLO v3 configuration\n├── objectLabels/\n│   └── coco.names          # COCO dataset class names (80 classes)\n└── weights/                # YOLO model weights (binary files)\n```\n\n## Integration Points\n\nThe models integrate with the proctoring system through several key mechanisms:\n\n1. **Object Detection Pipeline**: YOLO models loaded via `cv2.dnn.readNet()` for real-time object detection\n2. **Facial Analysis Pipeline**: dlib shape predictor models for 68-point facial landmark detection  \n3. **Configuration Management**: Model parameters defined in `.cfg` files control network behavior\n4. **Class Mapping**: COCO names file provides human-readable labels for detected objects\n\nThe `proctoringAlgo()` function orchestrates all model inference calls during active proctoring sessions, with each detection module responsible for loading and managing its specific model requirements.\n\nSources: [object_detection_model/config/yolov3.cfg](), [object_detection_model/config/yolov3-tiny.cfg](), [object_detection_model/objectLabels/coco.names]()"])</script><script>self.__next_f.push([1,"21:T20da,"])</script><script>self.__next_f.push([1,"# YOLO Object Detection Models\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [object_detection_model/config/yolov3-tiny.cfg](object_detection_model/config/yolov3-tiny.cfg)\n- [object_detection_model/config/yolov3.cfg](object_detection_model/config/yolov3.cfg)\n- [object_detection_model/objectLabels/coco.names](object_detection_model/objectLabels/coco.names)\n- [object_detection_model/weights/yolov3-tiny.weights](object_detection_model/weights/yolov3-tiny.weights)\n\n\u003c/details\u003e\n\n\n\n## Purpose and Scope\n\nThis document covers the YOLO (You Only Look Once) object detection models used in the AI-based Online Exam Proctoring System. These models are responsible for detecting and identifying objects in the video stream during exam sessions to identify potential unauthorized materials or suspicious activities.\n\nThe YOLO models specifically detect objects that may indicate cheating behavior, such as multiple persons, books, mobile phones, or other prohibited items. For information about other AI detection capabilities, see [Facial Analysis Modules](#4.1). For details on how object detection integrates with the overall proctoring workflow, see [Object and Audio Detection](#4.2).\n\n## Available Model Configurations\n\nThe system provides two YOLO model configurations optimized for different performance requirements:\n\n| Model | Configuration File | Input Resolution | Primary Use Case |\n|-------|-------------------|------------------|------------------|\n| YOLOv3 | `yolov3.cfg` | 608x608 pixels | High accuracy detection |\n| YOLOv3-Tiny | `yolov3-tiny.cfg` | 416x416 pixels | Fast real-time detection |\n\n### YOLOv3 Configuration\n\nThe standard YOLOv3 model [object_detection_model/config/yolov3.cfg:1-790]() provides comprehensive object detection with high accuracy. Key specifications:\n\n- **Input dimensions**: 608x608x3 [object_detection_model/config/yolov3.cfg:8-10]()\n- **Training batch size**: 64 with 16 subdivisions [object_detection_model/config/yolov3.cfg:6-7]()\n- **Detection layers**: Three YOLO layers at different scales [object_detection_model/config/yolov3.cfg:607-614](), [object_detection_model/config/yolov3.cfg:693-701](), [object_detection_model/config/yolov3.cfg:780-788]()\n- **Architecture**: Deep convolutional network with residual connections and skip connections\n\n### YOLOv3-Tiny Configuration\n\nThe lightweight YOLOv3-Tiny model [object_detection_model/config/yolov3-tiny.cfg:1-183]() is optimized for real-time performance:\n\n- **Input dimensions**: 416x416x3 [object_detection_model/config/yolov3-tiny.cfg:8-10]()\n- **Training batch size**: 1 with 1 subdivision for testing [object_detection_model/config/yolov3-tiny.cfg:3-4]()\n- **Detection layers**: Two YOLO layers [object_detection_model/config/yolov3-tiny.cfg:132-141](), [object_detection_model/config/yolov3-tiny.cfg:174-182]()\n- **Architecture**: Simplified network with fewer convolutional layers\n\n## Object Detection Classes\n\nBoth YOLO models detect objects from the COCO dataset, which includes 80 object classes [object_detection_model/objectLabels/coco.names:1-81](). The system monitors for specific objects relevant to exam proctoring:\n\n```mermaid\ngraph TD\n    CocoClasses[\"COCO Dataset Classes\u003cbr/\u003e(80 total classes)\"] --\u003e ProctoringRelevant[\"Proctoring-Relevant Objects\"]\n    CocoClasses --\u003e GeneralObjects[\"General Objects\"]\n    \n    ProctoringRelevant --\u003e People[\"person\"]\n    ProctoringRelevant --\u003e Electronics[\"cell phone\u003cbr/\u003elaptop\u003cbr/\u003emouse\u003cbr/\u003ekeyboard\u003cbr/\u003eremote\"]\n    ProctoringRelevant --\u003e Books[\"book\"]\n    ProctoringRelevant --\u003e Furniture[\"chair\u003cbr/\u003ediningtable\u003cbr/\u003ebed\"]\n    \n    GeneralObjects --\u003e Animals[\"cat, dog, bird\u003cbr/\u003ehorse, cow, etc.\"]\n    GeneralObjects --\u003e Vehicles[\"car, bicycle\u003cbr/\u003emotorbike, etc.\"]\n    GeneralObjects --\u003e Food[\"apple, banana\u003cbr/\u003esandwich, etc.\"]\n```\n\n### Key Proctoring Objects\n\nThe following objects are particularly relevant for exam monitoring:\n\n| Object Class | COCO ID | Proctoring Significance |\n|--------------|---------|------------------------|\n| person | 1 | Detects multiple people in frame |\n| book | 74 | Identifies reference materials |\n| cell phone | 68 | Detects mobile devices |\n| laptop | 64 | Identifies additional computers |\n| mouse | 65 | Detects computer peripherals |\n| keyboard | 67 | Identifies input devices |\n\nSources: [object_detection_model/objectLabels/coco.names:1-81]()\n\n## Model Architecture Integration\n\nThe YOLO models integrate with the proctoring system through a structured detection pipeline:\n\n```mermaid\ngraph TD\n    VideoStream[\"Video Stream Input\"] --\u003e ObjectDetection[\"object_detection.py\"]\n    \n    subgraph \"YOLO Model Components\"\n        ConfigFile[\"Configuration Files\u003cbr/\u003eyolov3.cfg or yolov3-tiny.cfg\"]\n        WeightFile[\"Pre-trained Weights\u003cbr/\u003e(Downloaded separately)\"]\n        ClassNames[\"Class Labels\u003cbr/\u003ecoco.names\"]\n    end\n    \n    ObjectDetection --\u003e ConfigFile\n    ObjectDetection --\u003e WeightFile  \n    ObjectDetection --\u003e ClassNames\n    \n    subgraph \"Detection Process\"\n        Preprocessing[\"Image Preprocessing\u003cbr/\u003eResize to model input size\"]\n        Inference[\"Neural Network Inference\u003cbr/\u003eForward pass through model\"]\n        Postprocessing[\"Post-processing\u003cbr/\u003eNMS, confidence filtering\"]\n    end\n    \n    ObjectDetection --\u003e Preprocessing\n    Preprocessing --\u003e Inference\n    Inference --\u003e Postprocessing\n    \n    Postprocessing --\u003e DetectionResults[\"Detection Results\u003cbr/\u003eBounding boxes + class labels\"]\n    DetectionResults --\u003e ProctoringLogic[\"Proctoring Logic\u003cbr/\u003eViolation detection\"]\n    ProctoringLogic --\u003e ActivityLogger[\"Activity Logger\u003cbr/\u003eLog violations to activity.txt\"]\n```\n\nSources: [object_detection_model/config/yolov3.cfg:1-790](), [object_detection_model/config/yolov3-tiny.cfg:1-183](), [object_detection_model/objectLabels/coco.names:1-81]()\n\n## Configuration File Structure\n\nBoth YOLO configuration files follow a structured format defining the neural network architecture:\n\n```mermaid\ngraph TD\n    ConfigFile[\"YOLO Configuration File\"] --\u003e NetSection[\"[net] Section\u003cbr/\u003eGlobal network parameters\"]\n    ConfigFile --\u003e ConvLayers[\"[convolutional] Layers\u003cbr/\u003eFeature extraction\"]\n    ConfigFile --\u003e SpecialLayers[\"Special Layers\u003cbr/\u003e[maxpool], [shortcut], [route], [upsample]\"]\n    ConfigFile --\u003e YoloLayers[\"[yolo] Layers\u003cbr/\u003eDetection outputs\"]\n    \n    NetSection --\u003e BatchSize[\"batch size\u003cbr/\u003esubdivisions\"]\n    NetSection --\u003e InputSize[\"width/height\u003cbr/\u003echannels\"]\n    NetSection --\u003e Training[\"learning_rate\u003cbr/\u003emomentum, decay\"]\n    \n    ConvLayers --\u003e Filters[\"filters: output channels\"]\n    ConvLayers --\u003e KernelSize[\"size: kernel dimensions\"]\n    ConvLayers --\u003e Activation[\"activation: leaky ReLU\"]\n    \n    YoloLayers --\u003e Anchors[\"anchors: predefined boxes\"]\n    YoloLayers --\u003e Classes[\"classes: 80 COCO classes\"]\n    YoloLayers --\u003e Masks[\"mask: anchor indices\"]\n```\n\n### Network Parameters Comparison\n\n| Parameter | YOLOv3 | YOLOv3-Tiny |\n|-----------|--------|-------------|\n| Input Resolution | 608×608 | 416×416 |\n| Convolutional Layers | ~75 layers | ~13 layers |\n| YOLO Detection Layers | 3 scales | 2 scales |\n| Anchor Boxes | 9 total (3 per scale) | 6 total (3 per scale) |\n| Model Complexity | High accuracy | Fast inference |\n\nSources: [object_detection_model/config/yolov3.cfg:1-24](), [object_detection_model/config/yolov3-tiny.cfg:1-24]()\n\n## Detection Layer Configuration\n\nThe YOLO detection layers are configured with specific parameters for object detection:\n\n### Anchor Box Configuration\n\nBoth models use predefined anchor boxes optimized for the COCO dataset:\n\n- **YOLOv3**: 9 anchor boxes across 3 scales [object_detection_model/config/yolov3.cfg:609-610]()\n  ```\n  anchors = 10,13, 16,30, 33,23, 30,61, 62,45, 59,119, 116,90, 156,198, 373,326\n  ```\n\n- **YOLOv3-Tiny**: 6 anchor boxes across 2 scales [object_detection_model/config/yolov3-tiny.cfg:134-135]()\n  ```\n  anchors = 10,14, 23,27, 37,58, 81,82, 135,169, 344,319\n  ```\n\n### Detection Parameters\n\n| Parameter | Value | Purpose |\n|-----------|-------|---------|\n| `classes` | 80 | Number of COCO dataset classes |\n| `ignore_thresh` | 0.7 | IoU threshold for ignoring predictions |\n| `truth_thresh` | 1.0 | IoU threshold for positive matches |\n| `jitter` | 0.3 | Data augmentation parameter |\n\nSources: [object_detection_model/config/yolov3.cfg:607-615](), [object_detection_model/config/yolov3-tiny.cfg:132-141]()"])</script><script>self.__next_f.push([1,"22:T2323,"])</script><script>self.__next_f.push([1,"# Facial Landmark Models\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [shape_predictor_model/The-68-specific-human-face-landmarks.png](shape_predictor_model/The-68-specific-human-face-landmarks.png)\n- [shape_predictor_model/shape_predictor_68_face_landmarks.dat](shape_predictor_model/shape_predictor_68_face_landmarks.dat)\n\n\u003c/details\u003e\n\n\n\n## Purpose and Scope\n\nThis document covers the facial landmark detection models used throughout the AI-based Online Exam Proctoring System for precise facial feature tracking and analysis. The system relies on 68-point facial landmark detection to enable sophisticated behavioral monitoring capabilities during online examinations.\n\nFor information about YOLO-based object detection models, see [YOLO Object Detection Models](#7.1). For details about how these landmarks are used in specific facial analysis algorithms, see [Facial Analysis Modules](#4.1).\n\n## Overview\n\nThe facial landmark detection system uses dlib's shape predictor model to identify 68 specific points on human faces. These landmarks serve as the foundation for all facial analysis modules in the proctoring system, enabling detection of eye movements, blink patterns, head pose changes, and mouth movements.\n\n**Facial Landmark Detection Pipeline**\n```mermaid\nflowchart TD\n    Input[\"Video Frame Input\"] --\u003e FaceDetection[\"dlib.get_frontal_face_detector()\"]\n    FaceDetection --\u003e FaceRect[\"Face Rectangle Detection\"]\n    FaceRect --\u003e ShapePredictor[\"shape_predictor_68_face_landmarks.dat\"]\n    ShapePredictor --\u003e Landmarks[\"68 Facial Landmark Points\"]\n    \n    Landmarks --\u003e EyePoints[\"Eye Region Points (36-47)\"]\n    Landmarks --\u003e MouthPoints[\"Mouth Region Points (48-67)\"]  \n    Landmarks --\u003e NosePoints[\"Nose Region Points (27-35)\"]\n    Landmarks --\u003e JawPoints[\"Jaw Region Points (0-16)\"]\n    Landmarks --\u003e EyebrowPoints[\"Eyebrow Region Points (17-26)\"]\n    \n    EyePoints --\u003e EyeTracking[\"eye_tracker.py\"]\n    EyePoints --\u003e BlinkDetection[\"blink_detection.py\"]\n    MouthPoints --\u003e MouthTracking[\"mouth_tracking.py\"]\n    JawPoints --\u003e HeadPose[\"head_pose_estimation.py\"]\n    NosePoints --\u003e HeadPose\n```\n\nSources: [shape_predictor_model/The-68-specific-human-face-landmarks.png:1-60]()\n\n## Landmark Point Mapping\n\nThe 68-point facial landmark model divides facial features into distinct regions, each serving specific analysis purposes within the proctoring system.\n\n### Landmark Region Distribution\n\n| Region | Point Range | Count | Primary Usage |\n|--------|-------------|-------|---------------|\n| Jaw | 0-16 | 17 | Head pose estimation, face boundary |\n| Right Eyebrow | 17-21 | 5 | Facial expression analysis |\n| Left Eyebrow | 22-26 | 5 | Facial expression analysis |\n| Nose | 27-35 | 9 | Head pose estimation, 3D orientation |\n| Right Eye | 36-41 | 6 | Eye tracking, blink detection |\n| Left Eye | 42-47 | 6 | Eye tracking, blink detection |\n| Mouth | 48-67 | 20 | Mouth movement detection, speech analysis |\n\n### Eye Region Landmarks (Critical for Proctoring)\n\n```mermaid\ngraph LR\n    subgraph \"Right Eye (36-41)\"\n        P36[\"Point 36\u003cbr/\u003eOuter Corner\"]\n        P37[\"Point 37\u003cbr/\u003eUpper Lid\"]\n        P38[\"Point 38\u003cbr/\u003eUpper Lid\"]\n        P39[\"Point 39\u003cbr/\u003eInner Corner\"]\n        P40[\"Point 40\u003cbr/\u003eLower Lid\"]\n        P41[\"Point 41\u003cbr/\u003eLower Lid\"]\n    end\n    \n    subgraph \"Left Eye (42-47)\"\n        P42[\"Point 42\u003cbr/\u003eInner Corner\"]\n        P43[\"Point 43\u003cbr/\u003eUpper Lid\"]\n        P44[\"Point 44\u003cbr/\u003eUpper Lid\"]\n        P45[\"Point 45\u003cbr/\u003eOuter Corner\"]\n        P46[\"Point 46\u003cbr/\u003eLower Lid\"]\n        P47[\"Point 47\u003cbr/\u003eLower Lid\"]\n    end\n    \n    P36 --\u003e EAR[\"Eye Aspect Ratio\u003cbr/\u003eCalculation\"]\n    P41 --\u003e EAR\n    P42 --\u003e EAR\n    P47 --\u003e EAR\n    \n    EAR --\u003e BlinkDetect[\"blink_detection.py\"]\n    EAR --\u003e EyeTrack[\"eye_tracker.py\"]\n```\n\nSources: [shape_predictor_model/The-68-specific-human-face-landmarks.png:1-60]()\n\n## Integration with Facial Analysis Modules\n\nThe facial landmark model serves as the core data source for multiple analysis modules within the proctoring system. Each module extracts specific landmark subsets for its analysis.\n\n**Module-Landmark Integration Architecture**\n```mermaid\nflowchart TB\n    ShapePredictor[\"shape_predictor_68_face_landmarks.dat\u003cbr/\u003eDlib Shape Predictor Model\"]\n    \n    subgraph \"Facial Analysis Modules\"\n        FacialDetections[\"facial_detections.py\u003cbr/\u003eFace boundary validation\"]\n        EyeTracker[\"eye_tracker.py\u003cbr/\u003eGaze direction analysis\"]  \n        BlinkDetection[\"blink_detection.py\u003cbr/\u003eEAR calculation\"]\n        HeadPose[\"head_pose_estimation.py\u003cbr/\u003e3D orientation\"]\n        MouthTracking[\"mouth_tracking.py\u003cbr/\u003eSpeech detection\"]\n    end\n    \n    subgraph \"Landmark Subsets\"\n        AllPoints[\"All 68 Points\"]\n        EyePoints[\"Eye Points (36-47)\"]\n        MouthPoints[\"Mouth Points (48-67)\"]\n        NoseJawPoints[\"Nose + Jaw Points (0-16, 27-35)\"]\n    end\n    \n    ShapePredictor --\u003e AllPoints\n    AllPoints --\u003e FacialDetections\n    AllPoints --\u003e EyePoints\n    AllPoints --\u003e MouthPoints  \n    AllPoints --\u003e NoseJawPoints\n    \n    EyePoints --\u003e EyeTracker\n    EyePoints --\u003e BlinkDetection\n    MouthPoints --\u003e MouthTracking\n    NoseJawPoints --\u003e HeadPose\n```\n\nSources: Based on system architecture from diagrams and inferred module interactions\n\n## Model Usage Patterns\n\n### Eye Aspect Ratio (EAR) Calculation\n\nThe most critical usage of facial landmarks in the proctoring system is for blink detection through Eye Aspect Ratio calculation. This uses 6 specific points per eye:\n\n```\nEAR = (|p2-p6| + |p3-p5|) / (2 * |p1-p4|)\n```\n\nWhere:\n- p1, p4: Horizontal eye corners\n- p2, p3, p5, p6: Vertical eye boundary points\n\n### Head Pose Estimation Points\n\nFor 3D head pose estimation, the system uses a subset of landmarks to create a 3D model:\n\n| 3D Model Point | Landmark Index | Description |\n|----------------|----------------|-------------|\n| Nose tip | 30 | Center reference point |\n| Chin | 8 | Lower face boundary |\n| Left eye corner | 36 | Left reference |\n| Right eye corner | 45 | Right reference |\n| Left mouth corner | 48 | Left mouth boundary |\n| Right mouth corner | 54 | Right mouth boundary |\n\n**Head Pose Calculation Flow**\n```mermaid\nsequenceDiagram\n    participant HPE as \"head_pose_estimation.py\"\n    participant Landmarks as \"68 Landmark Points\"\n    participant Model3D as \"3D Face Model Points\"\n    participant PnP as \"solvePnP Algorithm\"\n    \n    HPE-\u003e\u003eLandmarks: Extract 6 key points\n    Landmarks-\u003e\u003eModel3D: Map to 3D coordinates\n    Model3D-\u003e\u003ePnP: Calculate pose vectors\n    PnP-\u003e\u003eHPE: Return rotation/translation\n    HPE-\u003e\u003eHPE: Determine head orientation\n```\n\nSources: Inferred from system architecture and typical dlib shape predictor usage patterns\n\n## Technical Implementation Details\n\n### Model File Structure\n\nThe facial landmark detection relies on a pre-trained dlib model file, typically named `shape_predictor_68_face_landmarks.dat`. This binary model file contains:\n\n- Training data patterns for 68 facial points\n- Regression tree ensemble for point prediction\n- Feature descriptors for facial regions\n- Normalization parameters\n\n### Performance Characteristics\n\n| Metric | Value | Impact on Proctoring |\n|--------|-------|---------------------|\n| Landmark Points | 68 | High precision facial analysis |\n| Processing Speed | ~30-60 FPS | Real-time monitoring capability |\n| Accuracy | \u003e95% on frontal faces | Reliable detection for exam monitoring |\n| Memory Usage | ~60-100MB | Acceptable for real-time applications |\n\n### Integration Points in Codebase\n\nThe facial landmark model integrates with the proctoring system through several key interaction points:\n\n```mermaid\ngraph TD\n    subgraph \"Model Loading\"\n        ModelFile[\"shape_predictor_68_face_landmarks.dat\"]\n        DlibPredictor[\"dlib.shape_predictor()\"]\n    end\n    \n    subgraph \"Analysis Modules\"\n        FacialDetect[\"facial_detections.py\"]\n        EyeTrack[\"eye_tracker.py\"]\n        BlinkDetect[\"blink_detection.py\"]\n        HeadPoseEst[\"head_pose_estimation.py\"]\n        MouthTrack[\"mouth_tracking.py\"]\n    end\n    \n    subgraph \"Core Processing\"\n        MainApp[\"app.py\u003cbr/\u003eproctoringAlgo()\"]\n        VideoCapture[\"cv2.VideoCapture\"]\n    end\n    \n    ModelFile --\u003e DlibPredictor\n    VideoCapture --\u003e FacialDetect\n    DlibPredictor --\u003e FacialDetect\n    DlibPredictor --\u003e EyeTrack\n    DlibPredictor --\u003e BlinkDetect\n    DlibPredictor --\u003e HeadPoseEst\n    DlibPredictor --\u003e MouthTrack\n    \n    MainApp --\u003e VideoCapture\n    FacialDetect --\u003e MainApp\n    EyeTrack --\u003e MainApp\n    BlinkDetect --\u003e MainApp\n    HeadPoseEst --\u003e MainApp\n    MouthTrack --\u003e MainApp\n```\n\nSources: [shape_predictor_model/The-68-specific-human-face-landmarks.png:1-60](), system architecture diagrams, inferred from typical dlib integration patterns\n\nThe facial landmark models form the foundational layer for all behavioral analysis in the proctoring system, enabling precise tracking of facial features that indicate potential violations during online examinations."])</script><script>self.__next_f.push([1,"23:T21c4,"])</script><script>self.__next_f.push([1,"# Development and Testing\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [Project Overview.txt](Project Overview.txt)\n- [temp.py](temp.py)\n\n\u003c/details\u003e\n\n\n\nThis section covers the development tools, testing utilities, and debugging resources available for the AI-based Online Exam Proctoring System. It focuses on the testing infrastructure, individual module testing approaches, and development workflows used to ensure system reliability.\n\nFor information about the AI detection modules being tested, see [AI Detection Engine](#4). For details about the core application components, see [Core Application Components](#3.1).\n\n## Testing Infrastructure\n\nThe system provides a dedicated testing utility that enables developers to test individual AI detection modules in isolation or as an integrated pipeline. The primary testing tool is implemented as a temporary testing script that allows real-time validation of detection algorithms.\n\n### Testing Utility Overview\n\n```mermaid\ngraph TD\n    TestScript[\"temp.py\u003cbr/\u003eTesting Script\"] --\u003e VideoCapture[\"cv2.VideoCapture(0)\u003cbr/\u003eCamera Input\"]\n    VideoCapture --\u003e TestLoop[\"Testing Loop\u003cbr/\u003ewhile True\"]\n    \n    TestLoop --\u003e FaceTest[\"detectFace(frame)\u003cbr/\u003eFace Detection Test\"]\n    TestLoop --\u003e EyeTest[\"gazeDetection(faces, frame)\u003cbr/\u003eEye Tracking Test\"] \n    TestLoop --\u003e BlinkTest[\"isBlinking(faces, frame)\u003cbr/\u003eBlink Detection Test\"]\n    TestLoop --\u003e HeadTest[\"head_pose_detection(faces, frame)\u003cbr/\u003eHead Pose Test\"]\n    TestLoop --\u003e MouthTest[\"mouthTrack(faces, frame)\u003cbr/\u003eMouth Tracking Test\"]\n    TestLoop --\u003e ObjectTest[\"detectObject(frame)\u003cbr/\u003eObject Detection Test\"]\n    TestLoop --\u003e AudioTest[\"audio_detection()\u003cbr/\u003eAudio Detection Test\"]\n    \n    FaceTest --\u003e Display[\"cv2.imshow('Frame', frame)\u003cbr/\u003eVisual Output\"]\n    EyeTest --\u003e Display\n    BlinkTest --\u003e Display\n    HeadTest --\u003e Display\n    MouthTest --\u003e Console[\"print() Statements\u003cbr/\u003eConsole Output\"]\n    ObjectTest --\u003e Console\n    AudioTest --\u003e Console\n    \n    Display --\u003e ExitCheck[\"cv2.waitKey(1) \u0026 0xFF == ord('q')\u003cbr/\u003eExit Condition\"]\n    Console --\u003e ExitCheck\n    ExitCheck --\u003e TestLoop\n    ExitCheck --\u003e Cleanup[\"cam.release()\u003cbr/\u003ecv2.destroyAllWindows()\"]\n```\n\n**Testing Workflow with Core Functions**\n\nSources: [temp.py:1-48]()\n\n## Individual Module Testing\n\nThe testing framework allows developers to validate each AI detection module independently. Each module can be enabled or disabled for focused testing of specific functionality.\n\n### Module Test Configuration\n\n| Module | Function Call | Test Output | Status |\n|--------|---------------|-------------|---------|\n| Face Detection | `detectFace(frame)` | Face count and coordinates | Active |\n| Eye Tracking | `gazeDetection(faces, frame)` | Eye gaze status | Active |  \n| Blink Detection | `isBlinking(faces, frame)` | Blink status array | Active |\n| Head Pose | `head_pose_detection(faces, frame)` | Visual pose indicators | Active |\n| Mouth Tracking | `mouthTrack(faces, frame)` | Mouth status | Active with print |\n| Object Detection | `detectObject(frame)` | Detected objects | Active with print |\n| Audio Detection | `audio_detection()` | Audio analysis | Commented out |\n\nSources: [temp.py:15-37]()\n\n### Testing Data Flow\n\n```mermaid\nsequenceDiagram\n    participant Dev as \"Developer\"\n    participant TestScript as \"temp.py\"\n    participant Camera as \"cv2.VideoCapture\"\n    participant Modules as \"AI Detection Modules\"\n    participant Output as \"Display/Console\"\n    \n    Dev-\u003e\u003eTestScript: Execute testing script\n    TestScript-\u003e\u003eCamera: Initialize camera capture\n    \n    loop \"Testing Loop\"\n        Camera-\u003e\u003eTestScript: Capture frame\n        TestScript-\u003e\u003eModules: Call detectFace(frame)\n        Modules-\u003e\u003eTestScript: Return faceCount, faces\n        \n        TestScript-\u003e\u003eModules: Call gazeDetection(faces, frame) \n        Modules-\u003e\u003eTestScript: Return eyeStatus\n        \n        TestScript-\u003e\u003eModules: Call isBlinking(faces, frame)\n        Modules-\u003e\u003eTestScript: Return blinkStatus\n        \n        TestScript-\u003e\u003eModules: Call head_pose_detection(faces, frame)\n        Modules-\u003e\u003eOutput: Visual pose overlay\n        \n        TestScript-\u003e\u003eModules: Call mouthTrack(faces, frame)\n        Modules-\u003e\u003eOutput: Print mouth status\n        \n        TestScript-\u003e\u003eModules: Call detectObject(frame)\n        Modules-\u003e\u003eOutput: Print detected objects\n        \n        TestScript-\u003e\u003eOutput: Display processed frame\n        \n        Dev-\u003e\u003eTestScript: Press 'q' to quit\n        TestScript-\u003e\u003eCamera: Release resources\n    end\n```\n\n**Individual Module Testing Sequence**\n\nSources: [temp.py:10-47]()\n\n## Integration Testing Approach\n\nThe testing utility demonstrates how all AI detection modules work together in the complete proctoring pipeline. This integration testing validates the interaction between different detection algorithms and their combined performance.\n\n### Test Module Integration\n\nThe integration test imports all detection modules and executes them sequentially on each video frame:\n\n```mermaid\ngraph LR\n    subgraph \"Import Dependencies\"\n        CV2[\"cv2\u003cbr/\u003eComputer Vision\"]\n        FaceDet[\"facial_detections\u003cbr/\u003edetectFace\"]\n        EyeTr[\"eye_tracker\u003cbr/\u003egazeDetection\"]\n        BlinkDet[\"blink_detection\u003cbr/\u003eisBlinking\"]\n        HeadPose[\"head_pose_estimation\u003cbr/\u003ehead_pose_detection\"]\n        MouthTr[\"mouth_tracking\u003cbr/\u003emouthTrack\"]\n        ObjDet[\"object_detection\u003cbr/\u003edetectObject\"]\n        AudioDet[\"audio_detection\u003cbr/\u003eaudio_detection\"]\n    end\n    \n    subgraph \"Integration Pipeline\"\n        FrameInput[\"Video Frame Input\"] --\u003e FaceDetection[\"Face Detection\u003cbr/\u003efaceCount, faces\"]\n        FaceDetection --\u003e EyeTracking[\"Eye Tracking\u003cbr/\u003eeyeStatus\"]\n        FaceDetection --\u003e BlinkDetection[\"Blink Detection\u003cbr/\u003eblinkStatus\"]\n        FaceDetection --\u003e HeadPoseEst[\"Head Pose Estimation\u003cbr/\u003eVisual Output\"]\n        FaceDetection --\u003e MouthTracking[\"Mouth Tracking\u003cbr/\u003ePrint Output\"]\n        FrameInput --\u003e ObjectDetection[\"Object Detection\u003cbr/\u003ePrint Output\"]\n        FrameInput --\u003e AudioDetection[\"Audio Detection\u003cbr/\u003eCommented\"]\n    end\n    \n    FaceDet --\u003e FaceDetection\n    EyeTr --\u003e EyeTracking\n    BlinkDet --\u003e BlinkDetection\n    HeadPose --\u003e HeadPoseEst\n    MouthTr --\u003e MouthTracking\n    ObjDet --\u003e ObjectDetection\n    AudioDet --\u003e AudioDetection\n```\n\n**Module Integration Testing Architecture**\n\nSources: [temp.py:1-8]()\n\n## Development Workflow\n\nThe development workflow centers around iterative testing of individual components before integration into the main proctoring algorithm. The testing script serves as a sandbox environment for developers to validate changes.\n\n### Development Testing Process\n\n| Step | Action | File/Function | Purpose |\n|------|--------|---------------|---------|\n| 1 | Module Development | Individual detection files | Implement detection logic |\n| 2 | Unit Testing | `temp.py` with specific functions | Test individual modules |\n| 3 | Integration Testing | `temp.py` with all functions | Test module interactions |\n| 4 | System Integration | `main.py` or `app.py` | Integrate into main application |\n| 5 | Production Testing | Full proctoring system | End-to-end validation |\n\n### Testing Output Analysis\n\nThe testing script provides multiple output channels for debugging:\n\n- **Visual Output**: Processed video frames with overlays via `cv2.imshow()`\n- **Console Output**: Detection results printed to terminal for `mouthTrack()` and `detectObject()`\n- **Commented Testing**: Audio detection testing can be enabled by uncommenting the function call\n\nSources: [temp.py:31-37](), [temp.py:41-43]()\n\n## Debugging and Development Tools\n\nThe system provides several debugging capabilities through the testing infrastructure:\n\n### Real-time Debugging Features\n\n- **Live Video Feed**: Immediate visual feedback of detection algorithms\n- **Function Return Analysis**: Access to intermediate detection results\n- **Selective Testing**: Individual modules can be commented out for focused testing\n- **Interactive Exit**: Manual control over testing session duration\n\n### Development Guidelines\n\nBased on the testing infrastructure, developers should:\n\n1. Test individual modules using the `temp.py` framework before integration\n2. Validate visual outputs through the OpenCV display window\n3. Monitor console outputs for numerical detection results\n4. Use the modular testing approach to isolate issues in specific detection algorithms\n\nThe testing utility serves as both a development tool and a debugging platform, enabling rapid iteration and validation of AI detection improvements.\n\nSources: [Project Overview.txt:26](), [temp.py:1-48]()"])</script><script>self.__next_f.push([1,"24:T1741,"])</script><script>self.__next_f.push([1,"# Static Assets\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [static/images/Portal.jfif](static/images/Portal.jfif)\n- [static/images/Portal.jpg](static/images/Portal.jpg)\n\n\u003c/details\u003e\n\n\n\nThis document covers the static assets used in the AI-based Online Exam Proctoring System's web interface, including images, stylesheets, and client-side scripts that support the user interface components.\n\nFor information about the web interface templates and HTML structure, see [Web Interface](#5). For details about the authentication system that these assets support, see [Authentication System](#5.1).\n\n## Purpose and Scope\n\nThe static assets provide visual elements, styling, and client-side functionality for the web-based proctoring system. These assets are served by the Flask web server and support the user interface for authentication, exam presentation, and system interaction.\n\n## Directory Structure\n\nThe static assets are organized within the `static/` directory following Flask's standard static file organization:\n\n```mermaid\ngraph TD\n    static[\"static/\"]\n    static --\u003e images[\"images/\"]\n    static --\u003e scripts[\"JavaScript files\"]\n    static --\u003e styles[\"CSS files\"]\n    \n    images --\u003e portal_jfif[\"Portal.jfif\"]\n    images --\u003e portal_jpg[\"Portal.jpg\"]\n    \n    scripts --\u003e login_script[\"login_script.js\"]\n```\n\n**Static Asset Organization**\n| Directory | Purpose | File Types |\n|-----------|---------|------------|\n| `static/images/` | Visual assets and branding | `.jpg`, `.jfif` |\n| `static/` (root) | Client-side scripts | `.js` |\n| `static/` (root) | Stylesheets | `.css` |\n\nSources: [static/images/Portal.jfif](), [static/images/Portal.jpg]()\n\n## Image Assets\n\nThe system includes portal branding images used in the authentication interface:\n\n### Portal Images\n\nTwo portal images provide branding for the login and authentication pages:\n\n- **Portal.jfif**: Primary portal image in JFIF format\n- **Portal.jpg**: Portal image in standard JPEG format\n\nThese images appear to serve as branding elements for the exam portal interface, likely displayed during user authentication and system entry points.\n\n```mermaid\ngraph LR\n    auth_page[\"Authentication Page\"]\n    portal_jfif[\"Portal.jfif\"]\n    portal_jpg[\"Portal.jpg\"]\n    \n    auth_page --\u003e portal_jfif\n    auth_page --\u003e portal_jpg\n    \n    portal_jfif --\u003e branding[\"Portal Branding\"]\n    portal_jpg --\u003e branding\n```\n\n**Image Asset Properties**\n| File | Format | Usage Context |\n|------|--------|---------------|\n| `Portal.jfif` | JFIF/JPEG | Authentication interface branding |\n| `Portal.jpg` | JPEG | Authentication interface branding |\n\nSources: [static/images/Portal.jfif](), [static/images/Portal.jpg]()\n\n## Asset Serving Architecture\n\nFlask serves static assets through its built-in static file handling mechanism, integrated with the main web server:\n\n```mermaid\ngraph TD\n    client[\"Client Browser\"]\n    flask_server[\"Flask Server\u003cbr/\u003e(server.py)\"]\n    static_handler[\"Static File Handler\"]\n    static_dir[\"static/ Directory\"]\n    \n    client --\u003e|\"GET /static/images/Portal.jpg\"| flask_server\n    flask_server --\u003e static_handler\n    static_handler --\u003e static_dir\n    static_dir --\u003e|\"Image File\"| static_handler\n    static_handler --\u003e|\"HTTP Response\"| flask_server\n    flask_server --\u003e|\"Image Data\"| client\n    \n    subgraph \"Static Assets\"\n        images[\"images/\"]\n        scripts[\"JavaScript\"]\n        styles[\"CSS\"]\n    end\n    \n    static_dir --\u003e images\n    static_dir --\u003e scripts\n    static_dir --\u003e styles\n```\n\n**Asset Request Flow**\n1. Client requests static asset via URL path\n2. Flask server receives request for `/static/*` route\n3. Static file handler locates file in `static/` directory\n4. File content returned as HTTP response\n5. Browser renders or executes asset\n\nSources: Based on Flask static file serving architecture and [static/images/Portal.jfif](), [static/images/Portal.jpg]()\n\n## Integration with Web Interface\n\nStatic assets integrate with the web interface components to provide complete user experience:\n\n```mermaid\ngraph TD\n    templates[\"HTML Templates\"]\n    static_assets[\"Static Assets\"]\n    \n    templates --\u003e login_page[\"Login Page\"]\n    templates --\u003e quiz_page[\"Quiz Interface\"]\n    \n    static_assets --\u003e images[\"Portal Images\"]\n    static_assets --\u003e scripts[\"Client Scripts\"]\n    static_assets --\u003e styles[\"Stylesheets\"]\n    \n    login_page --\u003e images\n    login_page --\u003e scripts\n    quiz_page --\u003e scripts\n    quiz_page --\u003e styles\n    \n    subgraph \"User Experience\"\n        visual[\"Visual Branding\"]\n        interaction[\"Client Interaction\"]\n        presentation[\"Content Presentation\"]\n    end\n    \n    images --\u003e visual\n    scripts --\u003e interaction\n    styles --\u003e presentation\n```\n\n**Asset Usage Patterns**\n| Asset Type | Template Context | Functionality |\n|------------|------------------|---------------|\n| Portal Images | Authentication pages | Visual branding and user orientation |\n| JavaScript files | Interactive pages | Client-side form handling and validation |\n| CSS files | All pages | Visual styling and responsive layout |\n\nSources: Based on system architecture and [static/images/Portal.jfif](), [static/images/Portal.jpg]()\n\n## Asset Management\n\nThe static assets follow standard web development practices for organization and deployment:\n\n- **File Naming**: Descriptive names indicating purpose (`Portal.jpg`, `login_script.js`)\n- **Format Selection**: Appropriate formats for content type (JPEG for images, JS for scripts)\n- **Directory Organization**: Logical grouping by asset type (`images/`, scripts, styles)\n- **Flask Integration**: Standard Flask static file serving configuration\n\nThe system maintains separate image formats (`.jfif` and `.jpg`) for the same portal branding, potentially for browser compatibility or different usage contexts within the authentication flow.\n\nSources: [static/images/Portal.jfif](), [static/images/Portal.jpg]()"])</script><script>self.__next_f.push([1,"5:[\"$\",\"$L12\",null,{\"repoName\":\"krishnakumaragrawal/Artificial-Intelligence-based-Online-Exam-Proctoring-System\",\"hasConfig\":false,\"canSteer\":false,\"children\":[\"$\",\"$L13\",null,{\"wiki\":{\"metadata\":{\"repo_name\":\"krishnakumaragrawal/Artificial-Intelligence-based-Online-Exam-Proctoring-System\",\"commit_hash\":\"2b4f4e85\",\"generated_at\":\"2025-06-26T14:31:50.964283\",\"config\":null,\"config_source\":\"none\"},\"pages\":[{\"page_plan\":{\"id\":\"1\",\"title\":\"Overview\"},\"content\":\"$14\"},{\"page_plan\":{\"id\":\"2\",\"title\":\"Getting Started\"},\"content\":\"$15\"},{\"page_plan\":{\"id\":\"3\",\"title\":\"System Architecture\"},\"content\":\"$16\"},{\"page_plan\":{\"id\":\"3.1\",\"title\":\"Core Application Components\"},\"content\":\"$17\"},{\"page_plan\":{\"id\":\"3.2\",\"title\":\"Activity Logging System\"},\"content\":\"$18\"},{\"page_plan\":{\"id\":\"4\",\"title\":\"AI Detection Engine\"},\"content\":\"$19\"},{\"page_plan\":{\"id\":\"4.1\",\"title\":\"Facial Analysis Modules\"},\"content\":\"$1a\"},{\"page_plan\":{\"id\":\"4.2\",\"title\":\"Object and Audio Detection\"},\"content\":\"$1b\"},{\"page_plan\":{\"id\":\"5\",\"title\":\"Web Interface\"},\"content\":\"$1c\"},{\"page_plan\":{\"id\":\"5.1\",\"title\":\"Authentication System\"},\"content\":\"$1d\"},{\"page_plan\":{\"id\":\"5.2\",\"title\":\"Quiz Interface\"},\"content\":\"$1e\"},{\"page_plan\":{\"id\":\"6\",\"title\":\"Database and Data Management\"},\"content\":\"$1f\"},{\"page_plan\":{\"id\":\"7\",\"title\":\"AI Models and Configuration\"},\"content\":\"$20\"},{\"page_plan\":{\"id\":\"7.1\",\"title\":\"YOLO Object Detection Models\"},\"content\":\"$21\"},{\"page_plan\":{\"id\":\"7.2\",\"title\":\"Facial Landmark Models\"},\"content\":\"$22\"},{\"page_plan\":{\"id\":\"8\",\"title\":\"Development and Testing\"},\"content\":\"$23\"},{\"page_plan\":{\"id\":\"9\",\"title\":\"Static Assets\"},\"content\":\"$24\"}]},\"children\":[\"$\",\"$L3\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L4\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]}]}]\nc:null\n10:[[\"$\",\"title\",\"0\",{\"children\":\"Getting Started | krishnakumaragrawal/A"])</script><script>self.__next_f.push([1,"rtificial-Intelligence-based-Online-Exam-Proctoring-System | DeepWiki\"}],[\"$\",\"meta\",\"1\",{\"name\":\"description\",\"content\":\"This document provides a comprehensive guide to setting up and running the AI-based Online Exam Proctoring System. It covers the primary entry points, system initialization, and basic usage patterns t\"}],[\"$\",\"meta\",\"2\",{\"property\":\"og:title\",\"content\":\"Getting Started | krishnakumaragrawal/Artificial-Intelligence-based-Online-Exam-Proctoring-System | DeepWiki\"}],[\"$\",\"meta\",\"3\",{\"property\":\"og:description\",\"content\":\"This document provides a comprehensive guide to setting up and running the AI-based Online Exam Proctoring System. It covers the primary entry points, system initialization, and basic usage patterns t\"}],[\"$\",\"meta\",\"4\",{\"property\":\"og:url\",\"content\":\"https://deepwiki.com/krishnakumaragrawal/Artificial-Intelligence-based-Online-Exam-Proctoring-System/2-getting-started\"}],[\"$\",\"meta\",\"5\",{\"property\":\"og:site_name\",\"content\":\"DeepWiki\"}],[\"$\",\"meta\",\"6\",{\"property\":\"og:type\",\"content\":\"website\"}],[\"$\",\"meta\",\"7\",{\"name\":\"twitter:card\",\"content\":\"summary\"}],[\"$\",\"meta\",\"8\",{\"name\":\"twitter:title\",\"content\":\"Getting Started | krishnakumaragrawal/Artificial-Intelligence-based-Online-Exam-Proctoring-System | DeepWiki\"}],[\"$\",\"meta\",\"9\",{\"name\":\"twitter:description\",\"content\":\"This document provides a comprehensive guide to setting up and running the AI-based Online Exam Proctoring System. It covers the primary entry points, system initialization, and basic usage patterns t\"}],[\"$\",\"link\",\"10\",{\"rel\":\"icon\",\"href\":\"/favicon.ico\",\"type\":\"image/x-icon\",\"sizes\":\"48x48\"}],[\"$\",\"link\",\"11\",{\"rel\":\"icon\",\"href\":\"/icon.png?66aaf51e0e68c818\",\"type\":\"image/png\",\"sizes\":\"16x16\"}],[\"$\",\"link\",\"12\",{\"rel\":\"apple-touch-icon\",\"href\":\"/apple-icon.png?a4f658907db0ab87\",\"type\":\"image/png\",\"sizes\":\"180x180\"}]]\n"])</script></body></html>